{
  "best_metric": 0.5423048734664917,
  "best_model_checkpoint": "./results\\checkpoint-42144",
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 42144,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009491268033409264,
      "grad_norm": 6.781533718109131,
      "learning_rate": 1.8982536066818528e-08,
      "loss": 1.9045,
      "step": 50
    },
    {
      "epoch": 0.018982536066818528,
      "grad_norm": 7.266733646392822,
      "learning_rate": 3.7965072133637056e-08,
      "loss": 1.8964,
      "step": 100
    },
    {
      "epoch": 0.02847380410022779,
      "grad_norm": 7.532681941986084,
      "learning_rate": 5.694760820045559e-08,
      "loss": 1.9081,
      "step": 150
    },
    {
      "epoch": 0.037965072133637055,
      "grad_norm": 6.661933422088623,
      "learning_rate": 7.593014426727411e-08,
      "loss": 1.8962,
      "step": 200
    },
    {
      "epoch": 0.04745634016704632,
      "grad_norm": 4.782284259796143,
      "learning_rate": 9.491268033409265e-08,
      "loss": 1.9219,
      "step": 250
    },
    {
      "epoch": 0.05694760820045558,
      "grad_norm": 6.672574043273926,
      "learning_rate": 1.1389521640091118e-07,
      "loss": 1.8861,
      "step": 300
    },
    {
      "epoch": 0.06643887623386484,
      "grad_norm": 6.003439903259277,
      "learning_rate": 1.328777524677297e-07,
      "loss": 1.9094,
      "step": 350
    },
    {
      "epoch": 0.07593014426727411,
      "grad_norm": 6.052431583404541,
      "learning_rate": 1.5186028853454822e-07,
      "loss": 1.93,
      "step": 400
    },
    {
      "epoch": 0.08542141230068337,
      "grad_norm": 8.3948392868042,
      "learning_rate": 1.7084282460136675e-07,
      "loss": 1.9126,
      "step": 450
    },
    {
      "epoch": 0.09491268033409264,
      "grad_norm": 6.39821720123291,
      "learning_rate": 1.898253606681853e-07,
      "loss": 1.8955,
      "step": 500
    },
    {
      "epoch": 0.1044039483675019,
      "grad_norm": 6.71638298034668,
      "learning_rate": 2.0880789673500383e-07,
      "loss": 1.885,
      "step": 550
    },
    {
      "epoch": 0.11389521640091116,
      "grad_norm": 5.5600361824035645,
      "learning_rate": 2.2779043280182236e-07,
      "loss": 1.8805,
      "step": 600
    },
    {
      "epoch": 0.12338648443432043,
      "grad_norm": 8.162986755371094,
      "learning_rate": 2.4677296886864086e-07,
      "loss": 1.8604,
      "step": 650
    },
    {
      "epoch": 0.13287775246772968,
      "grad_norm": 6.437964916229248,
      "learning_rate": 2.657555049354594e-07,
      "loss": 1.88,
      "step": 700
    },
    {
      "epoch": 0.14236902050113895,
      "grad_norm": 6.0843186378479,
      "learning_rate": 2.847380410022779e-07,
      "loss": 1.8939,
      "step": 750
    },
    {
      "epoch": 0.15186028853454822,
      "grad_norm": 6.881246566772461,
      "learning_rate": 3.0372057706909645e-07,
      "loss": 1.8767,
      "step": 800
    },
    {
      "epoch": 0.16135155656795747,
      "grad_norm": 7.419300556182861,
      "learning_rate": 3.22703113135915e-07,
      "loss": 1.8718,
      "step": 850
    },
    {
      "epoch": 0.17084282460136674,
      "grad_norm": 5.736841678619385,
      "learning_rate": 3.416856492027335e-07,
      "loss": 1.8611,
      "step": 900
    },
    {
      "epoch": 0.180334092634776,
      "grad_norm": 6.552237033843994,
      "learning_rate": 3.6066818526955203e-07,
      "loss": 1.8646,
      "step": 950
    },
    {
      "epoch": 0.18982536066818528,
      "grad_norm": 7.06653356552124,
      "learning_rate": 3.796507213363706e-07,
      "loss": 1.8617,
      "step": 1000
    },
    {
      "epoch": 0.19931662870159453,
      "grad_norm": 6.590609073638916,
      "learning_rate": 3.9863325740318914e-07,
      "loss": 1.8511,
      "step": 1050
    },
    {
      "epoch": 0.2088078967350038,
      "grad_norm": 8.35180950164795,
      "learning_rate": 4.1761579347000767e-07,
      "loss": 1.8463,
      "step": 1100
    },
    {
      "epoch": 0.21829916476841307,
      "grad_norm": 7.201739311218262,
      "learning_rate": 4.365983295368262e-07,
      "loss": 1.8574,
      "step": 1150
    },
    {
      "epoch": 0.22779043280182232,
      "grad_norm": 6.361911773681641,
      "learning_rate": 4.555808656036447e-07,
      "loss": 1.8475,
      "step": 1200
    },
    {
      "epoch": 0.2372817008352316,
      "grad_norm": 8.124828338623047,
      "learning_rate": 4.7456340167046325e-07,
      "loss": 1.8672,
      "step": 1250
    },
    {
      "epoch": 0.24677296886864086,
      "grad_norm": 7.5856804847717285,
      "learning_rate": 4.935459377372817e-07,
      "loss": 1.8176,
      "step": 1300
    },
    {
      "epoch": 0.25626423690205014,
      "grad_norm": 5.980137825012207,
      "learning_rate": 5.125284738041003e-07,
      "loss": 1.8091,
      "step": 1350
    },
    {
      "epoch": 0.26575550493545935,
      "grad_norm": 7.428492069244385,
      "learning_rate": 5.315110098709188e-07,
      "loss": 1.8369,
      "step": 1400
    },
    {
      "epoch": 0.2752467729688686,
      "grad_norm": 6.454822063446045,
      "learning_rate": 5.504935459377373e-07,
      "loss": 1.8164,
      "step": 1450
    },
    {
      "epoch": 0.2847380410022779,
      "grad_norm": 6.958491325378418,
      "learning_rate": 5.694760820045558e-07,
      "loss": 1.7924,
      "step": 1500
    },
    {
      "epoch": 0.29422930903568717,
      "grad_norm": 6.073852062225342,
      "learning_rate": 5.884586180713744e-07,
      "loss": 1.7821,
      "step": 1550
    },
    {
      "epoch": 0.30372057706909644,
      "grad_norm": 6.060319423675537,
      "learning_rate": 6.074411541381929e-07,
      "loss": 1.8076,
      "step": 1600
    },
    {
      "epoch": 0.3132118451025057,
      "grad_norm": 6.922138214111328,
      "learning_rate": 6.264236902050115e-07,
      "loss": 1.7945,
      "step": 1650
    },
    {
      "epoch": 0.32270311313591493,
      "grad_norm": 8.807982444763184,
      "learning_rate": 6.4540622627183e-07,
      "loss": 1.7946,
      "step": 1700
    },
    {
      "epoch": 0.3321943811693242,
      "grad_norm": 8.394856452941895,
      "learning_rate": 6.643887623386485e-07,
      "loss": 1.7592,
      "step": 1750
    },
    {
      "epoch": 0.3416856492027335,
      "grad_norm": 7.335812568664551,
      "learning_rate": 6.83371298405467e-07,
      "loss": 1.7673,
      "step": 1800
    },
    {
      "epoch": 0.35117691723614275,
      "grad_norm": 5.55386209487915,
      "learning_rate": 7.023538344722855e-07,
      "loss": 1.7621,
      "step": 1850
    },
    {
      "epoch": 0.360668185269552,
      "grad_norm": 6.270002841949463,
      "learning_rate": 7.213363705391041e-07,
      "loss": 1.7644,
      "step": 1900
    },
    {
      "epoch": 0.3701594533029613,
      "grad_norm": 6.1040472984313965,
      "learning_rate": 7.403189066059226e-07,
      "loss": 1.742,
      "step": 1950
    },
    {
      "epoch": 0.37965072133637057,
      "grad_norm": 7.086050987243652,
      "learning_rate": 7.593014426727412e-07,
      "loss": 1.7636,
      "step": 2000
    },
    {
      "epoch": 0.3891419893697798,
      "grad_norm": 6.752301216125488,
      "learning_rate": 7.782839787395596e-07,
      "loss": 1.7136,
      "step": 2050
    },
    {
      "epoch": 0.39863325740318906,
      "grad_norm": 7.262324333190918,
      "learning_rate": 7.972665148063783e-07,
      "loss": 1.7126,
      "step": 2100
    },
    {
      "epoch": 0.40812452543659833,
      "grad_norm": 6.354671478271484,
      "learning_rate": 8.162490508731967e-07,
      "loss": 1.768,
      "step": 2150
    },
    {
      "epoch": 0.4176157934700076,
      "grad_norm": 6.536283016204834,
      "learning_rate": 8.352315869400153e-07,
      "loss": 1.7133,
      "step": 2200
    },
    {
      "epoch": 0.4271070615034169,
      "grad_norm": 5.915724277496338,
      "learning_rate": 8.542141230068338e-07,
      "loss": 1.6628,
      "step": 2250
    },
    {
      "epoch": 0.43659832953682615,
      "grad_norm": 6.443665981292725,
      "learning_rate": 8.731966590736524e-07,
      "loss": 1.6854,
      "step": 2300
    },
    {
      "epoch": 0.44608959757023536,
      "grad_norm": 6.667612552642822,
      "learning_rate": 8.921791951404708e-07,
      "loss": 1.6875,
      "step": 2350
    },
    {
      "epoch": 0.45558086560364464,
      "grad_norm": 5.933440208435059,
      "learning_rate": 9.111617312072894e-07,
      "loss": 1.6823,
      "step": 2400
    },
    {
      "epoch": 0.4650721336370539,
      "grad_norm": 5.7874932289123535,
      "learning_rate": 9.301442672741079e-07,
      "loss": 1.6848,
      "step": 2450
    },
    {
      "epoch": 0.4745634016704632,
      "grad_norm": 6.891671180725098,
      "learning_rate": 9.491268033409265e-07,
      "loss": 1.665,
      "step": 2500
    },
    {
      "epoch": 0.48405466970387245,
      "grad_norm": 6.06433629989624,
      "learning_rate": 9.68109339407745e-07,
      "loss": 1.6278,
      "step": 2550
    },
    {
      "epoch": 0.4935459377372817,
      "grad_norm": 6.95818567276001,
      "learning_rate": 9.870918754745634e-07,
      "loss": 1.6393,
      "step": 2600
    },
    {
      "epoch": 0.503037205770691,
      "grad_norm": 6.200324535369873,
      "learning_rate": 1.006074411541382e-06,
      "loss": 1.6185,
      "step": 2650
    },
    {
      "epoch": 0.5125284738041003,
      "grad_norm": 6.010601043701172,
      "learning_rate": 1.0250569476082005e-06,
      "loss": 1.5811,
      "step": 2700
    },
    {
      "epoch": 0.5220197418375095,
      "grad_norm": 8.4634428024292,
      "learning_rate": 1.044039483675019e-06,
      "loss": 1.599,
      "step": 2750
    },
    {
      "epoch": 0.5315110098709187,
      "grad_norm": 5.99493408203125,
      "learning_rate": 1.0630220197418376e-06,
      "loss": 1.6122,
      "step": 2800
    },
    {
      "epoch": 0.541002277904328,
      "grad_norm": 5.33767032623291,
      "learning_rate": 1.082004555808656e-06,
      "loss": 1.6236,
      "step": 2850
    },
    {
      "epoch": 0.5504935459377372,
      "grad_norm": 5.12007999420166,
      "learning_rate": 1.1009870918754746e-06,
      "loss": 1.604,
      "step": 2900
    },
    {
      "epoch": 0.5599848139711465,
      "grad_norm": 6.034331321716309,
      "learning_rate": 1.1199696279422931e-06,
      "loss": 1.6182,
      "step": 2950
    },
    {
      "epoch": 0.5694760820045558,
      "grad_norm": 5.838982582092285,
      "learning_rate": 1.1389521640091117e-06,
      "loss": 1.5432,
      "step": 3000
    },
    {
      "epoch": 0.5789673500379651,
      "grad_norm": 5.8897223472595215,
      "learning_rate": 1.1579347000759302e-06,
      "loss": 1.5486,
      "step": 3050
    },
    {
      "epoch": 0.5884586180713743,
      "grad_norm": 5.698884963989258,
      "learning_rate": 1.1769172361427487e-06,
      "loss": 1.5531,
      "step": 3100
    },
    {
      "epoch": 0.5979498861047836,
      "grad_norm": 6.753091335296631,
      "learning_rate": 1.1958997722095673e-06,
      "loss": 1.526,
      "step": 3150
    },
    {
      "epoch": 0.6074411541381929,
      "grad_norm": 5.610910415649414,
      "learning_rate": 1.2148823082763858e-06,
      "loss": 1.497,
      "step": 3200
    },
    {
      "epoch": 0.6169324221716022,
      "grad_norm": 5.57576847076416,
      "learning_rate": 1.2338648443432043e-06,
      "loss": 1.4717,
      "step": 3250
    },
    {
      "epoch": 0.6264236902050114,
      "grad_norm": 5.431122303009033,
      "learning_rate": 1.252847380410023e-06,
      "loss": 1.5459,
      "step": 3300
    },
    {
      "epoch": 0.6359149582384207,
      "grad_norm": 5.800791263580322,
      "learning_rate": 1.2718299164768414e-06,
      "loss": 1.4849,
      "step": 3350
    },
    {
      "epoch": 0.6454062262718299,
      "grad_norm": 6.740391254425049,
      "learning_rate": 1.29081245254366e-06,
      "loss": 1.547,
      "step": 3400
    },
    {
      "epoch": 0.6548974943052391,
      "grad_norm": 6.926745414733887,
      "learning_rate": 1.3097949886104786e-06,
      "loss": 1.6048,
      "step": 3450
    },
    {
      "epoch": 0.6643887623386484,
      "grad_norm": 6.111976146697998,
      "learning_rate": 1.328777524677297e-06,
      "loss": 1.5252,
      "step": 3500
    },
    {
      "epoch": 0.6738800303720577,
      "grad_norm": 7.135305881500244,
      "learning_rate": 1.3477600607441155e-06,
      "loss": 1.5267,
      "step": 3550
    },
    {
      "epoch": 0.683371298405467,
      "grad_norm": 6.208704948425293,
      "learning_rate": 1.366742596810934e-06,
      "loss": 1.4807,
      "step": 3600
    },
    {
      "epoch": 0.6928625664388762,
      "grad_norm": 7.233537673950195,
      "learning_rate": 1.3857251328777527e-06,
      "loss": 1.5276,
      "step": 3650
    },
    {
      "epoch": 0.7023538344722855,
      "grad_norm": 5.419264793395996,
      "learning_rate": 1.404707668944571e-06,
      "loss": 1.4692,
      "step": 3700
    },
    {
      "epoch": 0.7118451025056948,
      "grad_norm": 5.498508453369141,
      "learning_rate": 1.4236902050113896e-06,
      "loss": 1.4666,
      "step": 3750
    },
    {
      "epoch": 0.721336370539104,
      "grad_norm": 5.954653263092041,
      "learning_rate": 1.4426727410782081e-06,
      "loss": 1.4721,
      "step": 3800
    },
    {
      "epoch": 0.7308276385725133,
      "grad_norm": 5.4256205558776855,
      "learning_rate": 1.4616552771450269e-06,
      "loss": 1.4695,
      "step": 3850
    },
    {
      "epoch": 0.7403189066059226,
      "grad_norm": 5.338573932647705,
      "learning_rate": 1.4806378132118452e-06,
      "loss": 1.498,
      "step": 3900
    },
    {
      "epoch": 0.7498101746393319,
      "grad_norm": 5.638768672943115,
      "learning_rate": 1.4996203492786637e-06,
      "loss": 1.4868,
      "step": 3950
    },
    {
      "epoch": 0.7593014426727411,
      "grad_norm": 6.837031364440918,
      "learning_rate": 1.5186028853454824e-06,
      "loss": 1.4734,
      "step": 4000
    },
    {
      "epoch": 0.7687927107061503,
      "grad_norm": 5.757120609283447,
      "learning_rate": 1.537585421412301e-06,
      "loss": 1.4525,
      "step": 4050
    },
    {
      "epoch": 0.7782839787395596,
      "grad_norm": 4.101736068725586,
      "learning_rate": 1.5565679574791193e-06,
      "loss": 1.4537,
      "step": 4100
    },
    {
      "epoch": 0.7877752467729688,
      "grad_norm": 5.155644416809082,
      "learning_rate": 1.5755504935459378e-06,
      "loss": 1.4402,
      "step": 4150
    },
    {
      "epoch": 0.7972665148063781,
      "grad_norm": 5.0357770919799805,
      "learning_rate": 1.5945330296127566e-06,
      "loss": 1.3498,
      "step": 4200
    },
    {
      "epoch": 0.8067577828397874,
      "grad_norm": 5.147688388824463,
      "learning_rate": 1.6135155656795749e-06,
      "loss": 1.4572,
      "step": 4250
    },
    {
      "epoch": 0.8162490508731967,
      "grad_norm": 8.150679588317871,
      "learning_rate": 1.6324981017463934e-06,
      "loss": 1.4161,
      "step": 4300
    },
    {
      "epoch": 0.8257403189066059,
      "grad_norm": 5.87668514251709,
      "learning_rate": 1.651480637813212e-06,
      "loss": 1.45,
      "step": 4350
    },
    {
      "epoch": 0.8352315869400152,
      "grad_norm": 6.621004104614258,
      "learning_rate": 1.6704631738800307e-06,
      "loss": 1.4432,
      "step": 4400
    },
    {
      "epoch": 0.8447228549734245,
      "grad_norm": 5.4572649002075195,
      "learning_rate": 1.689445709946849e-06,
      "loss": 1.4434,
      "step": 4450
    },
    {
      "epoch": 0.8542141230068337,
      "grad_norm": 5.729280948638916,
      "learning_rate": 1.7084282460136675e-06,
      "loss": 1.4217,
      "step": 4500
    },
    {
      "epoch": 0.863705391040243,
      "grad_norm": 6.268518447875977,
      "learning_rate": 1.727410782080486e-06,
      "loss": 1.3319,
      "step": 4550
    },
    {
      "epoch": 0.8731966590736523,
      "grad_norm": 5.37209939956665,
      "learning_rate": 1.7463933181473048e-06,
      "loss": 1.4134,
      "step": 4600
    },
    {
      "epoch": 0.8826879271070615,
      "grad_norm": 5.681509494781494,
      "learning_rate": 1.765375854214123e-06,
      "loss": 1.4411,
      "step": 4650
    },
    {
      "epoch": 0.8921791951404707,
      "grad_norm": 5.10499382019043,
      "learning_rate": 1.7843583902809416e-06,
      "loss": 1.3592,
      "step": 4700
    },
    {
      "epoch": 0.90167046317388,
      "grad_norm": 5.10657262802124,
      "learning_rate": 1.8033409263477604e-06,
      "loss": 1.3716,
      "step": 4750
    },
    {
      "epoch": 0.9111617312072893,
      "grad_norm": 5.1439666748046875,
      "learning_rate": 1.8223234624145789e-06,
      "loss": 1.3919,
      "step": 4800
    },
    {
      "epoch": 0.9206529992406985,
      "grad_norm": 6.941906452178955,
      "learning_rate": 1.8413059984813972e-06,
      "loss": 1.3528,
      "step": 4850
    },
    {
      "epoch": 0.9301442672741078,
      "grad_norm": 4.646480560302734,
      "learning_rate": 1.8602885345482157e-06,
      "loss": 1.3679,
      "step": 4900
    },
    {
      "epoch": 0.9396355353075171,
      "grad_norm": 4.150309085845947,
      "learning_rate": 1.8792710706150345e-06,
      "loss": 1.354,
      "step": 4950
    },
    {
      "epoch": 0.9491268033409264,
      "grad_norm": 6.288318157196045,
      "learning_rate": 1.898253606681853e-06,
      "loss": 1.4323,
      "step": 5000
    },
    {
      "epoch": 0.9586180713743356,
      "grad_norm": 5.229976177215576,
      "learning_rate": 1.9172361427486713e-06,
      "loss": 1.3896,
      "step": 5050
    },
    {
      "epoch": 0.9681093394077449,
      "grad_norm": 5.8981852531433105,
      "learning_rate": 1.93621867881549e-06,
      "loss": 1.3476,
      "step": 5100
    },
    {
      "epoch": 0.9776006074411542,
      "grad_norm": 7.595494747161865,
      "learning_rate": 1.9552012148823084e-06,
      "loss": 1.4024,
      "step": 5150
    },
    {
      "epoch": 0.9870918754745635,
      "grad_norm": 7.758345127105713,
      "learning_rate": 1.974183750949127e-06,
      "loss": 1.3631,
      "step": 5200
    },
    {
      "epoch": 0.9965831435079726,
      "grad_norm": 5.87869930267334,
      "learning_rate": 1.9931662870159454e-06,
      "loss": 1.3904,
      "step": 5250
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5406662237828604,
      "eval_f1": 0.19993373817835974,
      "eval_loss": 1.3035351037979126,
      "eval_precision": 0.3118988727776137,
      "eval_recall": 0.257349917072959,
      "eval_runtime": 190.9898,
      "eval_samples_per_second": 55.17,
      "eval_steps_per_second": 6.901,
      "step": 5268
    },
    {
      "epoch": 1.006074411541382,
      "grad_norm": 5.436108112335205,
      "learning_rate": 2.012148823082764e-06,
      "loss": 1.3635,
      "step": 5300
    },
    {
      "epoch": 1.0155656795747912,
      "grad_norm": 6.2826313972473145,
      "learning_rate": 2.0311313591495825e-06,
      "loss": 1.3878,
      "step": 5350
    },
    {
      "epoch": 1.0250569476082005,
      "grad_norm": 5.39915132522583,
      "learning_rate": 2.050113895216401e-06,
      "loss": 1.3445,
      "step": 5400
    },
    {
      "epoch": 1.0345482156416097,
      "grad_norm": 5.8375630378723145,
      "learning_rate": 2.0690964312832195e-06,
      "loss": 1.3352,
      "step": 5450
    },
    {
      "epoch": 1.044039483675019,
      "grad_norm": 5.015005588531494,
      "learning_rate": 2.088078967350038e-06,
      "loss": 1.3592,
      "step": 5500
    },
    {
      "epoch": 1.0535307517084282,
      "grad_norm": 6.2365288734436035,
      "learning_rate": 2.1070615034168566e-06,
      "loss": 1.312,
      "step": 5550
    },
    {
      "epoch": 1.0630220197418374,
      "grad_norm": 7.182867527008057,
      "learning_rate": 2.126044039483675e-06,
      "loss": 1.3133,
      "step": 5600
    },
    {
      "epoch": 1.0725132877752468,
      "grad_norm": 6.894881248474121,
      "learning_rate": 2.1450265755504936e-06,
      "loss": 1.3833,
      "step": 5650
    },
    {
      "epoch": 1.082004555808656,
      "grad_norm": 4.371942520141602,
      "learning_rate": 2.164009111617312e-06,
      "loss": 1.3228,
      "step": 5700
    },
    {
      "epoch": 1.0914958238420653,
      "grad_norm": 6.1741943359375,
      "learning_rate": 2.1829916476841307e-06,
      "loss": 1.2974,
      "step": 5750
    },
    {
      "epoch": 1.1009870918754745,
      "grad_norm": 6.626845836639404,
      "learning_rate": 2.2019741837509492e-06,
      "loss": 1.23,
      "step": 5800
    },
    {
      "epoch": 1.1104783599088839,
      "grad_norm": 5.182972431182861,
      "learning_rate": 2.2209567198177678e-06,
      "loss": 1.2712,
      "step": 5850
    },
    {
      "epoch": 1.119969627942293,
      "grad_norm": 8.018512725830078,
      "learning_rate": 2.2399392558845863e-06,
      "loss": 1.2832,
      "step": 5900
    },
    {
      "epoch": 1.1294608959757024,
      "grad_norm": 5.832951068878174,
      "learning_rate": 2.258921791951405e-06,
      "loss": 1.3009,
      "step": 5950
    },
    {
      "epoch": 1.1389521640091116,
      "grad_norm": 9.562484741210938,
      "learning_rate": 2.2779043280182233e-06,
      "loss": 1.2876,
      "step": 6000
    },
    {
      "epoch": 1.148443432042521,
      "grad_norm": 4.55044412612915,
      "learning_rate": 2.296886864085042e-06,
      "loss": 1.2829,
      "step": 6050
    },
    {
      "epoch": 1.1579347000759301,
      "grad_norm": 6.148865222930908,
      "learning_rate": 2.3158694001518604e-06,
      "loss": 1.3325,
      "step": 6100
    },
    {
      "epoch": 1.1674259681093395,
      "grad_norm": 5.95477819442749,
      "learning_rate": 2.334851936218679e-06,
      "loss": 1.2746,
      "step": 6150
    },
    {
      "epoch": 1.1769172361427487,
      "grad_norm": 7.444988250732422,
      "learning_rate": 2.3538344722854975e-06,
      "loss": 1.2723,
      "step": 6200
    },
    {
      "epoch": 1.1864085041761578,
      "grad_norm": 4.726620197296143,
      "learning_rate": 2.372817008352316e-06,
      "loss": 1.3168,
      "step": 6250
    },
    {
      "epoch": 1.1958997722095672,
      "grad_norm": 4.022007465362549,
      "learning_rate": 2.3917995444191345e-06,
      "loss": 1.2664,
      "step": 6300
    },
    {
      "epoch": 1.2053910402429764,
      "grad_norm": 7.383367538452148,
      "learning_rate": 2.410782080485953e-06,
      "loss": 1.304,
      "step": 6350
    },
    {
      "epoch": 1.2148823082763858,
      "grad_norm": 7.247731685638428,
      "learning_rate": 2.4297646165527716e-06,
      "loss": 1.2088,
      "step": 6400
    },
    {
      "epoch": 1.224373576309795,
      "grad_norm": 6.626587390899658,
      "learning_rate": 2.44874715261959e-06,
      "loss": 1.2823,
      "step": 6450
    },
    {
      "epoch": 1.2338648443432043,
      "grad_norm": 5.657367706298828,
      "learning_rate": 2.4677296886864086e-06,
      "loss": 1.2416,
      "step": 6500
    },
    {
      "epoch": 1.2433561123766135,
      "grad_norm": 5.688270568847656,
      "learning_rate": 2.486712224753227e-06,
      "loss": 1.2169,
      "step": 6550
    },
    {
      "epoch": 1.2528473804100229,
      "grad_norm": 5.919439792633057,
      "learning_rate": 2.505694760820046e-06,
      "loss": 1.2919,
      "step": 6600
    },
    {
      "epoch": 1.262338648443432,
      "grad_norm": 11.849943161010742,
      "learning_rate": 2.524677296886864e-06,
      "loss": 1.2256,
      "step": 6650
    },
    {
      "epoch": 1.2718299164768414,
      "grad_norm": 7.510192394256592,
      "learning_rate": 2.5436598329536827e-06,
      "loss": 1.2183,
      "step": 6700
    },
    {
      "epoch": 1.2813211845102506,
      "grad_norm": 4.747722625732422,
      "learning_rate": 2.5626423690205017e-06,
      "loss": 1.269,
      "step": 6750
    },
    {
      "epoch": 1.2908124525436597,
      "grad_norm": 5.539303302764893,
      "learning_rate": 2.58162490508732e-06,
      "loss": 1.1963,
      "step": 6800
    },
    {
      "epoch": 1.300303720577069,
      "grad_norm": 8.497318267822266,
      "learning_rate": 2.6006074411541383e-06,
      "loss": 1.2793,
      "step": 6850
    },
    {
      "epoch": 1.3097949886104785,
      "grad_norm": 4.934028625488281,
      "learning_rate": 2.6195899772209573e-06,
      "loss": 1.1998,
      "step": 6900
    },
    {
      "epoch": 1.3192862566438877,
      "grad_norm": 7.271026134490967,
      "learning_rate": 2.6385725132877754e-06,
      "loss": 1.2192,
      "step": 6950
    },
    {
      "epoch": 1.3287775246772968,
      "grad_norm": 4.72705602645874,
      "learning_rate": 2.657555049354594e-06,
      "loss": 1.2457,
      "step": 7000
    },
    {
      "epoch": 1.3382687927107062,
      "grad_norm": 5.830032825469971,
      "learning_rate": 2.6765375854214124e-06,
      "loss": 1.2299,
      "step": 7050
    },
    {
      "epoch": 1.3477600607441154,
      "grad_norm": 8.889971733093262,
      "learning_rate": 2.695520121488231e-06,
      "loss": 1.3395,
      "step": 7100
    },
    {
      "epoch": 1.3572513287775247,
      "grad_norm": 8.790315628051758,
      "learning_rate": 2.71450265755505e-06,
      "loss": 1.2323,
      "step": 7150
    },
    {
      "epoch": 1.366742596810934,
      "grad_norm": 8.498093605041504,
      "learning_rate": 2.733485193621868e-06,
      "loss": 1.2371,
      "step": 7200
    },
    {
      "epoch": 1.3762338648443433,
      "grad_norm": 7.1971917152404785,
      "learning_rate": 2.7524677296886865e-06,
      "loss": 1.2715,
      "step": 7250
    },
    {
      "epoch": 1.3857251328777525,
      "grad_norm": 7.510544776916504,
      "learning_rate": 2.7714502657555055e-06,
      "loss": 1.1283,
      "step": 7300
    },
    {
      "epoch": 1.3952164009111616,
      "grad_norm": 7.349056720733643,
      "learning_rate": 2.7904328018223236e-06,
      "loss": 1.195,
      "step": 7350
    },
    {
      "epoch": 1.404707668944571,
      "grad_norm": 6.695216655731201,
      "learning_rate": 2.809415337889142e-06,
      "loss": 1.199,
      "step": 7400
    },
    {
      "epoch": 1.4141989369779804,
      "grad_norm": 4.827058792114258,
      "learning_rate": 2.828397873955961e-06,
      "loss": 1.1474,
      "step": 7450
    },
    {
      "epoch": 1.4236902050113895,
      "grad_norm": 4.820675373077393,
      "learning_rate": 2.847380410022779e-06,
      "loss": 1.2232,
      "step": 7500
    },
    {
      "epoch": 1.4331814730447987,
      "grad_norm": 4.273978233337402,
      "learning_rate": 2.8663629460895977e-06,
      "loss": 1.2519,
      "step": 7550
    },
    {
      "epoch": 1.442672741078208,
      "grad_norm": 8.836321830749512,
      "learning_rate": 2.8853454821564162e-06,
      "loss": 1.1708,
      "step": 7600
    },
    {
      "epoch": 1.4521640091116172,
      "grad_norm": 7.872626304626465,
      "learning_rate": 2.9043280182232348e-06,
      "loss": 1.1577,
      "step": 7650
    },
    {
      "epoch": 1.4616552771450266,
      "grad_norm": 11.652875900268555,
      "learning_rate": 2.9233105542900537e-06,
      "loss": 1.2186,
      "step": 7700
    },
    {
      "epoch": 1.4711465451784358,
      "grad_norm": 7.26560640335083,
      "learning_rate": 2.942293090356872e-06,
      "loss": 1.1554,
      "step": 7750
    },
    {
      "epoch": 1.4806378132118452,
      "grad_norm": 4.772738933563232,
      "learning_rate": 2.9612756264236903e-06,
      "loss": 1.2332,
      "step": 7800
    },
    {
      "epoch": 1.4901290812452543,
      "grad_norm": 8.238712310791016,
      "learning_rate": 2.9802581624905093e-06,
      "loss": 1.1547,
      "step": 7850
    },
    {
      "epoch": 1.4996203492786635,
      "grad_norm": 7.161807060241699,
      "learning_rate": 2.9992406985573274e-06,
      "loss": 1.1491,
      "step": 7900
    },
    {
      "epoch": 1.5091116173120729,
      "grad_norm": 9.07370662689209,
      "learning_rate": 3.018223234624146e-06,
      "loss": 1.2157,
      "step": 7950
    },
    {
      "epoch": 1.5186028853454823,
      "grad_norm": 10.924820899963379,
      "learning_rate": 3.037205770690965e-06,
      "loss": 1.1609,
      "step": 8000
    },
    {
      "epoch": 1.5280941533788914,
      "grad_norm": 9.03322696685791,
      "learning_rate": 3.056188306757783e-06,
      "loss": 1.1391,
      "step": 8050
    },
    {
      "epoch": 1.5375854214123006,
      "grad_norm": 9.818289756774902,
      "learning_rate": 3.075170842824602e-06,
      "loss": 1.1589,
      "step": 8100
    },
    {
      "epoch": 1.54707668944571,
      "grad_norm": 12.107394218444824,
      "learning_rate": 3.09415337889142e-06,
      "loss": 1.103,
      "step": 8150
    },
    {
      "epoch": 1.5565679574791194,
      "grad_norm": 8.504629135131836,
      "learning_rate": 3.1131359149582386e-06,
      "loss": 1.1712,
      "step": 8200
    },
    {
      "epoch": 1.5660592255125285,
      "grad_norm": 8.453020095825195,
      "learning_rate": 3.1321184510250575e-06,
      "loss": 1.134,
      "step": 8250
    },
    {
      "epoch": 1.5755504935459377,
      "grad_norm": 10.963301658630371,
      "learning_rate": 3.1511009870918756e-06,
      "loss": 1.1265,
      "step": 8300
    },
    {
      "epoch": 1.585041761579347,
      "grad_norm": 11.298511505126953,
      "learning_rate": 3.170083523158694e-06,
      "loss": 1.1353,
      "step": 8350
    },
    {
      "epoch": 1.5945330296127562,
      "grad_norm": 8.937603950500488,
      "learning_rate": 3.189066059225513e-06,
      "loss": 1.1502,
      "step": 8400
    },
    {
      "epoch": 1.6040242976461654,
      "grad_norm": 6.993666172027588,
      "learning_rate": 3.208048595292331e-06,
      "loss": 1.1709,
      "step": 8450
    },
    {
      "epoch": 1.6135155656795748,
      "grad_norm": 8.333600997924805,
      "learning_rate": 3.2270311313591497e-06,
      "loss": 1.1268,
      "step": 8500
    },
    {
      "epoch": 1.6230068337129842,
      "grad_norm": 9.15424919128418,
      "learning_rate": 3.2460136674259683e-06,
      "loss": 1.1556,
      "step": 8550
    },
    {
      "epoch": 1.6324981017463933,
      "grad_norm": 17.349645614624023,
      "learning_rate": 3.264996203492787e-06,
      "loss": 1.1123,
      "step": 8600
    },
    {
      "epoch": 1.6419893697798025,
      "grad_norm": 8.872285842895508,
      "learning_rate": 3.2839787395596057e-06,
      "loss": 1.1356,
      "step": 8650
    },
    {
      "epoch": 1.6514806378132119,
      "grad_norm": 8.16823673248291,
      "learning_rate": 3.302961275626424e-06,
      "loss": 1.0176,
      "step": 8700
    },
    {
      "epoch": 1.6609719058466212,
      "grad_norm": 7.7541117668151855,
      "learning_rate": 3.3219438116932424e-06,
      "loss": 1.1424,
      "step": 8750
    },
    {
      "epoch": 1.6704631738800304,
      "grad_norm": 6.881016254425049,
      "learning_rate": 3.3409263477600613e-06,
      "loss": 1.1307,
      "step": 8800
    },
    {
      "epoch": 1.6799544419134396,
      "grad_norm": 8.083966255187988,
      "learning_rate": 3.3599088838268794e-06,
      "loss": 1.1504,
      "step": 8850
    },
    {
      "epoch": 1.689445709946849,
      "grad_norm": 8.695137023925781,
      "learning_rate": 3.378891419893698e-06,
      "loss": 1.0736,
      "step": 8900
    },
    {
      "epoch": 1.6989369779802581,
      "grad_norm": 8.855749130249023,
      "learning_rate": 3.397873955960517e-06,
      "loss": 1.1542,
      "step": 8950
    },
    {
      "epoch": 1.7084282460136673,
      "grad_norm": 20.272502899169922,
      "learning_rate": 3.416856492027335e-06,
      "loss": 1.096,
      "step": 9000
    },
    {
      "epoch": 1.7179195140470767,
      "grad_norm": 11.040966987609863,
      "learning_rate": 3.435839028094154e-06,
      "loss": 1.1346,
      "step": 9050
    },
    {
      "epoch": 1.727410782080486,
      "grad_norm": 5.113995552062988,
      "learning_rate": 3.454821564160972e-06,
      "loss": 1.1019,
      "step": 9100
    },
    {
      "epoch": 1.7369020501138952,
      "grad_norm": 11.643346786499023,
      "learning_rate": 3.4738041002277906e-06,
      "loss": 1.0748,
      "step": 9150
    },
    {
      "epoch": 1.7463933181473044,
      "grad_norm": 10.62111759185791,
      "learning_rate": 3.4927866362946096e-06,
      "loss": 1.1142,
      "step": 9200
    },
    {
      "epoch": 1.7558845861807137,
      "grad_norm": 8.423551559448242,
      "learning_rate": 3.5117691723614277e-06,
      "loss": 1.0018,
      "step": 9250
    },
    {
      "epoch": 1.7653758542141231,
      "grad_norm": 10.693641662597656,
      "learning_rate": 3.530751708428246e-06,
      "loss": 1.0869,
      "step": 9300
    },
    {
      "epoch": 1.7748671222475323,
      "grad_norm": 10.273809432983398,
      "learning_rate": 3.549734244495065e-06,
      "loss": 1.0726,
      "step": 9350
    },
    {
      "epoch": 1.7843583902809415,
      "grad_norm": 6.998386383056641,
      "learning_rate": 3.5687167805618832e-06,
      "loss": 1.1555,
      "step": 9400
    },
    {
      "epoch": 1.7938496583143508,
      "grad_norm": 6.818355083465576,
      "learning_rate": 3.5876993166287018e-06,
      "loss": 1.077,
      "step": 9450
    },
    {
      "epoch": 1.8033409263477602,
      "grad_norm": 12.327679634094238,
      "learning_rate": 3.6066818526955207e-06,
      "loss": 1.1372,
      "step": 9500
    },
    {
      "epoch": 1.8128321943811692,
      "grad_norm": 12.766919136047363,
      "learning_rate": 3.625664388762339e-06,
      "loss": 1.088,
      "step": 9550
    },
    {
      "epoch": 1.8223234624145785,
      "grad_norm": 7.449977397918701,
      "learning_rate": 3.6446469248291578e-06,
      "loss": 1.0675,
      "step": 9600
    },
    {
      "epoch": 1.831814730447988,
      "grad_norm": 11.831316947937012,
      "learning_rate": 3.663629460895976e-06,
      "loss": 1.1741,
      "step": 9650
    },
    {
      "epoch": 1.841305998481397,
      "grad_norm": 15.062392234802246,
      "learning_rate": 3.6826119969627944e-06,
      "loss": 1.0829,
      "step": 9700
    },
    {
      "epoch": 1.8507972665148062,
      "grad_norm": 6.773420810699463,
      "learning_rate": 3.7015945330296134e-06,
      "loss": 1.0558,
      "step": 9750
    },
    {
      "epoch": 1.8602885345482156,
      "grad_norm": 9.245349884033203,
      "learning_rate": 3.7205770690964315e-06,
      "loss": 1.118,
      "step": 9800
    },
    {
      "epoch": 1.869779802581625,
      "grad_norm": 16.190425872802734,
      "learning_rate": 3.73955960516325e-06,
      "loss": 0.98,
      "step": 9850
    },
    {
      "epoch": 1.8792710706150342,
      "grad_norm": 5.989975929260254,
      "learning_rate": 3.758542141230069e-06,
      "loss": 1.0671,
      "step": 9900
    },
    {
      "epoch": 1.8887623386484433,
      "grad_norm": 10.689846992492676,
      "learning_rate": 3.777524677296887e-06,
      "loss": 1.1324,
      "step": 9950
    },
    {
      "epoch": 1.8982536066818527,
      "grad_norm": 7.839616775512695,
      "learning_rate": 3.796507213363706e-06,
      "loss": 1.0502,
      "step": 10000
    },
    {
      "epoch": 1.907744874715262,
      "grad_norm": 11.228259086608887,
      "learning_rate": 3.815489749430524e-06,
      "loss": 1.0626,
      "step": 10050
    },
    {
      "epoch": 1.9172361427486713,
      "grad_norm": 8.731637001037598,
      "learning_rate": 3.834472285497343e-06,
      "loss": 1.0557,
      "step": 10100
    },
    {
      "epoch": 1.9267274107820804,
      "grad_norm": 7.369215488433838,
      "learning_rate": 3.853454821564161e-06,
      "loss": 1.0551,
      "step": 10150
    },
    {
      "epoch": 1.9362186788154898,
      "grad_norm": 13.776960372924805,
      "learning_rate": 3.87243735763098e-06,
      "loss": 0.9098,
      "step": 10200
    },
    {
      "epoch": 1.945709946848899,
      "grad_norm": 14.922910690307617,
      "learning_rate": 3.891419893697798e-06,
      "loss": 1.0094,
      "step": 10250
    },
    {
      "epoch": 1.9552012148823081,
      "grad_norm": 14.49189281463623,
      "learning_rate": 3.910402429764617e-06,
      "loss": 1.0408,
      "step": 10300
    },
    {
      "epoch": 1.9646924829157175,
      "grad_norm": 8.074590682983398,
      "learning_rate": 3.929384965831435e-06,
      "loss": 1.0013,
      "step": 10350
    },
    {
      "epoch": 1.974183750949127,
      "grad_norm": 13.741362571716309,
      "learning_rate": 3.948367501898254e-06,
      "loss": 1.0558,
      "step": 10400
    },
    {
      "epoch": 1.983675018982536,
      "grad_norm": 8.54836368560791,
      "learning_rate": 3.967350037965072e-06,
      "loss": 1.1325,
      "step": 10450
    },
    {
      "epoch": 1.9931662870159452,
      "grad_norm": 8.981844902038574,
      "learning_rate": 3.986332574031891e-06,
      "loss": 1.025,
      "step": 10500
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6111796526525577,
      "eval_f1": 0.3383157578598897,
      "eval_loss": 0.9857298731803894,
      "eval_precision": 0.32815693863098555,
      "eval_recall": 0.38700316043361305,
      "eval_runtime": 171.5949,
      "eval_samples_per_second": 61.406,
      "eval_steps_per_second": 7.681,
      "step": 10536
    },
    {
      "epoch": 2.0026575550493546,
      "grad_norm": 8.053272247314453,
      "learning_rate": 4.005315110098709e-06,
      "loss": 1.0352,
      "step": 10550
    },
    {
      "epoch": 2.012148823082764,
      "grad_norm": 6.401072025299072,
      "learning_rate": 4.024297646165528e-06,
      "loss": 0.9993,
      "step": 10600
    },
    {
      "epoch": 2.021640091116173,
      "grad_norm": 8.908872604370117,
      "learning_rate": 4.0432801822323464e-06,
      "loss": 0.9309,
      "step": 10650
    },
    {
      "epoch": 2.0311313591495823,
      "grad_norm": 18.409168243408203,
      "learning_rate": 4.062262718299165e-06,
      "loss": 1.0781,
      "step": 10700
    },
    {
      "epoch": 2.0406226271829917,
      "grad_norm": 8.006268501281738,
      "learning_rate": 4.0812452543659835e-06,
      "loss": 0.9994,
      "step": 10750
    },
    {
      "epoch": 2.050113895216401,
      "grad_norm": 12.994401931762695,
      "learning_rate": 4.100227790432802e-06,
      "loss": 0.9454,
      "step": 10800
    },
    {
      "epoch": 2.05960516324981,
      "grad_norm": 13.9449462890625,
      "learning_rate": 4.1192103264996205e-06,
      "loss": 0.9682,
      "step": 10850
    },
    {
      "epoch": 2.0690964312832194,
      "grad_norm": 10.504644393920898,
      "learning_rate": 4.138192862566439e-06,
      "loss": 0.9618,
      "step": 10900
    },
    {
      "epoch": 2.078587699316629,
      "grad_norm": 15.46628475189209,
      "learning_rate": 4.157175398633258e-06,
      "loss": 1.0413,
      "step": 10950
    },
    {
      "epoch": 2.088078967350038,
      "grad_norm": 18.973779678344727,
      "learning_rate": 4.176157934700076e-06,
      "loss": 1.0523,
      "step": 11000
    },
    {
      "epoch": 2.097570235383447,
      "grad_norm": 13.982255935668945,
      "learning_rate": 4.195140470766895e-06,
      "loss": 1.0198,
      "step": 11050
    },
    {
      "epoch": 2.1070615034168565,
      "grad_norm": 5.509376525878906,
      "learning_rate": 4.214123006833713e-06,
      "loss": 0.9973,
      "step": 11100
    },
    {
      "epoch": 2.116552771450266,
      "grad_norm": 6.099702835083008,
      "learning_rate": 4.233105542900532e-06,
      "loss": 0.9418,
      "step": 11150
    },
    {
      "epoch": 2.126044039483675,
      "grad_norm": 22.023571014404297,
      "learning_rate": 4.25208807896735e-06,
      "loss": 1.0543,
      "step": 11200
    },
    {
      "epoch": 2.135535307517084,
      "grad_norm": 13.960505485534668,
      "learning_rate": 4.271070615034169e-06,
      "loss": 1.0449,
      "step": 11250
    },
    {
      "epoch": 2.1450265755504936,
      "grad_norm": 16.882123947143555,
      "learning_rate": 4.290053151100987e-06,
      "loss": 0.9795,
      "step": 11300
    },
    {
      "epoch": 2.154517843583903,
      "grad_norm": 14.241255760192871,
      "learning_rate": 4.309035687167806e-06,
      "loss": 0.975,
      "step": 11350
    },
    {
      "epoch": 2.164009111617312,
      "grad_norm": 17.469114303588867,
      "learning_rate": 4.328018223234624e-06,
      "loss": 0.9592,
      "step": 11400
    },
    {
      "epoch": 2.1735003796507213,
      "grad_norm": 19.448890686035156,
      "learning_rate": 4.347000759301443e-06,
      "loss": 1.0652,
      "step": 11450
    },
    {
      "epoch": 2.1829916476841307,
      "grad_norm": 17.956144332885742,
      "learning_rate": 4.365983295368261e-06,
      "loss": 0.9862,
      "step": 11500
    },
    {
      "epoch": 2.19248291571754,
      "grad_norm": 6.523434638977051,
      "learning_rate": 4.38496583143508e-06,
      "loss": 1.0048,
      "step": 11550
    },
    {
      "epoch": 2.201974183750949,
      "grad_norm": 15.391818046569824,
      "learning_rate": 4.4039483675018985e-06,
      "loss": 1.0413,
      "step": 11600
    },
    {
      "epoch": 2.2114654517843584,
      "grad_norm": 7.022346019744873,
      "learning_rate": 4.422930903568717e-06,
      "loss": 1.0113,
      "step": 11650
    },
    {
      "epoch": 2.2209567198177678,
      "grad_norm": 10.486368179321289,
      "learning_rate": 4.4419134396355355e-06,
      "loss": 0.9765,
      "step": 11700
    },
    {
      "epoch": 2.2304479878511767,
      "grad_norm": 12.600993156433105,
      "learning_rate": 4.460895975702354e-06,
      "loss": 0.9611,
      "step": 11750
    },
    {
      "epoch": 2.239939255884586,
      "grad_norm": 21.893531799316406,
      "learning_rate": 4.4798785117691726e-06,
      "loss": 1.0058,
      "step": 11800
    },
    {
      "epoch": 2.2494305239179955,
      "grad_norm": 10.432188034057617,
      "learning_rate": 4.498861047835991e-06,
      "loss": 0.8984,
      "step": 11850
    },
    {
      "epoch": 2.258921791951405,
      "grad_norm": 12.501123428344727,
      "learning_rate": 4.51784358390281e-06,
      "loss": 0.9922,
      "step": 11900
    },
    {
      "epoch": 2.268413059984814,
      "grad_norm": 11.425581932067871,
      "learning_rate": 4.536826119969628e-06,
      "loss": 1.0186,
      "step": 11950
    },
    {
      "epoch": 2.277904328018223,
      "grad_norm": 4.125765323638916,
      "learning_rate": 4.555808656036447e-06,
      "loss": 1.0221,
      "step": 12000
    },
    {
      "epoch": 2.2873955960516326,
      "grad_norm": 24.960241317749023,
      "learning_rate": 4.574791192103265e-06,
      "loss": 1.0535,
      "step": 12050
    },
    {
      "epoch": 2.296886864085042,
      "grad_norm": 5.319580554962158,
      "learning_rate": 4.593773728170084e-06,
      "loss": 0.9933,
      "step": 12100
    },
    {
      "epoch": 2.306378132118451,
      "grad_norm": 5.7121148109436035,
      "learning_rate": 4.612756264236902e-06,
      "loss": 0.9485,
      "step": 12150
    },
    {
      "epoch": 2.3158694001518603,
      "grad_norm": 10.599714279174805,
      "learning_rate": 4.631738800303721e-06,
      "loss": 1.0054,
      "step": 12200
    },
    {
      "epoch": 2.3253606681852697,
      "grad_norm": 9.165621757507324,
      "learning_rate": 4.650721336370539e-06,
      "loss": 0.9181,
      "step": 12250
    },
    {
      "epoch": 2.334851936218679,
      "grad_norm": 9.547904014587402,
      "learning_rate": 4.669703872437358e-06,
      "loss": 0.9029,
      "step": 12300
    },
    {
      "epoch": 2.344343204252088,
      "grad_norm": 15.514917373657227,
      "learning_rate": 4.688686408504176e-06,
      "loss": 0.9714,
      "step": 12350
    },
    {
      "epoch": 2.3538344722854974,
      "grad_norm": 5.501564025878906,
      "learning_rate": 4.707668944570995e-06,
      "loss": 0.8902,
      "step": 12400
    },
    {
      "epoch": 2.3633257403189067,
      "grad_norm": 24.39622688293457,
      "learning_rate": 4.726651480637814e-06,
      "loss": 0.9044,
      "step": 12450
    },
    {
      "epoch": 2.3728170083523157,
      "grad_norm": 6.758551597595215,
      "learning_rate": 4.745634016704632e-06,
      "loss": 0.9038,
      "step": 12500
    },
    {
      "epoch": 2.382308276385725,
      "grad_norm": 5.8299641609191895,
      "learning_rate": 4.7646165527714505e-06,
      "loss": 0.9166,
      "step": 12550
    },
    {
      "epoch": 2.3917995444191344,
      "grad_norm": 14.342028617858887,
      "learning_rate": 4.783599088838269e-06,
      "loss": 1.0343,
      "step": 12600
    },
    {
      "epoch": 2.401290812452544,
      "grad_norm": 9.4110107421875,
      "learning_rate": 4.8025816249050875e-06,
      "loss": 0.9538,
      "step": 12650
    },
    {
      "epoch": 2.4107820804859528,
      "grad_norm": 24.431760787963867,
      "learning_rate": 4.821564160971906e-06,
      "loss": 0.9065,
      "step": 12700
    },
    {
      "epoch": 2.420273348519362,
      "grad_norm": 17.534738540649414,
      "learning_rate": 4.840546697038725e-06,
      "loss": 0.9507,
      "step": 12750
    },
    {
      "epoch": 2.4297646165527715,
      "grad_norm": 7.148378849029541,
      "learning_rate": 4.859529233105543e-06,
      "loss": 0.8933,
      "step": 12800
    },
    {
      "epoch": 2.4392558845861805,
      "grad_norm": 14.349488258361816,
      "learning_rate": 4.878511769172362e-06,
      "loss": 1.0324,
      "step": 12850
    },
    {
      "epoch": 2.44874715261959,
      "grad_norm": 12.241732597351074,
      "learning_rate": 4.89749430523918e-06,
      "loss": 0.9053,
      "step": 12900
    },
    {
      "epoch": 2.4582384206529992,
      "grad_norm": 12.187847137451172,
      "learning_rate": 4.916476841305999e-06,
      "loss": 0.9049,
      "step": 12950
    },
    {
      "epoch": 2.4677296886864086,
      "grad_norm": 5.260674476623535,
      "learning_rate": 4.935459377372817e-06,
      "loss": 0.9509,
      "step": 13000
    },
    {
      "epoch": 2.477220956719818,
      "grad_norm": 19.768796920776367,
      "learning_rate": 4.954441913439636e-06,
      "loss": 0.9798,
      "step": 13050
    },
    {
      "epoch": 2.486712224753227,
      "grad_norm": 14.84368896484375,
      "learning_rate": 4.973424449506454e-06,
      "loss": 0.9732,
      "step": 13100
    },
    {
      "epoch": 2.4962034927866363,
      "grad_norm": 7.285784721374512,
      "learning_rate": 4.992406985573273e-06,
      "loss": 0.9158,
      "step": 13150
    },
    {
      "epoch": 2.5056947608200457,
      "grad_norm": 10.427937507629395,
      "learning_rate": 5.011389521640092e-06,
      "loss": 1.004,
      "step": 13200
    },
    {
      "epoch": 2.5151860288534547,
      "grad_norm": 25.47098731994629,
      "learning_rate": 5.030372057706911e-06,
      "loss": 0.9438,
      "step": 13250
    },
    {
      "epoch": 2.524677296886864,
      "grad_norm": 20.943105697631836,
      "learning_rate": 5.049354593773728e-06,
      "loss": 1.0211,
      "step": 13300
    },
    {
      "epoch": 2.5341685649202734,
      "grad_norm": 21.593299865722656,
      "learning_rate": 5.068337129840547e-06,
      "loss": 0.9352,
      "step": 13350
    },
    {
      "epoch": 2.543659832953683,
      "grad_norm": 5.397031784057617,
      "learning_rate": 5.0873196659073655e-06,
      "loss": 0.9186,
      "step": 13400
    },
    {
      "epoch": 2.5531511009870917,
      "grad_norm": 9.584195137023926,
      "learning_rate": 5.106302201974184e-06,
      "loss": 0.8648,
      "step": 13450
    },
    {
      "epoch": 2.562642369020501,
      "grad_norm": 11.039591789245605,
      "learning_rate": 5.125284738041003e-06,
      "loss": 0.9105,
      "step": 13500
    },
    {
      "epoch": 2.5721336370539105,
      "grad_norm": 25.08954620361328,
      "learning_rate": 5.144267274107821e-06,
      "loss": 0.9889,
      "step": 13550
    },
    {
      "epoch": 2.5816249050873195,
      "grad_norm": 8.909172058105469,
      "learning_rate": 5.16324981017464e-06,
      "loss": 0.9447,
      "step": 13600
    },
    {
      "epoch": 2.591116173120729,
      "grad_norm": 5.847607135772705,
      "learning_rate": 5.182232346241458e-06,
      "loss": 0.9235,
      "step": 13650
    },
    {
      "epoch": 2.600607441154138,
      "grad_norm": 12.02507209777832,
      "learning_rate": 5.201214882308277e-06,
      "loss": 0.8637,
      "step": 13700
    },
    {
      "epoch": 2.6100987091875476,
      "grad_norm": 17.63705825805664,
      "learning_rate": 5.220197418375096e-06,
      "loss": 0.9516,
      "step": 13750
    },
    {
      "epoch": 2.619589977220957,
      "grad_norm": 6.873347759246826,
      "learning_rate": 5.2391799544419145e-06,
      "loss": 0.9503,
      "step": 13800
    },
    {
      "epoch": 2.629081245254366,
      "grad_norm": 3.940760374069214,
      "learning_rate": 5.258162490508732e-06,
      "loss": 0.88,
      "step": 13850
    },
    {
      "epoch": 2.6385725132877753,
      "grad_norm": 6.009947776794434,
      "learning_rate": 5.277145026575551e-06,
      "loss": 0.9167,
      "step": 13900
    },
    {
      "epoch": 2.6480637813211843,
      "grad_norm": 16.393348693847656,
      "learning_rate": 5.296127562642369e-06,
      "loss": 0.8733,
      "step": 13950
    },
    {
      "epoch": 2.6575550493545936,
      "grad_norm": 11.104180335998535,
      "learning_rate": 5.315110098709188e-06,
      "loss": 0.9286,
      "step": 14000
    },
    {
      "epoch": 2.667046317388003,
      "grad_norm": 21.233993530273438,
      "learning_rate": 5.334092634776007e-06,
      "loss": 0.927,
      "step": 14050
    },
    {
      "epoch": 2.6765375854214124,
      "grad_norm": 11.242561340332031,
      "learning_rate": 5.353075170842825e-06,
      "loss": 0.9426,
      "step": 14100
    },
    {
      "epoch": 2.686028853454822,
      "grad_norm": 5.567835807800293,
      "learning_rate": 5.372057706909643e-06,
      "loss": 0.8982,
      "step": 14150
    },
    {
      "epoch": 2.6955201214882307,
      "grad_norm": 18.347885131835938,
      "learning_rate": 5.391040242976462e-06,
      "loss": 0.9318,
      "step": 14200
    },
    {
      "epoch": 2.70501138952164,
      "grad_norm": 36.40949630737305,
      "learning_rate": 5.4100227790432804e-06,
      "loss": 0.9532,
      "step": 14250
    },
    {
      "epoch": 2.7145026575550495,
      "grad_norm": 29.326385498046875,
      "learning_rate": 5.4290053151101e-06,
      "loss": 0.8995,
      "step": 14300
    },
    {
      "epoch": 2.7239939255884584,
      "grad_norm": 2.1680731773376465,
      "learning_rate": 5.447987851176918e-06,
      "loss": 0.9124,
      "step": 14350
    },
    {
      "epoch": 2.733485193621868,
      "grad_norm": 10.704232215881348,
      "learning_rate": 5.466970387243736e-06,
      "loss": 0.8577,
      "step": 14400
    },
    {
      "epoch": 2.742976461655277,
      "grad_norm": 12.967594146728516,
      "learning_rate": 5.4859529233105546e-06,
      "loss": 0.8849,
      "step": 14450
    },
    {
      "epoch": 2.7524677296886866,
      "grad_norm": 20.072662353515625,
      "learning_rate": 5.504935459377373e-06,
      "loss": 0.9293,
      "step": 14500
    },
    {
      "epoch": 2.7619589977220955,
      "grad_norm": 9.76231575012207,
      "learning_rate": 5.523917995444192e-06,
      "loss": 0.9103,
      "step": 14550
    },
    {
      "epoch": 2.771450265755505,
      "grad_norm": 17.904836654663086,
      "learning_rate": 5.542900531511011e-06,
      "loss": 0.8766,
      "step": 14600
    },
    {
      "epoch": 2.7809415337889143,
      "grad_norm": 12.68100357055664,
      "learning_rate": 5.561883067577829e-06,
      "loss": 0.868,
      "step": 14650
    },
    {
      "epoch": 2.7904328018223232,
      "grad_norm": 8.943753242492676,
      "learning_rate": 5.580865603644647e-06,
      "loss": 0.9286,
      "step": 14700
    },
    {
      "epoch": 2.7999240698557326,
      "grad_norm": 22.137622833251953,
      "learning_rate": 5.599848139711466e-06,
      "loss": 0.8854,
      "step": 14750
    },
    {
      "epoch": 2.809415337889142,
      "grad_norm": 24.715463638305664,
      "learning_rate": 5.618830675778284e-06,
      "loss": 0.8831,
      "step": 14800
    },
    {
      "epoch": 2.8189066059225514,
      "grad_norm": 17.743698120117188,
      "learning_rate": 5.637813211845104e-06,
      "loss": 0.8238,
      "step": 14850
    },
    {
      "epoch": 2.8283978739559608,
      "grad_norm": 8.613927841186523,
      "learning_rate": 5.656795747911922e-06,
      "loss": 0.9054,
      "step": 14900
    },
    {
      "epoch": 2.8378891419893697,
      "grad_norm": 15.881367683410645,
      "learning_rate": 5.67577828397874e-06,
      "loss": 0.8643,
      "step": 14950
    },
    {
      "epoch": 2.847380410022779,
      "grad_norm": 7.400337219238281,
      "learning_rate": 5.694760820045558e-06,
      "loss": 0.9015,
      "step": 15000
    },
    {
      "epoch": 2.8568716780561885,
      "grad_norm": 11.40087604522705,
      "learning_rate": 5.713743356112377e-06,
      "loss": 0.8406,
      "step": 15050
    },
    {
      "epoch": 2.8663629460895974,
      "grad_norm": 4.264243125915527,
      "learning_rate": 5.732725892179195e-06,
      "loss": 0.9179,
      "step": 15100
    },
    {
      "epoch": 2.875854214123007,
      "grad_norm": 11.606842994689941,
      "learning_rate": 5.751708428246015e-06,
      "loss": 0.9134,
      "step": 15150
    },
    {
      "epoch": 2.885345482156416,
      "grad_norm": 10.16572380065918,
      "learning_rate": 5.7706909643128325e-06,
      "loss": 0.8177,
      "step": 15200
    },
    {
      "epoch": 2.8948367501898256,
      "grad_norm": 12.598143577575684,
      "learning_rate": 5.789673500379651e-06,
      "loss": 0.8407,
      "step": 15250
    },
    {
      "epoch": 2.9043280182232345,
      "grad_norm": 6.530345916748047,
      "learning_rate": 5.8086560364464695e-06,
      "loss": 0.8022,
      "step": 15300
    },
    {
      "epoch": 2.913819286256644,
      "grad_norm": 16.85447883605957,
      "learning_rate": 5.827638572513288e-06,
      "loss": 0.8925,
      "step": 15350
    },
    {
      "epoch": 2.9233105542900533,
      "grad_norm": 14.597683906555176,
      "learning_rate": 5.8466211085801074e-06,
      "loss": 0.9195,
      "step": 15400
    },
    {
      "epoch": 2.932801822323462,
      "grad_norm": 11.083624839782715,
      "learning_rate": 5.865603644646926e-06,
      "loss": 0.998,
      "step": 15450
    },
    {
      "epoch": 2.9422930903568716,
      "grad_norm": 5.017083168029785,
      "learning_rate": 5.884586180713744e-06,
      "loss": 0.9216,
      "step": 15500
    },
    {
      "epoch": 2.951784358390281,
      "grad_norm": 13.89981746673584,
      "learning_rate": 5.903568716780562e-06,
      "loss": 0.8791,
      "step": 15550
    },
    {
      "epoch": 2.9612756264236904,
      "grad_norm": 29.379423141479492,
      "learning_rate": 5.922551252847381e-06,
      "loss": 0.8502,
      "step": 15600
    },
    {
      "epoch": 2.9707668944570997,
      "grad_norm": 11.952815055847168,
      "learning_rate": 5.9415337889142e-06,
      "loss": 0.9269,
      "step": 15650
    },
    {
      "epoch": 2.9802581624905087,
      "grad_norm": 12.616369247436523,
      "learning_rate": 5.960516324981019e-06,
      "loss": 0.9211,
      "step": 15700
    },
    {
      "epoch": 2.989749430523918,
      "grad_norm": 4.125850200653076,
      "learning_rate": 5.979498861047836e-06,
      "loss": 0.8543,
      "step": 15750
    },
    {
      "epoch": 2.999240698557327,
      "grad_norm": 8.022698402404785,
      "learning_rate": 5.998481397114655e-06,
      "loss": 0.9157,
      "step": 15800
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.6592009110752586,
      "eval_f1": 0.4567417197688027,
      "eval_loss": 0.8483139276504517,
      "eval_precision": 0.47811200573693696,
      "eval_recall": 0.4885486975096485,
      "eval_runtime": 178.9489,
      "eval_samples_per_second": 58.883,
      "eval_steps_per_second": 7.365,
      "step": 15804
    },
    {
      "epoch": 3.0087319665907364,
      "grad_norm": 5.997342586517334,
      "learning_rate": 6.017463933181473e-06,
      "loss": 0.8226,
      "step": 15850
    },
    {
      "epoch": 3.0182232346241458,
      "grad_norm": 24.48729705810547,
      "learning_rate": 6.036446469248292e-06,
      "loss": 0.8325,
      "step": 15900
    },
    {
      "epoch": 3.027714502657555,
      "grad_norm": 12.83834457397461,
      "learning_rate": 6.055429005315111e-06,
      "loss": 0.8701,
      "step": 15950
    },
    {
      "epoch": 3.0372057706909645,
      "grad_norm": 32.75111389160156,
      "learning_rate": 6.07441154138193e-06,
      "loss": 0.8171,
      "step": 16000
    },
    {
      "epoch": 3.0466970387243735,
      "grad_norm": 11.756657600402832,
      "learning_rate": 6.0933940774487474e-06,
      "loss": 0.861,
      "step": 16050
    },
    {
      "epoch": 3.056188306757783,
      "grad_norm": 10.750813484191895,
      "learning_rate": 6.112376613515566e-06,
      "loss": 0.8855,
      "step": 16100
    },
    {
      "epoch": 3.0656795747911922,
      "grad_norm": 14.193553924560547,
      "learning_rate": 6.1313591495823845e-06,
      "loss": 0.84,
      "step": 16150
    },
    {
      "epoch": 3.075170842824601,
      "grad_norm": 9.536857604980469,
      "learning_rate": 6.150341685649204e-06,
      "loss": 0.8989,
      "step": 16200
    },
    {
      "epoch": 3.0846621108580106,
      "grad_norm": 17.304798126220703,
      "learning_rate": 6.169324221716022e-06,
      "loss": 0.866,
      "step": 16250
    },
    {
      "epoch": 3.09415337889142,
      "grad_norm": 18.733915328979492,
      "learning_rate": 6.18830675778284e-06,
      "loss": 0.875,
      "step": 16300
    },
    {
      "epoch": 3.1036446469248293,
      "grad_norm": 6.891103744506836,
      "learning_rate": 6.207289293849659e-06,
      "loss": 0.8241,
      "step": 16350
    },
    {
      "epoch": 3.1131359149582383,
      "grad_norm": 10.04080581665039,
      "learning_rate": 6.226271829916477e-06,
      "loss": 0.9091,
      "step": 16400
    },
    {
      "epoch": 3.1226271829916477,
      "grad_norm": 14.0333251953125,
      "learning_rate": 6.245254365983296e-06,
      "loss": 0.7884,
      "step": 16450
    },
    {
      "epoch": 3.132118451025057,
      "grad_norm": 16.61358642578125,
      "learning_rate": 6.264236902050115e-06,
      "loss": 0.8353,
      "step": 16500
    },
    {
      "epoch": 3.141609719058466,
      "grad_norm": 4.471676826477051,
      "learning_rate": 6.283219438116933e-06,
      "loss": 0.8904,
      "step": 16550
    },
    {
      "epoch": 3.1511009870918754,
      "grad_norm": 20.4199161529541,
      "learning_rate": 6.302201974183751e-06,
      "loss": 0.8378,
      "step": 16600
    },
    {
      "epoch": 3.1605922551252847,
      "grad_norm": 19.12438201904297,
      "learning_rate": 6.32118451025057e-06,
      "loss": 0.9128,
      "step": 16650
    },
    {
      "epoch": 3.170083523158694,
      "grad_norm": 7.2920918464660645,
      "learning_rate": 6.340167046317388e-06,
      "loss": 0.8988,
      "step": 16700
    },
    {
      "epoch": 3.179574791192103,
      "grad_norm": 14.303485870361328,
      "learning_rate": 6.359149582384208e-06,
      "loss": 0.8257,
      "step": 16750
    },
    {
      "epoch": 3.1890660592255125,
      "grad_norm": 17.748291015625,
      "learning_rate": 6.378132118451026e-06,
      "loss": 0.8944,
      "step": 16800
    },
    {
      "epoch": 3.198557327258922,
      "grad_norm": 7.693470478057861,
      "learning_rate": 6.397114654517844e-06,
      "loss": 0.8192,
      "step": 16850
    },
    {
      "epoch": 3.208048595292331,
      "grad_norm": 16.337238311767578,
      "learning_rate": 6.416097190584662e-06,
      "loss": 0.8521,
      "step": 16900
    },
    {
      "epoch": 3.21753986332574,
      "grad_norm": 18.741249084472656,
      "learning_rate": 6.435079726651481e-06,
      "loss": 0.7989,
      "step": 16950
    },
    {
      "epoch": 3.2270311313591495,
      "grad_norm": 23.731075286865234,
      "learning_rate": 6.4540622627182995e-06,
      "loss": 0.8399,
      "step": 17000
    },
    {
      "epoch": 3.236522399392559,
      "grad_norm": 24.212236404418945,
      "learning_rate": 6.473044798785119e-06,
      "loss": 0.854,
      "step": 17050
    },
    {
      "epoch": 3.2460136674259683,
      "grad_norm": 12.772549629211426,
      "learning_rate": 6.4920273348519365e-06,
      "loss": 0.7862,
      "step": 17100
    },
    {
      "epoch": 3.2555049354593772,
      "grad_norm": 12.787792205810547,
      "learning_rate": 6.511009870918755e-06,
      "loss": 0.7595,
      "step": 17150
    },
    {
      "epoch": 3.2649962034927866,
      "grad_norm": 12.363590240478516,
      "learning_rate": 6.529992406985574e-06,
      "loss": 0.7687,
      "step": 17200
    },
    {
      "epoch": 3.274487471526196,
      "grad_norm": 6.721357345581055,
      "learning_rate": 6.548974943052392e-06,
      "loss": 0.8439,
      "step": 17250
    },
    {
      "epoch": 3.283978739559605,
      "grad_norm": 13.997309684753418,
      "learning_rate": 6.5679574791192115e-06,
      "loss": 0.8081,
      "step": 17300
    },
    {
      "epoch": 3.2934700075930143,
      "grad_norm": 30.56717300415039,
      "learning_rate": 6.58694001518603e-06,
      "loss": 0.862,
      "step": 17350
    },
    {
      "epoch": 3.3029612756264237,
      "grad_norm": 9.836735725402832,
      "learning_rate": 6.605922551252848e-06,
      "loss": 0.8419,
      "step": 17400
    },
    {
      "epoch": 3.312452543659833,
      "grad_norm": 19.659711837768555,
      "learning_rate": 6.624905087319666e-06,
      "loss": 0.775,
      "step": 17450
    },
    {
      "epoch": 3.321943811693242,
      "grad_norm": 10.509561538696289,
      "learning_rate": 6.643887623386485e-06,
      "loss": 0.8521,
      "step": 17500
    },
    {
      "epoch": 3.3314350797266514,
      "grad_norm": 12.767932891845703,
      "learning_rate": 6.662870159453303e-06,
      "loss": 0.855,
      "step": 17550
    },
    {
      "epoch": 3.340926347760061,
      "grad_norm": 17.596858978271484,
      "learning_rate": 6.681852695520123e-06,
      "loss": 0.8339,
      "step": 17600
    },
    {
      "epoch": 3.35041761579347,
      "grad_norm": 23.406169891357422,
      "learning_rate": 6.70083523158694e-06,
      "loss": 0.7395,
      "step": 17650
    },
    {
      "epoch": 3.359908883826879,
      "grad_norm": 4.497544765472412,
      "learning_rate": 6.719817767653759e-06,
      "loss": 0.8464,
      "step": 17700
    },
    {
      "epoch": 3.3694001518602885,
      "grad_norm": 40.90102005004883,
      "learning_rate": 6.738800303720577e-06,
      "loss": 0.8081,
      "step": 17750
    },
    {
      "epoch": 3.378891419893698,
      "grad_norm": 19.229198455810547,
      "learning_rate": 6.757782839787396e-06,
      "loss": 0.8396,
      "step": 17800
    },
    {
      "epoch": 3.3883826879271073,
      "grad_norm": 6.52122688293457,
      "learning_rate": 6.776765375854215e-06,
      "loss": 0.9306,
      "step": 17850
    },
    {
      "epoch": 3.3978739559605162,
      "grad_norm": 9.627395629882812,
      "learning_rate": 6.795747911921034e-06,
      "loss": 0.8613,
      "step": 17900
    },
    {
      "epoch": 3.4073652239939256,
      "grad_norm": 25.42774772644043,
      "learning_rate": 6.8147304479878515e-06,
      "loss": 0.8171,
      "step": 17950
    },
    {
      "epoch": 3.416856492027335,
      "grad_norm": 17.591506958007812,
      "learning_rate": 6.83371298405467e-06,
      "loss": 0.8034,
      "step": 18000
    },
    {
      "epoch": 3.426347760060744,
      "grad_norm": 4.845323085784912,
      "learning_rate": 6.8526955201214886e-06,
      "loss": 0.8759,
      "step": 18050
    },
    {
      "epoch": 3.4358390280941533,
      "grad_norm": 11.12725830078125,
      "learning_rate": 6.871678056188308e-06,
      "loss": 0.8415,
      "step": 18100
    },
    {
      "epoch": 3.4453302961275627,
      "grad_norm": 11.668724060058594,
      "learning_rate": 6.8906605922551265e-06,
      "loss": 0.7611,
      "step": 18150
    },
    {
      "epoch": 3.454821564160972,
      "grad_norm": 12.761807441711426,
      "learning_rate": 6.909643128321944e-06,
      "loss": 0.8,
      "step": 18200
    },
    {
      "epoch": 3.464312832194381,
      "grad_norm": 18.002777099609375,
      "learning_rate": 6.928625664388763e-06,
      "loss": 0.7748,
      "step": 18250
    },
    {
      "epoch": 3.4738041002277904,
      "grad_norm": 11.603888511657715,
      "learning_rate": 6.947608200455581e-06,
      "loss": 0.8725,
      "step": 18300
    },
    {
      "epoch": 3.4832953682612,
      "grad_norm": 15.807866096496582,
      "learning_rate": 6.9665907365224e-06,
      "loss": 0.8671,
      "step": 18350
    },
    {
      "epoch": 3.4927866362946087,
      "grad_norm": 8.409049987792969,
      "learning_rate": 6.985573272589219e-06,
      "loss": 0.8368,
      "step": 18400
    },
    {
      "epoch": 3.502277904328018,
      "grad_norm": 10.034148216247559,
      "learning_rate": 7.004555808656038e-06,
      "loss": 0.8282,
      "step": 18450
    },
    {
      "epoch": 3.5117691723614275,
      "grad_norm": 13.656588554382324,
      "learning_rate": 7.023538344722855e-06,
      "loss": 0.8355,
      "step": 18500
    },
    {
      "epoch": 3.521260440394837,
      "grad_norm": 12.090187072753906,
      "learning_rate": 7.042520880789674e-06,
      "loss": 0.8207,
      "step": 18550
    },
    {
      "epoch": 3.5307517084282463,
      "grad_norm": 13.884026527404785,
      "learning_rate": 7.061503416856492e-06,
      "loss": 0.8674,
      "step": 18600
    },
    {
      "epoch": 3.540242976461655,
      "grad_norm": 18.692401885986328,
      "learning_rate": 7.080485952923312e-06,
      "loss": 0.7906,
      "step": 18650
    },
    {
      "epoch": 3.5497342444950646,
      "grad_norm": 21.117862701416016,
      "learning_rate": 7.09946848899013e-06,
      "loss": 0.9248,
      "step": 18700
    },
    {
      "epoch": 3.559225512528474,
      "grad_norm": 8.100530624389648,
      "learning_rate": 7.118451025056948e-06,
      "loss": 0.8401,
      "step": 18750
    },
    {
      "epoch": 3.568716780561883,
      "grad_norm": 12.483731269836426,
      "learning_rate": 7.1374335611237665e-06,
      "loss": 0.8803,
      "step": 18800
    },
    {
      "epoch": 3.5782080485952923,
      "grad_norm": 11.097543716430664,
      "learning_rate": 7.156416097190585e-06,
      "loss": 0.7843,
      "step": 18850
    },
    {
      "epoch": 3.5876993166287017,
      "grad_norm": 3.8094444274902344,
      "learning_rate": 7.1753986332574035e-06,
      "loss": 0.7935,
      "step": 18900
    },
    {
      "epoch": 3.597190584662111,
      "grad_norm": 7.1298508644104,
      "learning_rate": 7.194381169324223e-06,
      "loss": 0.8501,
      "step": 18950
    },
    {
      "epoch": 3.60668185269552,
      "grad_norm": 25.148744583129883,
      "learning_rate": 7.2133637053910414e-06,
      "loss": 0.7719,
      "step": 19000
    },
    {
      "epoch": 3.6161731207289294,
      "grad_norm": 13.403111457824707,
      "learning_rate": 7.232346241457859e-06,
      "loss": 0.8431,
      "step": 19050
    },
    {
      "epoch": 3.6256643887623388,
      "grad_norm": 31.748830795288086,
      "learning_rate": 7.251328777524678e-06,
      "loss": 0.831,
      "step": 19100
    },
    {
      "epoch": 3.6351556567957477,
      "grad_norm": 13.751605033874512,
      "learning_rate": 7.270311313591496e-06,
      "loss": 0.832,
      "step": 19150
    },
    {
      "epoch": 3.644646924829157,
      "grad_norm": 10.035350799560547,
      "learning_rate": 7.2892938496583155e-06,
      "loss": 0.9519,
      "step": 19200
    },
    {
      "epoch": 3.6541381928625665,
      "grad_norm": 24.165945053100586,
      "learning_rate": 7.308276385725134e-06,
      "loss": 0.7673,
      "step": 19250
    },
    {
      "epoch": 3.663629460895976,
      "grad_norm": 30.963836669921875,
      "learning_rate": 7.327258921791952e-06,
      "loss": 0.8229,
      "step": 19300
    },
    {
      "epoch": 3.6731207289293852,
      "grad_norm": 8.261945724487305,
      "learning_rate": 7.34624145785877e-06,
      "loss": 0.8127,
      "step": 19350
    },
    {
      "epoch": 3.682611996962794,
      "grad_norm": 13.631850242614746,
      "learning_rate": 7.365223993925589e-06,
      "loss": 0.6971,
      "step": 19400
    },
    {
      "epoch": 3.6921032649962036,
      "grad_norm": 23.3999080657959,
      "learning_rate": 7.384206529992407e-06,
      "loss": 0.908,
      "step": 19450
    },
    {
      "epoch": 3.7015945330296125,
      "grad_norm": 9.245640754699707,
      "learning_rate": 7.403189066059227e-06,
      "loss": 0.755,
      "step": 19500
    },
    {
      "epoch": 3.711085801063022,
      "grad_norm": 6.858055591583252,
      "learning_rate": 7.422171602126044e-06,
      "loss": 0.8711,
      "step": 19550
    },
    {
      "epoch": 3.7205770690964313,
      "grad_norm": 20.75617027282715,
      "learning_rate": 7.441154138192863e-06,
      "loss": 0.7696,
      "step": 19600
    },
    {
      "epoch": 3.7300683371298406,
      "grad_norm": 26.74806022644043,
      "learning_rate": 7.4601366742596815e-06,
      "loss": 0.8172,
      "step": 19650
    },
    {
      "epoch": 3.73955960516325,
      "grad_norm": 11.003649711608887,
      "learning_rate": 7.4791192103265e-06,
      "loss": 0.8036,
      "step": 19700
    },
    {
      "epoch": 3.749050873196659,
      "grad_norm": 15.964193344116211,
      "learning_rate": 7.498101746393319e-06,
      "loss": 0.7941,
      "step": 19750
    },
    {
      "epoch": 3.7585421412300684,
      "grad_norm": 7.206452369689941,
      "learning_rate": 7.517084282460138e-06,
      "loss": 0.7575,
      "step": 19800
    },
    {
      "epoch": 3.7680334092634777,
      "grad_norm": 10.4799222946167,
      "learning_rate": 7.5360668185269556e-06,
      "loss": 0.8496,
      "step": 19850
    },
    {
      "epoch": 3.7775246772968867,
      "grad_norm": 12.289904594421387,
      "learning_rate": 7.555049354593774e-06,
      "loss": 0.7831,
      "step": 19900
    },
    {
      "epoch": 3.787015945330296,
      "grad_norm": 13.299138069152832,
      "learning_rate": 7.574031890660593e-06,
      "loss": 0.7025,
      "step": 19950
    },
    {
      "epoch": 3.7965072133637054,
      "grad_norm": 21.67481803894043,
      "learning_rate": 7.593014426727412e-06,
      "loss": 0.9252,
      "step": 20000
    },
    {
      "epoch": 3.805998481397115,
      "grad_norm": 11.432907104492188,
      "learning_rate": 7.6119969627942305e-06,
      "loss": 0.7447,
      "step": 20050
    },
    {
      "epoch": 3.8154897494305238,
      "grad_norm": 24.214984893798828,
      "learning_rate": 7.630979498861048e-06,
      "loss": 0.8385,
      "step": 20100
    },
    {
      "epoch": 3.824981017463933,
      "grad_norm": 15.736380577087402,
      "learning_rate": 7.649962034927867e-06,
      "loss": 0.8498,
      "step": 20150
    },
    {
      "epoch": 3.8344722854973425,
      "grad_norm": 13.740530967712402,
      "learning_rate": 7.668944570994685e-06,
      "loss": 0.8154,
      "step": 20200
    },
    {
      "epoch": 3.8439635535307515,
      "grad_norm": 8.861577987670898,
      "learning_rate": 7.687927107061504e-06,
      "loss": 0.7466,
      "step": 20250
    },
    {
      "epoch": 3.853454821564161,
      "grad_norm": 14.518856048583984,
      "learning_rate": 7.706909643128322e-06,
      "loss": 0.7409,
      "step": 20300
    },
    {
      "epoch": 3.8629460895975702,
      "grad_norm": 14.758543968200684,
      "learning_rate": 7.72589217919514e-06,
      "loss": 0.6812,
      "step": 20350
    },
    {
      "epoch": 3.8724373576309796,
      "grad_norm": 11.504440307617188,
      "learning_rate": 7.74487471526196e-06,
      "loss": 0.7466,
      "step": 20400
    },
    {
      "epoch": 3.881928625664389,
      "grad_norm": 16.662830352783203,
      "learning_rate": 7.763857251328778e-06,
      "loss": 0.791,
      "step": 20450
    },
    {
      "epoch": 3.891419893697798,
      "grad_norm": 24.831533432006836,
      "learning_rate": 7.782839787395596e-06,
      "loss": 0.8391,
      "step": 20500
    },
    {
      "epoch": 3.9009111617312073,
      "grad_norm": 16.89289093017578,
      "learning_rate": 7.801822323462415e-06,
      "loss": 0.8154,
      "step": 20550
    },
    {
      "epoch": 3.9104024297646167,
      "grad_norm": 4.467473983764648,
      "learning_rate": 7.820804859529233e-06,
      "loss": 0.7032,
      "step": 20600
    },
    {
      "epoch": 3.9198936977980257,
      "grad_norm": 10.108407974243164,
      "learning_rate": 7.839787395596052e-06,
      "loss": 0.8041,
      "step": 20650
    },
    {
      "epoch": 3.929384965831435,
      "grad_norm": 5.1586713790893555,
      "learning_rate": 7.85876993166287e-06,
      "loss": 0.7562,
      "step": 20700
    },
    {
      "epoch": 3.9388762338648444,
      "grad_norm": 14.441492080688477,
      "learning_rate": 7.877752467729689e-06,
      "loss": 0.8079,
      "step": 20750
    },
    {
      "epoch": 3.948367501898254,
      "grad_norm": 17.93697738647461,
      "learning_rate": 7.896735003796508e-06,
      "loss": 0.778,
      "step": 20800
    },
    {
      "epoch": 3.9578587699316627,
      "grad_norm": 4.091307640075684,
      "learning_rate": 7.915717539863326e-06,
      "loss": 0.7562,
      "step": 20850
    },
    {
      "epoch": 3.967350037965072,
      "grad_norm": 19.032649993896484,
      "learning_rate": 7.934700075930145e-06,
      "loss": 0.766,
      "step": 20900
    },
    {
      "epoch": 3.9768413059984815,
      "grad_norm": 20.59929656982422,
      "learning_rate": 7.953682611996963e-06,
      "loss": 0.7645,
      "step": 20950
    },
    {
      "epoch": 3.9863325740318905,
      "grad_norm": 11.491216659545898,
      "learning_rate": 7.972665148063782e-06,
      "loss": 0.7518,
      "step": 21000
    },
    {
      "epoch": 3.9958238420653,
      "grad_norm": 18.77153205871582,
      "learning_rate": 7.9916476841306e-06,
      "loss": 0.7588,
      "step": 21050
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.7058935180791497,
      "eval_f1": 0.5245115568310004,
      "eval_loss": 0.737375795841217,
      "eval_precision": 0.5338324176391311,
      "eval_recall": 0.5416911104105614,
      "eval_runtime": 144.881,
      "eval_samples_per_second": 72.729,
      "eval_steps_per_second": 9.097,
      "step": 21072
    },
    {
      "epoch": 4.005315110098709,
      "grad_norm": 40.879981994628906,
      "learning_rate": 8.010630220197419e-06,
      "loss": 0.8712,
      "step": 21100
    },
    {
      "epoch": 4.014806378132119,
      "grad_norm": 21.996355056762695,
      "learning_rate": 8.029612756264237e-06,
      "loss": 0.773,
      "step": 21150
    },
    {
      "epoch": 4.024297646165528,
      "grad_norm": 4.110564231872559,
      "learning_rate": 8.048595292331056e-06,
      "loss": 0.8243,
      "step": 21200
    },
    {
      "epoch": 4.033788914198937,
      "grad_norm": 5.547402858734131,
      "learning_rate": 8.067577828397874e-06,
      "loss": 0.7434,
      "step": 21250
    },
    {
      "epoch": 4.043280182232346,
      "grad_norm": 33.6383171081543,
      "learning_rate": 8.086560364464693e-06,
      "loss": 0.7354,
      "step": 21300
    },
    {
      "epoch": 4.052771450265755,
      "grad_norm": 11.402295112609863,
      "learning_rate": 8.105542900531511e-06,
      "loss": 0.766,
      "step": 21350
    },
    {
      "epoch": 4.062262718299165,
      "grad_norm": 16.258085250854492,
      "learning_rate": 8.12452543659833e-06,
      "loss": 0.7874,
      "step": 21400
    },
    {
      "epoch": 4.071753986332574,
      "grad_norm": 15.596636772155762,
      "learning_rate": 8.143507972665148e-06,
      "loss": 0.7298,
      "step": 21450
    },
    {
      "epoch": 4.081245254365983,
      "grad_norm": 15.132148742675781,
      "learning_rate": 8.162490508731967e-06,
      "loss": 0.7928,
      "step": 21500
    },
    {
      "epoch": 4.090736522399393,
      "grad_norm": 21.859760284423828,
      "learning_rate": 8.181473044798786e-06,
      "loss": 0.8162,
      "step": 21550
    },
    {
      "epoch": 4.100227790432802,
      "grad_norm": 10.14914608001709,
      "learning_rate": 8.200455580865604e-06,
      "loss": 0.8368,
      "step": 21600
    },
    {
      "epoch": 4.109719058466211,
      "grad_norm": 9.355070114135742,
      "learning_rate": 8.219438116932423e-06,
      "loss": 0.7638,
      "step": 21650
    },
    {
      "epoch": 4.11921032649962,
      "grad_norm": 14.903036117553711,
      "learning_rate": 8.238420652999241e-06,
      "loss": 0.773,
      "step": 21700
    },
    {
      "epoch": 4.128701594533029,
      "grad_norm": 18.40943717956543,
      "learning_rate": 8.25740318906606e-06,
      "loss": 0.7328,
      "step": 21750
    },
    {
      "epoch": 4.138192862566439,
      "grad_norm": 16.52802276611328,
      "learning_rate": 8.276385725132878e-06,
      "loss": 0.753,
      "step": 21800
    },
    {
      "epoch": 4.147684130599848,
      "grad_norm": 8.603109359741211,
      "learning_rate": 8.295368261199697e-06,
      "loss": 0.7064,
      "step": 21850
    },
    {
      "epoch": 4.157175398633258,
      "grad_norm": 7.127761363983154,
      "learning_rate": 8.314350797266515e-06,
      "loss": 0.7899,
      "step": 21900
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 9.914599418640137,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.7521,
      "step": 21950
    },
    {
      "epoch": 4.176157934700076,
      "grad_norm": 20.033544540405273,
      "learning_rate": 8.352315869400152e-06,
      "loss": 0.6975,
      "step": 22000
    },
    {
      "epoch": 4.185649202733485,
      "grad_norm": 14.468106269836426,
      "learning_rate": 8.37129840546697e-06,
      "loss": 0.7058,
      "step": 22050
    },
    {
      "epoch": 4.195140470766894,
      "grad_norm": 14.760231018066406,
      "learning_rate": 8.39028094153379e-06,
      "loss": 0.6958,
      "step": 22100
    },
    {
      "epoch": 4.204631738800304,
      "grad_norm": 14.183677673339844,
      "learning_rate": 8.409263477600608e-06,
      "loss": 0.725,
      "step": 22150
    },
    {
      "epoch": 4.214123006833713,
      "grad_norm": 26.59519386291504,
      "learning_rate": 8.428246013667426e-06,
      "loss": 0.6584,
      "step": 22200
    },
    {
      "epoch": 4.223614274867122,
      "grad_norm": 5.766632080078125,
      "learning_rate": 8.447228549734245e-06,
      "loss": 0.6743,
      "step": 22250
    },
    {
      "epoch": 4.233105542900532,
      "grad_norm": 28.57036590576172,
      "learning_rate": 8.466211085801063e-06,
      "loss": 0.8392,
      "step": 22300
    },
    {
      "epoch": 4.242596810933941,
      "grad_norm": 7.364418029785156,
      "learning_rate": 8.485193621867882e-06,
      "loss": 0.7272,
      "step": 22350
    },
    {
      "epoch": 4.25208807896735,
      "grad_norm": 23.479021072387695,
      "learning_rate": 8.5041761579347e-06,
      "loss": 0.8076,
      "step": 22400
    },
    {
      "epoch": 4.261579347000759,
      "grad_norm": 18.58649253845215,
      "learning_rate": 8.523158694001519e-06,
      "loss": 0.802,
      "step": 22450
    },
    {
      "epoch": 4.271070615034168,
      "grad_norm": 18.45518684387207,
      "learning_rate": 8.542141230068338e-06,
      "loss": 0.6975,
      "step": 22500
    },
    {
      "epoch": 4.280561883067578,
      "grad_norm": 8.873688697814941,
      "learning_rate": 8.561123766135156e-06,
      "loss": 0.6962,
      "step": 22550
    },
    {
      "epoch": 4.290053151100987,
      "grad_norm": 13.245271682739258,
      "learning_rate": 8.580106302201975e-06,
      "loss": 0.7604,
      "step": 22600
    },
    {
      "epoch": 4.2995444191343966,
      "grad_norm": 18.28141212463379,
      "learning_rate": 8.599088838268793e-06,
      "loss": 0.5815,
      "step": 22650
    },
    {
      "epoch": 4.309035687167806,
      "grad_norm": 17.950231552124023,
      "learning_rate": 8.618071374335612e-06,
      "loss": 0.6372,
      "step": 22700
    },
    {
      "epoch": 4.318526955201214,
      "grad_norm": 23.53057289123535,
      "learning_rate": 8.63705391040243e-06,
      "loss": 0.6941,
      "step": 22750
    },
    {
      "epoch": 4.328018223234624,
      "grad_norm": 4.870722770690918,
      "learning_rate": 8.656036446469249e-06,
      "loss": 0.7483,
      "step": 22800
    },
    {
      "epoch": 4.337509491268033,
      "grad_norm": 15.603994369506836,
      "learning_rate": 8.675018982536067e-06,
      "loss": 0.6794,
      "step": 22850
    },
    {
      "epoch": 4.347000759301443,
      "grad_norm": 15.226490020751953,
      "learning_rate": 8.694001518602886e-06,
      "loss": 0.7699,
      "step": 22900
    },
    {
      "epoch": 4.356492027334852,
      "grad_norm": 17.15919303894043,
      "learning_rate": 8.712984054669704e-06,
      "loss": 0.7402,
      "step": 22950
    },
    {
      "epoch": 4.365983295368261,
      "grad_norm": 31.46242332458496,
      "learning_rate": 8.731966590736523e-06,
      "loss": 0.8036,
      "step": 23000
    },
    {
      "epoch": 4.375474563401671,
      "grad_norm": 48.213191986083984,
      "learning_rate": 8.750949126803341e-06,
      "loss": 0.6653,
      "step": 23050
    },
    {
      "epoch": 4.38496583143508,
      "grad_norm": 13.808045387268066,
      "learning_rate": 8.76993166287016e-06,
      "loss": 0.6869,
      "step": 23100
    },
    {
      "epoch": 4.394457099468489,
      "grad_norm": 6.328959941864014,
      "learning_rate": 8.788914198936978e-06,
      "loss": 0.7004,
      "step": 23150
    },
    {
      "epoch": 4.403948367501898,
      "grad_norm": 6.655340194702148,
      "learning_rate": 8.807896735003797e-06,
      "loss": 0.7523,
      "step": 23200
    },
    {
      "epoch": 4.413439635535307,
      "grad_norm": 19.372568130493164,
      "learning_rate": 8.826879271070615e-06,
      "loss": 0.7324,
      "step": 23250
    },
    {
      "epoch": 4.422930903568717,
      "grad_norm": 17.271329879760742,
      "learning_rate": 8.845861807137434e-06,
      "loss": 0.689,
      "step": 23300
    },
    {
      "epoch": 4.432422171602126,
      "grad_norm": 11.43354606628418,
      "learning_rate": 8.864844343204253e-06,
      "loss": 0.7297,
      "step": 23350
    },
    {
      "epoch": 4.4419134396355355,
      "grad_norm": 12.753850936889648,
      "learning_rate": 8.883826879271071e-06,
      "loss": 0.7179,
      "step": 23400
    },
    {
      "epoch": 4.451404707668945,
      "grad_norm": 32.335540771484375,
      "learning_rate": 8.90280941533789e-06,
      "loss": 0.7597,
      "step": 23450
    },
    {
      "epoch": 4.460895975702353,
      "grad_norm": 19.78553009033203,
      "learning_rate": 8.921791951404708e-06,
      "loss": 0.7622,
      "step": 23500
    },
    {
      "epoch": 4.470387243735763,
      "grad_norm": 16.339826583862305,
      "learning_rate": 8.940774487471527e-06,
      "loss": 0.7675,
      "step": 23550
    },
    {
      "epoch": 4.479878511769172,
      "grad_norm": 10.74133586883545,
      "learning_rate": 8.959757023538345e-06,
      "loss": 0.6661,
      "step": 23600
    },
    {
      "epoch": 4.489369779802582,
      "grad_norm": 24.28737449645996,
      "learning_rate": 8.978739559605164e-06,
      "loss": 0.661,
      "step": 23650
    },
    {
      "epoch": 4.498861047835991,
      "grad_norm": 11.71721076965332,
      "learning_rate": 8.997722095671982e-06,
      "loss": 0.6803,
      "step": 23700
    },
    {
      "epoch": 4.5083523158694,
      "grad_norm": 40.895084381103516,
      "learning_rate": 9.0167046317388e-06,
      "loss": 0.7442,
      "step": 23750
    },
    {
      "epoch": 4.51784358390281,
      "grad_norm": 9.477216720581055,
      "learning_rate": 9.03568716780562e-06,
      "loss": 0.7508,
      "step": 23800
    },
    {
      "epoch": 4.527334851936219,
      "grad_norm": 13.180760383605957,
      "learning_rate": 9.054669703872438e-06,
      "loss": 0.7223,
      "step": 23850
    },
    {
      "epoch": 4.536826119969628,
      "grad_norm": 8.461947441101074,
      "learning_rate": 9.073652239939256e-06,
      "loss": 0.7033,
      "step": 23900
    },
    {
      "epoch": 4.546317388003037,
      "grad_norm": 19.254056930541992,
      "learning_rate": 9.092634776006075e-06,
      "loss": 0.836,
      "step": 23950
    },
    {
      "epoch": 4.555808656036446,
      "grad_norm": 8.53461742401123,
      "learning_rate": 9.111617312072893e-06,
      "loss": 0.798,
      "step": 24000
    },
    {
      "epoch": 4.565299924069856,
      "grad_norm": 21.596755981445312,
      "learning_rate": 9.130599848139712e-06,
      "loss": 0.7091,
      "step": 24050
    },
    {
      "epoch": 4.574791192103265,
      "grad_norm": 11.977944374084473,
      "learning_rate": 9.14958238420653e-06,
      "loss": 0.6851,
      "step": 24100
    },
    {
      "epoch": 4.5842824601366745,
      "grad_norm": 16.341365814208984,
      "learning_rate": 9.168564920273349e-06,
      "loss": 0.6274,
      "step": 24150
    },
    {
      "epoch": 4.593773728170084,
      "grad_norm": 20.922597885131836,
      "learning_rate": 9.187547456340167e-06,
      "loss": 0.6946,
      "step": 24200
    },
    {
      "epoch": 4.603264996203492,
      "grad_norm": 13.681497573852539,
      "learning_rate": 9.206529992406986e-06,
      "loss": 0.6754,
      "step": 24250
    },
    {
      "epoch": 4.612756264236902,
      "grad_norm": 24.285919189453125,
      "learning_rate": 9.225512528473805e-06,
      "loss": 0.794,
      "step": 24300
    },
    {
      "epoch": 4.622247532270311,
      "grad_norm": 10.554081916809082,
      "learning_rate": 9.244495064540623e-06,
      "loss": 0.7896,
      "step": 24350
    },
    {
      "epoch": 4.6317388003037205,
      "grad_norm": 23.62117576599121,
      "learning_rate": 9.263477600607442e-06,
      "loss": 0.6728,
      "step": 24400
    },
    {
      "epoch": 4.64123006833713,
      "grad_norm": 19.604223251342773,
      "learning_rate": 9.28246013667426e-06,
      "loss": 0.7931,
      "step": 24450
    },
    {
      "epoch": 4.650721336370539,
      "grad_norm": 14.099035263061523,
      "learning_rate": 9.301442672741079e-06,
      "loss": 0.7471,
      "step": 24500
    },
    {
      "epoch": 4.660212604403949,
      "grad_norm": 21.771099090576172,
      "learning_rate": 9.320425208807897e-06,
      "loss": 0.7727,
      "step": 24550
    },
    {
      "epoch": 4.669703872437358,
      "grad_norm": 25.489315032958984,
      "learning_rate": 9.339407744874716e-06,
      "loss": 0.6456,
      "step": 24600
    },
    {
      "epoch": 4.679195140470767,
      "grad_norm": 10.258816719055176,
      "learning_rate": 9.358390280941534e-06,
      "loss": 0.7089,
      "step": 24650
    },
    {
      "epoch": 4.688686408504176,
      "grad_norm": 18.147674560546875,
      "learning_rate": 9.377372817008353e-06,
      "loss": 0.6328,
      "step": 24700
    },
    {
      "epoch": 4.698177676537585,
      "grad_norm": 19.294498443603516,
      "learning_rate": 9.396355353075171e-06,
      "loss": 0.6398,
      "step": 24750
    },
    {
      "epoch": 4.707668944570995,
      "grad_norm": 10.175861358642578,
      "learning_rate": 9.41533788914199e-06,
      "loss": 0.6993,
      "step": 24800
    },
    {
      "epoch": 4.717160212604404,
      "grad_norm": 21.511714935302734,
      "learning_rate": 9.434320425208808e-06,
      "loss": 0.6236,
      "step": 24850
    },
    {
      "epoch": 4.7266514806378135,
      "grad_norm": 11.2882661819458,
      "learning_rate": 9.453302961275629e-06,
      "loss": 0.6522,
      "step": 24900
    },
    {
      "epoch": 4.736142748671222,
      "grad_norm": 15.990723609924316,
      "learning_rate": 9.472285497342445e-06,
      "loss": 0.6419,
      "step": 24950
    },
    {
      "epoch": 4.745634016704631,
      "grad_norm": 29.048330307006836,
      "learning_rate": 9.491268033409264e-06,
      "loss": 0.6582,
      "step": 25000
    },
    {
      "epoch": 4.755125284738041,
      "grad_norm": 18.184574127197266,
      "learning_rate": 9.510250569476082e-06,
      "loss": 0.714,
      "step": 25050
    },
    {
      "epoch": 4.76461655277145,
      "grad_norm": 17.29563331604004,
      "learning_rate": 9.529233105542901e-06,
      "loss": 0.6827,
      "step": 25100
    },
    {
      "epoch": 4.7741078208048595,
      "grad_norm": 21.406768798828125,
      "learning_rate": 9.54821564160972e-06,
      "loss": 0.6765,
      "step": 25150
    },
    {
      "epoch": 4.783599088838269,
      "grad_norm": 19.531707763671875,
      "learning_rate": 9.567198177676538e-06,
      "loss": 0.6361,
      "step": 25200
    },
    {
      "epoch": 4.793090356871678,
      "grad_norm": 20.954322814941406,
      "learning_rate": 9.586180713743357e-06,
      "loss": 0.7084,
      "step": 25250
    },
    {
      "epoch": 4.802581624905088,
      "grad_norm": 16.278339385986328,
      "learning_rate": 9.605163249810175e-06,
      "loss": 0.6914,
      "step": 25300
    },
    {
      "epoch": 4.812072892938497,
      "grad_norm": 16.269454956054688,
      "learning_rate": 9.624145785876994e-06,
      "loss": 0.6737,
      "step": 25350
    },
    {
      "epoch": 4.8215641609719055,
      "grad_norm": 6.564281463623047,
      "learning_rate": 9.643128321943812e-06,
      "loss": 0.7218,
      "step": 25400
    },
    {
      "epoch": 4.831055429005315,
      "grad_norm": 6.406619071960449,
      "learning_rate": 9.662110858010632e-06,
      "loss": 0.5951,
      "step": 25450
    },
    {
      "epoch": 4.840546697038724,
      "grad_norm": 13.578781127929688,
      "learning_rate": 9.68109339407745e-06,
      "loss": 0.7262,
      "step": 25500
    },
    {
      "epoch": 4.850037965072134,
      "grad_norm": 4.0845866203308105,
      "learning_rate": 9.700075930144268e-06,
      "loss": 0.6229,
      "step": 25550
    },
    {
      "epoch": 4.859529233105543,
      "grad_norm": 17.324356079101562,
      "learning_rate": 9.719058466211086e-06,
      "loss": 0.703,
      "step": 25600
    },
    {
      "epoch": 4.8690205011389525,
      "grad_norm": 30.58555030822754,
      "learning_rate": 9.738041002277905e-06,
      "loss": 0.5374,
      "step": 25650
    },
    {
      "epoch": 4.878511769172361,
      "grad_norm": 4.38105583190918,
      "learning_rate": 9.757023538344723e-06,
      "loss": 0.6309,
      "step": 25700
    },
    {
      "epoch": 4.88800303720577,
      "grad_norm": 29.663965225219727,
      "learning_rate": 9.776006074411542e-06,
      "loss": 0.7003,
      "step": 25750
    },
    {
      "epoch": 4.89749430523918,
      "grad_norm": 27.768095016479492,
      "learning_rate": 9.79498861047836e-06,
      "loss": 0.6149,
      "step": 25800
    },
    {
      "epoch": 4.906985573272589,
      "grad_norm": 18.008386611938477,
      "learning_rate": 9.813971146545179e-06,
      "loss": 0.7477,
      "step": 25850
    },
    {
      "epoch": 4.9164768413059985,
      "grad_norm": 6.0099568367004395,
      "learning_rate": 9.832953682611997e-06,
      "loss": 0.6569,
      "step": 25900
    },
    {
      "epoch": 4.925968109339408,
      "grad_norm": 8.468093872070312,
      "learning_rate": 9.851936218678816e-06,
      "loss": 0.7145,
      "step": 25950
    },
    {
      "epoch": 4.935459377372817,
      "grad_norm": 3.5808286666870117,
      "learning_rate": 9.870918754745634e-06,
      "loss": 0.694,
      "step": 26000
    },
    {
      "epoch": 4.944950645406227,
      "grad_norm": 11.028132438659668,
      "learning_rate": 9.889901290812453e-06,
      "loss": 0.6346,
      "step": 26050
    },
    {
      "epoch": 4.954441913439636,
      "grad_norm": 10.126849174499512,
      "learning_rate": 9.908883826879272e-06,
      "loss": 0.7045,
      "step": 26100
    },
    {
      "epoch": 4.9639331814730445,
      "grad_norm": 8.80348014831543,
      "learning_rate": 9.92786636294609e-06,
      "loss": 0.6442,
      "step": 26150
    },
    {
      "epoch": 4.973424449506454,
      "grad_norm": 20.961952209472656,
      "learning_rate": 9.946848899012909e-06,
      "loss": 0.6837,
      "step": 26200
    },
    {
      "epoch": 4.982915717539863,
      "grad_norm": 8.853753089904785,
      "learning_rate": 9.965831435079727e-06,
      "loss": 0.678,
      "step": 26250
    },
    {
      "epoch": 4.992406985573273,
      "grad_norm": 23.66146469116211,
      "learning_rate": 9.984813971146546e-06,
      "loss": 0.5969,
      "step": 26300
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.7487899781721553,
      "eval_f1": 0.6375227596941756,
      "eval_loss": 0.640886127948761,
      "eval_precision": 0.7138072186856146,
      "eval_recall": 0.6374757062781228,
      "eval_runtime": 146.7192,
      "eval_samples_per_second": 71.817,
      "eval_steps_per_second": 8.983,
      "step": 26340
    },
    {
      "epoch": 5.001898253606682,
      "grad_norm": 9.795027732849121,
      "learning_rate": 1.0003796507213364e-05,
      "loss": 0.6288,
      "step": 26350
    },
    {
      "epoch": 5.011389521640091,
      "grad_norm": 2.5421605110168457,
      "learning_rate": 1.0022779043280184e-05,
      "loss": 0.6836,
      "step": 26400
    },
    {
      "epoch": 5.020880789673501,
      "grad_norm": 22.198081970214844,
      "learning_rate": 1.0041761579347003e-05,
      "loss": 0.6377,
      "step": 26450
    },
    {
      "epoch": 5.030372057706909,
      "grad_norm": 7.202115058898926,
      "learning_rate": 1.0060744115413821e-05,
      "loss": 0.6435,
      "step": 26500
    },
    {
      "epoch": 5.039863325740319,
      "grad_norm": 11.092123031616211,
      "learning_rate": 1.0079726651480638e-05,
      "loss": 0.6743,
      "step": 26550
    },
    {
      "epoch": 5.049354593773728,
      "grad_norm": 4.593706130981445,
      "learning_rate": 1.0098709187547457e-05,
      "loss": 0.7279,
      "step": 26600
    },
    {
      "epoch": 5.0588458618071375,
      "grad_norm": 13.087091445922852,
      "learning_rate": 1.0117691723614275e-05,
      "loss": 0.6765,
      "step": 26650
    },
    {
      "epoch": 5.068337129840547,
      "grad_norm": 20.325361251831055,
      "learning_rate": 1.0136674259681094e-05,
      "loss": 0.6532,
      "step": 26700
    },
    {
      "epoch": 5.077828397873956,
      "grad_norm": 23.200597763061523,
      "learning_rate": 1.0155656795747912e-05,
      "loss": 0.6841,
      "step": 26750
    },
    {
      "epoch": 5.087319665907366,
      "grad_norm": 16.865440368652344,
      "learning_rate": 1.0174639331814731e-05,
      "loss": 0.6024,
      "step": 26800
    },
    {
      "epoch": 5.096810933940774,
      "grad_norm": 13.262837409973145,
      "learning_rate": 1.019362186788155e-05,
      "loss": 0.6792,
      "step": 26850
    },
    {
      "epoch": 5.1063022019741835,
      "grad_norm": 28.62660026550293,
      "learning_rate": 1.0212604403948368e-05,
      "loss": 0.6977,
      "step": 26900
    },
    {
      "epoch": 5.115793470007593,
      "grad_norm": 8.914382934570312,
      "learning_rate": 1.0231586940015188e-05,
      "loss": 0.6668,
      "step": 26950
    },
    {
      "epoch": 5.125284738041002,
      "grad_norm": 12.077852249145508,
      "learning_rate": 1.0250569476082007e-05,
      "loss": 0.6225,
      "step": 27000
    },
    {
      "epoch": 5.134776006074412,
      "grad_norm": 30.667346954345703,
      "learning_rate": 1.0269552012148825e-05,
      "loss": 0.6443,
      "step": 27050
    },
    {
      "epoch": 5.144267274107821,
      "grad_norm": 19.586870193481445,
      "learning_rate": 1.0288534548215642e-05,
      "loss": 0.6299,
      "step": 27100
    },
    {
      "epoch": 5.15375854214123,
      "grad_norm": 4.758602619171143,
      "learning_rate": 1.030751708428246e-05,
      "loss": 0.7151,
      "step": 27150
    },
    {
      "epoch": 5.163249810174639,
      "grad_norm": 21.118389129638672,
      "learning_rate": 1.032649962034928e-05,
      "loss": 0.7602,
      "step": 27200
    },
    {
      "epoch": 5.172741078208048,
      "grad_norm": 17.501317977905273,
      "learning_rate": 1.0345482156416098e-05,
      "loss": 0.6822,
      "step": 27250
    },
    {
      "epoch": 5.182232346241458,
      "grad_norm": 23.395353317260742,
      "learning_rate": 1.0364464692482916e-05,
      "loss": 0.6165,
      "step": 27300
    },
    {
      "epoch": 5.191723614274867,
      "grad_norm": 26.357120513916016,
      "learning_rate": 1.0383447228549735e-05,
      "loss": 0.6445,
      "step": 27350
    },
    {
      "epoch": 5.201214882308276,
      "grad_norm": 14.685632705688477,
      "learning_rate": 1.0402429764616553e-05,
      "loss": 0.6396,
      "step": 27400
    },
    {
      "epoch": 5.210706150341686,
      "grad_norm": 11.925140380859375,
      "learning_rate": 1.0421412300683372e-05,
      "loss": 0.6577,
      "step": 27450
    },
    {
      "epoch": 5.220197418375095,
      "grad_norm": 16.92893409729004,
      "learning_rate": 1.0440394836750192e-05,
      "loss": 0.56,
      "step": 27500
    },
    {
      "epoch": 5.229688686408505,
      "grad_norm": 16.676734924316406,
      "learning_rate": 1.045937737281701e-05,
      "loss": 0.6295,
      "step": 27550
    },
    {
      "epoch": 5.239179954441913,
      "grad_norm": 6.681787014007568,
      "learning_rate": 1.0478359908883829e-05,
      "loss": 0.6488,
      "step": 27600
    },
    {
      "epoch": 5.2486712224753225,
      "grad_norm": 15.274462699890137,
      "learning_rate": 1.0497342444950646e-05,
      "loss": 0.6347,
      "step": 27650
    },
    {
      "epoch": 5.258162490508732,
      "grad_norm": 15.347599029541016,
      "learning_rate": 1.0516324981017464e-05,
      "loss": 0.6165,
      "step": 27700
    },
    {
      "epoch": 5.267653758542141,
      "grad_norm": 6.897571086883545,
      "learning_rate": 1.0535307517084283e-05,
      "loss": 0.6619,
      "step": 27750
    },
    {
      "epoch": 5.277145026575551,
      "grad_norm": 7.285301685333252,
      "learning_rate": 1.0554290053151101e-05,
      "loss": 0.6327,
      "step": 27800
    },
    {
      "epoch": 5.28663629460896,
      "grad_norm": 5.789418697357178,
      "learning_rate": 1.057327258921792e-05,
      "loss": 0.6595,
      "step": 27850
    },
    {
      "epoch": 5.296127562642369,
      "grad_norm": 10.880417823791504,
      "learning_rate": 1.0592255125284739e-05,
      "loss": 0.6151,
      "step": 27900
    },
    {
      "epoch": 5.305618830675778,
      "grad_norm": 11.616003036499023,
      "learning_rate": 1.0611237661351557e-05,
      "loss": 0.5859,
      "step": 27950
    },
    {
      "epoch": 5.315110098709187,
      "grad_norm": 28.79874038696289,
      "learning_rate": 1.0630220197418376e-05,
      "loss": 0.618,
      "step": 28000
    },
    {
      "epoch": 5.324601366742597,
      "grad_norm": 19.128005981445312,
      "learning_rate": 1.0649202733485196e-05,
      "loss": 0.5538,
      "step": 28050
    },
    {
      "epoch": 5.334092634776006,
      "grad_norm": 9.007640838623047,
      "learning_rate": 1.0668185269552014e-05,
      "loss": 0.6511,
      "step": 28100
    },
    {
      "epoch": 5.343583902809415,
      "grad_norm": 22.641845703125,
      "learning_rate": 1.0687167805618833e-05,
      "loss": 0.6283,
      "step": 28150
    },
    {
      "epoch": 5.353075170842825,
      "grad_norm": 24.33152961730957,
      "learning_rate": 1.070615034168565e-05,
      "loss": 0.633,
      "step": 28200
    },
    {
      "epoch": 5.362566438876234,
      "grad_norm": 18.873516082763672,
      "learning_rate": 1.0725132877752468e-05,
      "loss": 0.6739,
      "step": 28250
    },
    {
      "epoch": 5.372057706909644,
      "grad_norm": 7.463794231414795,
      "learning_rate": 1.0744115413819287e-05,
      "loss": 0.6225,
      "step": 28300
    },
    {
      "epoch": 5.381548974943052,
      "grad_norm": 19.010530471801758,
      "learning_rate": 1.0763097949886105e-05,
      "loss": 0.8146,
      "step": 28350
    },
    {
      "epoch": 5.3910402429764614,
      "grad_norm": 18.23872184753418,
      "learning_rate": 1.0782080485952924e-05,
      "loss": 0.6427,
      "step": 28400
    },
    {
      "epoch": 5.400531511009871,
      "grad_norm": 13.396796226501465,
      "learning_rate": 1.0801063022019742e-05,
      "loss": 0.6541,
      "step": 28450
    },
    {
      "epoch": 5.41002277904328,
      "grad_norm": 17.98758316040039,
      "learning_rate": 1.0820045558086561e-05,
      "loss": 0.6139,
      "step": 28500
    },
    {
      "epoch": 5.41951404707669,
      "grad_norm": 18.974258422851562,
      "learning_rate": 1.083902809415338e-05,
      "loss": 0.6314,
      "step": 28550
    },
    {
      "epoch": 5.429005315110099,
      "grad_norm": 24.407939910888672,
      "learning_rate": 1.08580106302202e-05,
      "loss": 0.5716,
      "step": 28600
    },
    {
      "epoch": 5.438496583143508,
      "grad_norm": 22.92230987548828,
      "learning_rate": 1.0876993166287018e-05,
      "loss": 0.6669,
      "step": 28650
    },
    {
      "epoch": 5.447987851176917,
      "grad_norm": 18.94512939453125,
      "learning_rate": 1.0895975702353837e-05,
      "loss": 0.6307,
      "step": 28700
    },
    {
      "epoch": 5.457479119210326,
      "grad_norm": 12.966320991516113,
      "learning_rate": 1.0914958238420654e-05,
      "loss": 0.529,
      "step": 28750
    },
    {
      "epoch": 5.466970387243736,
      "grad_norm": 11.884342193603516,
      "learning_rate": 1.0933940774487472e-05,
      "loss": 0.6677,
      "step": 28800
    },
    {
      "epoch": 5.476461655277145,
      "grad_norm": 9.315463066101074,
      "learning_rate": 1.095292331055429e-05,
      "loss": 0.6206,
      "step": 28850
    },
    {
      "epoch": 5.485952923310554,
      "grad_norm": 10.403999328613281,
      "learning_rate": 1.0971905846621109e-05,
      "loss": 0.6126,
      "step": 28900
    },
    {
      "epoch": 5.495444191343964,
      "grad_norm": 10.819497108459473,
      "learning_rate": 1.0990888382687928e-05,
      "loss": 0.6467,
      "step": 28950
    },
    {
      "epoch": 5.504935459377373,
      "grad_norm": 18.490022659301758,
      "learning_rate": 1.1009870918754746e-05,
      "loss": 0.6928,
      "step": 29000
    },
    {
      "epoch": 5.5144267274107825,
      "grad_norm": 18.865062713623047,
      "learning_rate": 1.1028853454821565e-05,
      "loss": 0.6701,
      "step": 29050
    },
    {
      "epoch": 5.523917995444191,
      "grad_norm": 12.644123077392578,
      "learning_rate": 1.1047835990888383e-05,
      "loss": 0.5958,
      "step": 29100
    },
    {
      "epoch": 5.5334092634776,
      "grad_norm": 24.17318344116211,
      "learning_rate": 1.1066818526955203e-05,
      "loss": 0.5785,
      "step": 29150
    },
    {
      "epoch": 5.54290053151101,
      "grad_norm": 6.824512004852295,
      "learning_rate": 1.1085801063022022e-05,
      "loss": 0.6597,
      "step": 29200
    },
    {
      "epoch": 5.552391799544419,
      "grad_norm": 14.228928565979004,
      "learning_rate": 1.110478359908884e-05,
      "loss": 0.5881,
      "step": 29250
    },
    {
      "epoch": 5.561883067577829,
      "grad_norm": 16.53935432434082,
      "learning_rate": 1.1123766135155657e-05,
      "loss": 0.6498,
      "step": 29300
    },
    {
      "epoch": 5.571374335611238,
      "grad_norm": 17.92060089111328,
      "learning_rate": 1.1142748671222476e-05,
      "loss": 0.6499,
      "step": 29350
    },
    {
      "epoch": 5.5808656036446465,
      "grad_norm": 23.31061363220215,
      "learning_rate": 1.1161731207289294e-05,
      "loss": 0.7245,
      "step": 29400
    },
    {
      "epoch": 5.590356871678056,
      "grad_norm": 12.147388458251953,
      "learning_rate": 1.1180713743356113e-05,
      "loss": 0.663,
      "step": 29450
    },
    {
      "epoch": 5.599848139711465,
      "grad_norm": 19.959644317626953,
      "learning_rate": 1.1199696279422931e-05,
      "loss": 0.6011,
      "step": 29500
    },
    {
      "epoch": 5.609339407744875,
      "grad_norm": 8.745780944824219,
      "learning_rate": 1.121867881548975e-05,
      "loss": 0.6587,
      "step": 29550
    },
    {
      "epoch": 5.618830675778284,
      "grad_norm": 26.27973747253418,
      "learning_rate": 1.1237661351556568e-05,
      "loss": 0.6046,
      "step": 29600
    },
    {
      "epoch": 5.628321943811693,
      "grad_norm": 17.446012496948242,
      "learning_rate": 1.1256643887623387e-05,
      "loss": 0.6452,
      "step": 29650
    },
    {
      "epoch": 5.637813211845103,
      "grad_norm": 11.536932945251465,
      "learning_rate": 1.1275626423690207e-05,
      "loss": 0.6535,
      "step": 29700
    },
    {
      "epoch": 5.647304479878512,
      "grad_norm": 22.49864959716797,
      "learning_rate": 1.1294608959757026e-05,
      "loss": 0.6399,
      "step": 29750
    },
    {
      "epoch": 5.6567957479119215,
      "grad_norm": 19.722858428955078,
      "learning_rate": 1.1313591495823844e-05,
      "loss": 0.612,
      "step": 29800
    },
    {
      "epoch": 5.66628701594533,
      "grad_norm": 19.008264541625977,
      "learning_rate": 1.1332574031890661e-05,
      "loss": 0.5586,
      "step": 29850
    },
    {
      "epoch": 5.675778283978739,
      "grad_norm": 13.76337718963623,
      "learning_rate": 1.135155656795748e-05,
      "loss": 0.6627,
      "step": 29900
    },
    {
      "epoch": 5.685269552012149,
      "grad_norm": 12.758164405822754,
      "learning_rate": 1.1370539104024298e-05,
      "loss": 0.6148,
      "step": 29950
    },
    {
      "epoch": 5.694760820045558,
      "grad_norm": 32.61209487915039,
      "learning_rate": 1.1389521640091117e-05,
      "loss": 0.6211,
      "step": 30000
    },
    {
      "epoch": 5.7042520880789676,
      "grad_norm": 4.073261737823486,
      "learning_rate": 1.1408504176157935e-05,
      "loss": 0.622,
      "step": 30050
    },
    {
      "epoch": 5.713743356112377,
      "grad_norm": 14.48926067352295,
      "learning_rate": 1.1427486712224754e-05,
      "loss": 0.6523,
      "step": 30100
    },
    {
      "epoch": 5.723234624145785,
      "grad_norm": 13.601847648620605,
      "learning_rate": 1.1446469248291572e-05,
      "loss": 0.5565,
      "step": 30150
    },
    {
      "epoch": 5.732725892179195,
      "grad_norm": 8.850166320800781,
      "learning_rate": 1.146545178435839e-05,
      "loss": 0.5841,
      "step": 30200
    },
    {
      "epoch": 5.742217160212604,
      "grad_norm": 45.74338150024414,
      "learning_rate": 1.1484434320425211e-05,
      "loss": 0.6261,
      "step": 30250
    },
    {
      "epoch": 5.751708428246014,
      "grad_norm": 21.913074493408203,
      "learning_rate": 1.150341685649203e-05,
      "loss": 0.6965,
      "step": 30300
    },
    {
      "epoch": 5.761199696279423,
      "grad_norm": 43.87322235107422,
      "learning_rate": 1.1522399392558848e-05,
      "loss": 0.6661,
      "step": 30350
    },
    {
      "epoch": 5.770690964312832,
      "grad_norm": 36.293636322021484,
      "learning_rate": 1.1541381928625665e-05,
      "loss": 0.5833,
      "step": 30400
    },
    {
      "epoch": 5.780182232346242,
      "grad_norm": 15.046648979187012,
      "learning_rate": 1.1560364464692483e-05,
      "loss": 0.5403,
      "step": 30450
    },
    {
      "epoch": 5.789673500379651,
      "grad_norm": 32.17162322998047,
      "learning_rate": 1.1579347000759302e-05,
      "loss": 0.588,
      "step": 30500
    },
    {
      "epoch": 5.79916476841306,
      "grad_norm": 15.677923202514648,
      "learning_rate": 1.159832953682612e-05,
      "loss": 0.6482,
      "step": 30550
    },
    {
      "epoch": 5.808656036446469,
      "grad_norm": 25.389352798461914,
      "learning_rate": 1.1617312072892939e-05,
      "loss": 0.6584,
      "step": 30600
    },
    {
      "epoch": 5.818147304479878,
      "grad_norm": 24.18623161315918,
      "learning_rate": 1.1636294608959758e-05,
      "loss": 0.6326,
      "step": 30650
    },
    {
      "epoch": 5.827638572513288,
      "grad_norm": 8.38514518737793,
      "learning_rate": 1.1655277145026576e-05,
      "loss": 0.622,
      "step": 30700
    },
    {
      "epoch": 5.837129840546697,
      "grad_norm": 22.850059509277344,
      "learning_rate": 1.1674259681093396e-05,
      "loss": 0.5986,
      "step": 30750
    },
    {
      "epoch": 5.8466211085801065,
      "grad_norm": 10.53097915649414,
      "learning_rate": 1.1693242217160215e-05,
      "loss": 0.5956,
      "step": 30800
    },
    {
      "epoch": 5.856112376613516,
      "grad_norm": 2.4012296199798584,
      "learning_rate": 1.1712224753227033e-05,
      "loss": 0.6253,
      "step": 30850
    },
    {
      "epoch": 5.865603644646924,
      "grad_norm": 12.96875,
      "learning_rate": 1.1731207289293852e-05,
      "loss": 0.6183,
      "step": 30900
    },
    {
      "epoch": 5.875094912680334,
      "grad_norm": 16.07073974609375,
      "learning_rate": 1.1750189825360669e-05,
      "loss": 0.6007,
      "step": 30950
    },
    {
      "epoch": 5.884586180713743,
      "grad_norm": 37.881690979003906,
      "learning_rate": 1.1769172361427487e-05,
      "loss": 0.5951,
      "step": 31000
    },
    {
      "epoch": 5.894077448747153,
      "grad_norm": 14.182661056518555,
      "learning_rate": 1.1788154897494306e-05,
      "loss": 0.6303,
      "step": 31050
    },
    {
      "epoch": 5.903568716780562,
      "grad_norm": 13.826074600219727,
      "learning_rate": 1.1807137433561124e-05,
      "loss": 0.6064,
      "step": 31100
    },
    {
      "epoch": 5.913059984813971,
      "grad_norm": 2.815690279006958,
      "learning_rate": 1.1826119969627943e-05,
      "loss": 0.6048,
      "step": 31150
    },
    {
      "epoch": 5.922551252847381,
      "grad_norm": 33.10071563720703,
      "learning_rate": 1.1845102505694761e-05,
      "loss": 0.6245,
      "step": 31200
    },
    {
      "epoch": 5.93204252088079,
      "grad_norm": 13.62360668182373,
      "learning_rate": 1.186408504176158e-05,
      "loss": 0.503,
      "step": 31250
    },
    {
      "epoch": 5.941533788914199,
      "grad_norm": 15.176743507385254,
      "learning_rate": 1.18830675778284e-05,
      "loss": 0.6405,
      "step": 31300
    },
    {
      "epoch": 5.951025056947608,
      "grad_norm": 31.917736053466797,
      "learning_rate": 1.1902050113895219e-05,
      "loss": 0.6577,
      "step": 31350
    },
    {
      "epoch": 5.960516324981017,
      "grad_norm": 18.19858169555664,
      "learning_rate": 1.1921032649962037e-05,
      "loss": 0.6766,
      "step": 31400
    },
    {
      "epoch": 5.970007593014427,
      "grad_norm": 26.133071899414062,
      "learning_rate": 1.1940015186028856e-05,
      "loss": 0.6617,
      "step": 31450
    },
    {
      "epoch": 5.979498861047836,
      "grad_norm": 15.60947322845459,
      "learning_rate": 1.1958997722095673e-05,
      "loss": 0.6548,
      "step": 31500
    },
    {
      "epoch": 5.9889901290812455,
      "grad_norm": 14.17601490020752,
      "learning_rate": 1.1977980258162491e-05,
      "loss": 0.6441,
      "step": 31550
    },
    {
      "epoch": 5.998481397114655,
      "grad_norm": 16.62676239013672,
      "learning_rate": 1.199696279422931e-05,
      "loss": 0.6308,
      "step": 31600
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.7796336718230995,
      "eval_f1": 0.7253810621286098,
      "eval_loss": 0.5837939977645874,
      "eval_precision": 0.7531187100271249,
      "eval_recall": 0.7220184004085013,
      "eval_runtime": 155.667,
      "eval_samples_per_second": 67.689,
      "eval_steps_per_second": 8.467,
      "step": 31608
    },
    {
      "epoch": 6.007972665148063,
      "grad_norm": 14.71807861328125,
      "learning_rate": 1.2015945330296128e-05,
      "loss": 0.6663,
      "step": 31650
    },
    {
      "epoch": 6.017463933181473,
      "grad_norm": 23.47587776184082,
      "learning_rate": 1.2034927866362947e-05,
      "loss": 0.5931,
      "step": 31700
    },
    {
      "epoch": 6.026955201214882,
      "grad_norm": 24.787498474121094,
      "learning_rate": 1.2053910402429765e-05,
      "loss": 0.6063,
      "step": 31750
    },
    {
      "epoch": 6.0364464692482915,
      "grad_norm": 25.479915618896484,
      "learning_rate": 1.2072892938496584e-05,
      "loss": 0.6263,
      "step": 31800
    },
    {
      "epoch": 6.045937737281701,
      "grad_norm": 15.901619911193848,
      "learning_rate": 1.2091875474563404e-05,
      "loss": 0.5826,
      "step": 31850
    },
    {
      "epoch": 6.05542900531511,
      "grad_norm": 10.340659141540527,
      "learning_rate": 1.2110858010630222e-05,
      "loss": 0.4836,
      "step": 31900
    },
    {
      "epoch": 6.06492027334852,
      "grad_norm": 3.6549744606018066,
      "learning_rate": 1.2129840546697041e-05,
      "loss": 0.6327,
      "step": 31950
    },
    {
      "epoch": 6.074411541381929,
      "grad_norm": 20.87631607055664,
      "learning_rate": 1.214882308276386e-05,
      "loss": 0.5405,
      "step": 32000
    },
    {
      "epoch": 6.083902809415338,
      "grad_norm": 26.845977783203125,
      "learning_rate": 1.2167805618830676e-05,
      "loss": 0.5895,
      "step": 32050
    },
    {
      "epoch": 6.093394077448747,
      "grad_norm": 24.557479858398438,
      "learning_rate": 1.2186788154897495e-05,
      "loss": 0.6133,
      "step": 32100
    },
    {
      "epoch": 6.102885345482156,
      "grad_norm": 12.075623512268066,
      "learning_rate": 1.2205770690964313e-05,
      "loss": 0.5828,
      "step": 32150
    },
    {
      "epoch": 6.112376613515566,
      "grad_norm": 23.435565948486328,
      "learning_rate": 1.2224753227031132e-05,
      "loss": 0.5106,
      "step": 32200
    },
    {
      "epoch": 6.121867881548975,
      "grad_norm": 29.251008987426758,
      "learning_rate": 1.224373576309795e-05,
      "loss": 0.5827,
      "step": 32250
    },
    {
      "epoch": 6.1313591495823845,
      "grad_norm": 25.5485782623291,
      "learning_rate": 1.2262718299164769e-05,
      "loss": 0.6088,
      "step": 32300
    },
    {
      "epoch": 6.140850417615794,
      "grad_norm": 12.725555419921875,
      "learning_rate": 1.2281700835231588e-05,
      "loss": 0.584,
      "step": 32350
    },
    {
      "epoch": 6.150341685649202,
      "grad_norm": 8.201872825622559,
      "learning_rate": 1.2300683371298408e-05,
      "loss": 0.5972,
      "step": 32400
    },
    {
      "epoch": 6.159832953682612,
      "grad_norm": 12.660252571105957,
      "learning_rate": 1.2319665907365226e-05,
      "loss": 0.562,
      "step": 32450
    },
    {
      "epoch": 6.169324221716021,
      "grad_norm": 6.4297776222229,
      "learning_rate": 1.2338648443432045e-05,
      "loss": 0.5678,
      "step": 32500
    },
    {
      "epoch": 6.1788154897494305,
      "grad_norm": 40.20481872558594,
      "learning_rate": 1.2357630979498862e-05,
      "loss": 0.6394,
      "step": 32550
    },
    {
      "epoch": 6.18830675778284,
      "grad_norm": 13.085807800292969,
      "learning_rate": 1.237661351556568e-05,
      "loss": 0.562,
      "step": 32600
    },
    {
      "epoch": 6.197798025816249,
      "grad_norm": 20.622913360595703,
      "learning_rate": 1.2395596051632499e-05,
      "loss": 0.6125,
      "step": 32650
    },
    {
      "epoch": 6.207289293849659,
      "grad_norm": 27.190128326416016,
      "learning_rate": 1.2414578587699317e-05,
      "loss": 0.5705,
      "step": 32700
    },
    {
      "epoch": 6.216780561883067,
      "grad_norm": 5.925875663757324,
      "learning_rate": 1.2433561123766136e-05,
      "loss": 0.5436,
      "step": 32750
    },
    {
      "epoch": 6.2262718299164765,
      "grad_norm": 15.545586585998535,
      "learning_rate": 1.2452543659832954e-05,
      "loss": 0.5745,
      "step": 32800
    },
    {
      "epoch": 6.235763097949886,
      "grad_norm": 29.143810272216797,
      "learning_rate": 1.2471526195899773e-05,
      "loss": 0.6151,
      "step": 32850
    },
    {
      "epoch": 6.245254365983295,
      "grad_norm": 7.814499855041504,
      "learning_rate": 1.2490508731966591e-05,
      "loss": 0.5092,
      "step": 32900
    },
    {
      "epoch": 6.254745634016705,
      "grad_norm": 28.032569885253906,
      "learning_rate": 1.2509491268033412e-05,
      "loss": 0.5954,
      "step": 32950
    },
    {
      "epoch": 6.264236902050114,
      "grad_norm": 12.790244102478027,
      "learning_rate": 1.252847380410023e-05,
      "loss": 0.6544,
      "step": 33000
    },
    {
      "epoch": 6.2737281700835235,
      "grad_norm": 19.194496154785156,
      "learning_rate": 1.2547456340167049e-05,
      "loss": 0.4885,
      "step": 33050
    },
    {
      "epoch": 6.283219438116932,
      "grad_norm": 11.726158142089844,
      "learning_rate": 1.2566438876233865e-05,
      "loss": 0.5529,
      "step": 33100
    },
    {
      "epoch": 6.292710706150341,
      "grad_norm": 12.444308280944824,
      "learning_rate": 1.2585421412300684e-05,
      "loss": 0.621,
      "step": 33150
    },
    {
      "epoch": 6.302201974183751,
      "grad_norm": 3.158257246017456,
      "learning_rate": 1.2604403948367503e-05,
      "loss": 0.5705,
      "step": 33200
    },
    {
      "epoch": 6.31169324221716,
      "grad_norm": 12.701665878295898,
      "learning_rate": 1.2623386484434321e-05,
      "loss": 0.5515,
      "step": 33250
    },
    {
      "epoch": 6.3211845102505695,
      "grad_norm": 50.15918731689453,
      "learning_rate": 1.264236902050114e-05,
      "loss": 0.5728,
      "step": 33300
    },
    {
      "epoch": 6.330675778283979,
      "grad_norm": 12.242770195007324,
      "learning_rate": 1.2661351556567958e-05,
      "loss": 0.5826,
      "step": 33350
    },
    {
      "epoch": 6.340167046317388,
      "grad_norm": 27.671342849731445,
      "learning_rate": 1.2680334092634777e-05,
      "loss": 0.5947,
      "step": 33400
    },
    {
      "epoch": 6.349658314350798,
      "grad_norm": 2.810150384902954,
      "learning_rate": 1.2699316628701595e-05,
      "loss": 0.5868,
      "step": 33450
    },
    {
      "epoch": 6.359149582384206,
      "grad_norm": 17.296016693115234,
      "learning_rate": 1.2718299164768415e-05,
      "loss": 0.5468,
      "step": 33500
    },
    {
      "epoch": 6.3686408504176155,
      "grad_norm": 8.134387969970703,
      "learning_rate": 1.2737281700835234e-05,
      "loss": 0.5961,
      "step": 33550
    },
    {
      "epoch": 6.378132118451025,
      "grad_norm": 29.44133186340332,
      "learning_rate": 1.2756264236902052e-05,
      "loss": 0.6739,
      "step": 33600
    },
    {
      "epoch": 6.387623386484434,
      "grad_norm": 31.62057113647461,
      "learning_rate": 1.277524677296887e-05,
      "loss": 0.6036,
      "step": 33650
    },
    {
      "epoch": 6.397114654517844,
      "grad_norm": 14.191816329956055,
      "learning_rate": 1.2794229309035688e-05,
      "loss": 0.6201,
      "step": 33700
    },
    {
      "epoch": 6.406605922551253,
      "grad_norm": 23.5435733795166,
      "learning_rate": 1.2813211845102506e-05,
      "loss": 0.5271,
      "step": 33750
    },
    {
      "epoch": 6.416097190584662,
      "grad_norm": 13.52528190612793,
      "learning_rate": 1.2832194381169325e-05,
      "loss": 0.6737,
      "step": 33800
    },
    {
      "epoch": 6.425588458618071,
      "grad_norm": 15.758415222167969,
      "learning_rate": 1.2851176917236143e-05,
      "loss": 0.6006,
      "step": 33850
    },
    {
      "epoch": 6.43507972665148,
      "grad_norm": 16.414716720581055,
      "learning_rate": 1.2870159453302962e-05,
      "loss": 0.5805,
      "step": 33900
    },
    {
      "epoch": 6.44457099468489,
      "grad_norm": 12.385001182556152,
      "learning_rate": 1.288914198936978e-05,
      "loss": 0.6173,
      "step": 33950
    },
    {
      "epoch": 6.454062262718299,
      "grad_norm": 9.181899070739746,
      "learning_rate": 1.2908124525436599e-05,
      "loss": 0.5715,
      "step": 34000
    },
    {
      "epoch": 6.4635535307517085,
      "grad_norm": 36.13960647583008,
      "learning_rate": 1.292710706150342e-05,
      "loss": 0.5996,
      "step": 34050
    },
    {
      "epoch": 6.473044798785118,
      "grad_norm": 15.81373405456543,
      "learning_rate": 1.2946089597570238e-05,
      "loss": 0.5509,
      "step": 34100
    },
    {
      "epoch": 6.482536066818527,
      "grad_norm": 11.395079612731934,
      "learning_rate": 1.2965072133637056e-05,
      "loss": 0.6057,
      "step": 34150
    },
    {
      "epoch": 6.492027334851937,
      "grad_norm": 9.257925033569336,
      "learning_rate": 1.2984054669703873e-05,
      "loss": 0.629,
      "step": 34200
    },
    {
      "epoch": 6.501518602885345,
      "grad_norm": 18.49415397644043,
      "learning_rate": 1.3003037205770692e-05,
      "loss": 0.6168,
      "step": 34250
    },
    {
      "epoch": 6.5110098709187545,
      "grad_norm": 27.7918701171875,
      "learning_rate": 1.302201974183751e-05,
      "loss": 0.6051,
      "step": 34300
    },
    {
      "epoch": 6.520501138952164,
      "grad_norm": 8.317486763000488,
      "learning_rate": 1.3041002277904329e-05,
      "loss": 0.5586,
      "step": 34350
    },
    {
      "epoch": 6.529992406985573,
      "grad_norm": 9.554991722106934,
      "learning_rate": 1.3059984813971147e-05,
      "loss": 0.6019,
      "step": 34400
    },
    {
      "epoch": 6.539483675018983,
      "grad_norm": 7.342281341552734,
      "learning_rate": 1.3078967350037966e-05,
      "loss": 0.5226,
      "step": 34450
    },
    {
      "epoch": 6.548974943052392,
      "grad_norm": 10.308302879333496,
      "learning_rate": 1.3097949886104784e-05,
      "loss": 0.5884,
      "step": 34500
    },
    {
      "epoch": 6.558466211085801,
      "grad_norm": 18.5123291015625,
      "learning_rate": 1.3116932422171603e-05,
      "loss": 0.5446,
      "step": 34550
    },
    {
      "epoch": 6.56795747911921,
      "grad_norm": 40.23725891113281,
      "learning_rate": 1.3135914958238423e-05,
      "loss": 0.6664,
      "step": 34600
    },
    {
      "epoch": 6.577448747152619,
      "grad_norm": 31.055986404418945,
      "learning_rate": 1.3154897494305242e-05,
      "loss": 0.6357,
      "step": 34650
    },
    {
      "epoch": 6.586940015186029,
      "grad_norm": 10.92739200592041,
      "learning_rate": 1.317388003037206e-05,
      "loss": 0.5985,
      "step": 34700
    },
    {
      "epoch": 6.596431283219438,
      "grad_norm": 15.619224548339844,
      "learning_rate": 1.3192862566438877e-05,
      "loss": 0.6809,
      "step": 34750
    },
    {
      "epoch": 6.605922551252847,
      "grad_norm": 20.712526321411133,
      "learning_rate": 1.3211845102505695e-05,
      "loss": 0.4894,
      "step": 34800
    },
    {
      "epoch": 6.615413819286257,
      "grad_norm": 15.176057815551758,
      "learning_rate": 1.3230827638572514e-05,
      "loss": 0.5624,
      "step": 34850
    },
    {
      "epoch": 6.624905087319666,
      "grad_norm": 8.644272804260254,
      "learning_rate": 1.3249810174639332e-05,
      "loss": 0.5991,
      "step": 34900
    },
    {
      "epoch": 6.634396355353076,
      "grad_norm": 8.707335472106934,
      "learning_rate": 1.3268792710706151e-05,
      "loss": 0.5377,
      "step": 34950
    },
    {
      "epoch": 6.643887623386484,
      "grad_norm": 18.510753631591797,
      "learning_rate": 1.328777524677297e-05,
      "loss": 0.5551,
      "step": 35000
    },
    {
      "epoch": 6.6533788914198935,
      "grad_norm": 3.790358543395996,
      "learning_rate": 1.3306757782839788e-05,
      "loss": 0.5847,
      "step": 35050
    },
    {
      "epoch": 6.662870159453303,
      "grad_norm": 34.81423568725586,
      "learning_rate": 1.3325740318906607e-05,
      "loss": 0.5965,
      "step": 35100
    },
    {
      "epoch": 6.672361427486712,
      "grad_norm": 34.267913818359375,
      "learning_rate": 1.3344722854973427e-05,
      "loss": 0.6099,
      "step": 35150
    },
    {
      "epoch": 6.681852695520122,
      "grad_norm": 8.178994178771973,
      "learning_rate": 1.3363705391040245e-05,
      "loss": 0.6723,
      "step": 35200
    },
    {
      "epoch": 6.691343963553531,
      "grad_norm": 17.46973419189453,
      "learning_rate": 1.3382687927107064e-05,
      "loss": 0.5603,
      "step": 35250
    },
    {
      "epoch": 6.70083523158694,
      "grad_norm": 7.626657962799072,
      "learning_rate": 1.340167046317388e-05,
      "loss": 0.5988,
      "step": 35300
    },
    {
      "epoch": 6.710326499620349,
      "grad_norm": 14.775214195251465,
      "learning_rate": 1.34206529992407e-05,
      "loss": 0.6123,
      "step": 35350
    },
    {
      "epoch": 6.719817767653758,
      "grad_norm": 51.16766357421875,
      "learning_rate": 1.3439635535307518e-05,
      "loss": 0.5551,
      "step": 35400
    },
    {
      "epoch": 6.729309035687168,
      "grad_norm": 21.98273468017578,
      "learning_rate": 1.3458618071374336e-05,
      "loss": 0.4989,
      "step": 35450
    },
    {
      "epoch": 6.738800303720577,
      "grad_norm": 7.3901753425598145,
      "learning_rate": 1.3477600607441155e-05,
      "loss": 0.501,
      "step": 35500
    },
    {
      "epoch": 6.748291571753986,
      "grad_norm": 19.339645385742188,
      "learning_rate": 1.3496583143507973e-05,
      "loss": 0.5784,
      "step": 35550
    },
    {
      "epoch": 6.757782839787396,
      "grad_norm": 26.57708740234375,
      "learning_rate": 1.3515565679574792e-05,
      "loss": 0.5734,
      "step": 35600
    },
    {
      "epoch": 6.767274107820805,
      "grad_norm": 26.052600860595703,
      "learning_rate": 1.3534548215641612e-05,
      "loss": 0.5682,
      "step": 35650
    },
    {
      "epoch": 6.776765375854215,
      "grad_norm": 15.740824699401855,
      "learning_rate": 1.355353075170843e-05,
      "loss": 0.584,
      "step": 35700
    },
    {
      "epoch": 6.786256643887623,
      "grad_norm": 24.54867935180664,
      "learning_rate": 1.3572513287775249e-05,
      "loss": 0.5696,
      "step": 35750
    },
    {
      "epoch": 6.7957479119210324,
      "grad_norm": 16.15022850036621,
      "learning_rate": 1.3591495823842068e-05,
      "loss": 0.5912,
      "step": 35800
    },
    {
      "epoch": 6.805239179954442,
      "grad_norm": 15.723899841308594,
      "learning_rate": 1.3610478359908884e-05,
      "loss": 0.5452,
      "step": 35850
    },
    {
      "epoch": 6.814730447987851,
      "grad_norm": 8.262714385986328,
      "learning_rate": 1.3629460895975703e-05,
      "loss": 0.5285,
      "step": 35900
    },
    {
      "epoch": 6.824221716021261,
      "grad_norm": 17.555631637573242,
      "learning_rate": 1.3648443432042522e-05,
      "loss": 0.6482,
      "step": 35950
    },
    {
      "epoch": 6.83371298405467,
      "grad_norm": 16.624975204467773,
      "learning_rate": 1.366742596810934e-05,
      "loss": 0.5347,
      "step": 36000
    },
    {
      "epoch": 6.8432042520880785,
      "grad_norm": 24.982192993164062,
      "learning_rate": 1.3686408504176159e-05,
      "loss": 0.5954,
      "step": 36050
    },
    {
      "epoch": 6.852695520121488,
      "grad_norm": 9.790502548217773,
      "learning_rate": 1.3705391040242977e-05,
      "loss": 0.5586,
      "step": 36100
    },
    {
      "epoch": 6.862186788154897,
      "grad_norm": 8.842637062072754,
      "learning_rate": 1.3724373576309796e-05,
      "loss": 0.5652,
      "step": 36150
    },
    {
      "epoch": 6.871678056188307,
      "grad_norm": 14.367218017578125,
      "learning_rate": 1.3743356112376616e-05,
      "loss": 0.6209,
      "step": 36200
    },
    {
      "epoch": 6.881169324221716,
      "grad_norm": 23.760658264160156,
      "learning_rate": 1.3762338648443434e-05,
      "loss": 0.5689,
      "step": 36250
    },
    {
      "epoch": 6.890660592255125,
      "grad_norm": 40.2740592956543,
      "learning_rate": 1.3781321184510253e-05,
      "loss": 0.6168,
      "step": 36300
    },
    {
      "epoch": 6.900151860288535,
      "grad_norm": 6.037051677703857,
      "learning_rate": 1.3800303720577071e-05,
      "loss": 0.644,
      "step": 36350
    },
    {
      "epoch": 6.909643128321944,
      "grad_norm": 39.980770111083984,
      "learning_rate": 1.3819286256643888e-05,
      "loss": 0.6573,
      "step": 36400
    },
    {
      "epoch": 6.9191343963553535,
      "grad_norm": 27.368589401245117,
      "learning_rate": 1.3838268792710707e-05,
      "loss": 0.576,
      "step": 36450
    },
    {
      "epoch": 6.928625664388762,
      "grad_norm": 24.629043579101562,
      "learning_rate": 1.3857251328777525e-05,
      "loss": 0.5395,
      "step": 36500
    },
    {
      "epoch": 6.938116932422171,
      "grad_norm": 24.965784072875977,
      "learning_rate": 1.3876233864844344e-05,
      "loss": 0.5346,
      "step": 36550
    },
    {
      "epoch": 6.947608200455581,
      "grad_norm": 18.203630447387695,
      "learning_rate": 1.3895216400911162e-05,
      "loss": 0.5594,
      "step": 36600
    },
    {
      "epoch": 6.95709946848899,
      "grad_norm": 9.421103477478027,
      "learning_rate": 1.3914198936977981e-05,
      "loss": 0.4574,
      "step": 36650
    },
    {
      "epoch": 6.9665907365224,
      "grad_norm": 39.869930267333984,
      "learning_rate": 1.39331814730448e-05,
      "loss": 0.7167,
      "step": 36700
    },
    {
      "epoch": 6.976082004555809,
      "grad_norm": 31.43335723876953,
      "learning_rate": 1.395216400911162e-05,
      "loss": 0.6064,
      "step": 36750
    },
    {
      "epoch": 6.9855732725892175,
      "grad_norm": 10.218758583068848,
      "learning_rate": 1.3971146545178438e-05,
      "loss": 0.6048,
      "step": 36800
    },
    {
      "epoch": 6.995064540622627,
      "grad_norm": 8.943864822387695,
      "learning_rate": 1.3990129081245257e-05,
      "loss": 0.6038,
      "step": 36850
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.7780203093859732,
      "eval_f1": 0.7368781898102077,
      "eval_loss": 0.5791099667549133,
      "eval_precision": 0.7648168571199715,
      "eval_recall": 0.735376677394915,
      "eval_runtime": 155.2657,
      "eval_samples_per_second": 67.864,
      "eval_steps_per_second": 8.489,
      "step": 36876
    },
    {
      "epoch": 7.004555808656036,
      "grad_norm": 9.169449806213379,
      "learning_rate": 1.4009111617312075e-05,
      "loss": 0.5154,
      "step": 36900
    },
    {
      "epoch": 7.014047076689446,
      "grad_norm": 32.545654296875,
      "learning_rate": 1.4028094153378892e-05,
      "loss": 0.5605,
      "step": 36950
    },
    {
      "epoch": 7.023538344722855,
      "grad_norm": 6.282914161682129,
      "learning_rate": 1.404707668944571e-05,
      "loss": 0.4976,
      "step": 37000
    },
    {
      "epoch": 7.033029612756264,
      "grad_norm": 13.866438865661621,
      "learning_rate": 1.4066059225512529e-05,
      "loss": 0.5367,
      "step": 37050
    },
    {
      "epoch": 7.042520880789674,
      "grad_norm": 14.044926643371582,
      "learning_rate": 1.4085041761579348e-05,
      "loss": 0.5205,
      "step": 37100
    },
    {
      "epoch": 7.052012148823083,
      "grad_norm": 18.063865661621094,
      "learning_rate": 1.4104024297646166e-05,
      "loss": 0.598,
      "step": 37150
    },
    {
      "epoch": 7.061503416856492,
      "grad_norm": 27.920244216918945,
      "learning_rate": 1.4123006833712985e-05,
      "loss": 0.5472,
      "step": 37200
    },
    {
      "epoch": 7.070994684889901,
      "grad_norm": 22.36508560180664,
      "learning_rate": 1.4141989369779803e-05,
      "loss": 0.5511,
      "step": 37250
    },
    {
      "epoch": 7.08048595292331,
      "grad_norm": 14.861268997192383,
      "learning_rate": 1.4160971905846623e-05,
      "loss": 0.4907,
      "step": 37300
    },
    {
      "epoch": 7.08997722095672,
      "grad_norm": 12.970065116882324,
      "learning_rate": 1.4179954441913442e-05,
      "loss": 0.5883,
      "step": 37350
    },
    {
      "epoch": 7.099468488990129,
      "grad_norm": 15.04117202758789,
      "learning_rate": 1.419893697798026e-05,
      "loss": 0.5211,
      "step": 37400
    },
    {
      "epoch": 7.1089597570235386,
      "grad_norm": 13.045881271362305,
      "learning_rate": 1.4217919514047079e-05,
      "loss": 0.5326,
      "step": 37450
    },
    {
      "epoch": 7.118451025056948,
      "grad_norm": 21.4885196685791,
      "learning_rate": 1.4236902050113896e-05,
      "loss": 0.6153,
      "step": 37500
    },
    {
      "epoch": 7.127942293090356,
      "grad_norm": 12.773184776306152,
      "learning_rate": 1.4255884586180714e-05,
      "loss": 0.5552,
      "step": 37550
    },
    {
      "epoch": 7.137433561123766,
      "grad_norm": 25.41444969177246,
      "learning_rate": 1.4274867122247533e-05,
      "loss": 0.5103,
      "step": 37600
    },
    {
      "epoch": 7.146924829157175,
      "grad_norm": 9.807701110839844,
      "learning_rate": 1.4293849658314351e-05,
      "loss": 0.4908,
      "step": 37650
    },
    {
      "epoch": 7.156416097190585,
      "grad_norm": 6.97927713394165,
      "learning_rate": 1.431283219438117e-05,
      "loss": 0.4808,
      "step": 37700
    },
    {
      "epoch": 7.165907365223994,
      "grad_norm": 6.281946182250977,
      "learning_rate": 1.4331814730447989e-05,
      "loss": 0.6082,
      "step": 37750
    },
    {
      "epoch": 7.175398633257403,
      "grad_norm": 18.0386962890625,
      "learning_rate": 1.4350797266514807e-05,
      "loss": 0.5315,
      "step": 37800
    },
    {
      "epoch": 7.184889901290813,
      "grad_norm": 3.099024534225464,
      "learning_rate": 1.4369779802581627e-05,
      "loss": 0.6048,
      "step": 37850
    },
    {
      "epoch": 7.194381169324222,
      "grad_norm": 20.58028793334961,
      "learning_rate": 1.4388762338648446e-05,
      "loss": 0.53,
      "step": 37900
    },
    {
      "epoch": 7.203872437357631,
      "grad_norm": 11.147360801696777,
      "learning_rate": 1.4407744874715264e-05,
      "loss": 0.6402,
      "step": 37950
    },
    {
      "epoch": 7.21336370539104,
      "grad_norm": 23.77498435974121,
      "learning_rate": 1.4426727410782083e-05,
      "loss": 0.573,
      "step": 38000
    },
    {
      "epoch": 7.222854973424449,
      "grad_norm": 5.749937534332275,
      "learning_rate": 1.44457099468489e-05,
      "loss": 0.538,
      "step": 38050
    },
    {
      "epoch": 7.232346241457859,
      "grad_norm": 8.550345420837402,
      "learning_rate": 1.4464692482915718e-05,
      "loss": 0.5156,
      "step": 38100
    },
    {
      "epoch": 7.241837509491268,
      "grad_norm": 14.97209358215332,
      "learning_rate": 1.4483675018982537e-05,
      "loss": 0.5924,
      "step": 38150
    },
    {
      "epoch": 7.2513287775246775,
      "grad_norm": 26.83019256591797,
      "learning_rate": 1.4502657555049355e-05,
      "loss": 0.5491,
      "step": 38200
    },
    {
      "epoch": 7.260820045558087,
      "grad_norm": 23.25040054321289,
      "learning_rate": 1.4521640091116174e-05,
      "loss": 0.5955,
      "step": 38250
    },
    {
      "epoch": 7.270311313591495,
      "grad_norm": 4.579469680786133,
      "learning_rate": 1.4540622627182992e-05,
      "loss": 0.5373,
      "step": 38300
    },
    {
      "epoch": 7.279802581624905,
      "grad_norm": 13.063370704650879,
      "learning_rate": 1.4559605163249811e-05,
      "loss": 0.5026,
      "step": 38350
    },
    {
      "epoch": 7.289293849658314,
      "grad_norm": 27.334388732910156,
      "learning_rate": 1.4578587699316631e-05,
      "loss": 0.5878,
      "step": 38400
    },
    {
      "epoch": 7.298785117691724,
      "grad_norm": 20.359375,
      "learning_rate": 1.459757023538345e-05,
      "loss": 0.603,
      "step": 38450
    },
    {
      "epoch": 7.308276385725133,
      "grad_norm": 13.299768447875977,
      "learning_rate": 1.4616552771450268e-05,
      "loss": 0.6081,
      "step": 38500
    },
    {
      "epoch": 7.317767653758542,
      "grad_norm": 32.643856048583984,
      "learning_rate": 1.4635535307517087e-05,
      "loss": 0.5532,
      "step": 38550
    },
    {
      "epoch": 7.327258921791952,
      "grad_norm": 10.25902271270752,
      "learning_rate": 1.4654517843583904e-05,
      "loss": 0.4765,
      "step": 38600
    },
    {
      "epoch": 7.336750189825361,
      "grad_norm": 23.456050872802734,
      "learning_rate": 1.4673500379650722e-05,
      "loss": 0.4545,
      "step": 38650
    },
    {
      "epoch": 7.34624145785877,
      "grad_norm": 11.795975685119629,
      "learning_rate": 1.469248291571754e-05,
      "loss": 0.4989,
      "step": 38700
    },
    {
      "epoch": 7.355732725892179,
      "grad_norm": 25.56654167175293,
      "learning_rate": 1.4711465451784359e-05,
      "loss": 0.5679,
      "step": 38750
    },
    {
      "epoch": 7.365223993925588,
      "grad_norm": 8.09856128692627,
      "learning_rate": 1.4730447987851178e-05,
      "loss": 0.5042,
      "step": 38800
    },
    {
      "epoch": 7.374715261958998,
      "grad_norm": 17.269933700561523,
      "learning_rate": 1.4749430523917996e-05,
      "loss": 0.5194,
      "step": 38850
    },
    {
      "epoch": 7.384206529992407,
      "grad_norm": 14.640281677246094,
      "learning_rate": 1.4768413059984815e-05,
      "loss": 0.4888,
      "step": 38900
    },
    {
      "epoch": 7.3936977980258165,
      "grad_norm": 34.97153091430664,
      "learning_rate": 1.4787395596051635e-05,
      "loss": 0.5835,
      "step": 38950
    },
    {
      "epoch": 7.403189066059226,
      "grad_norm": 15.552140235900879,
      "learning_rate": 1.4806378132118453e-05,
      "loss": 0.5568,
      "step": 39000
    },
    {
      "epoch": 7.412680334092634,
      "grad_norm": 22.362281799316406,
      "learning_rate": 1.4825360668185272e-05,
      "loss": 0.5939,
      "step": 39050
    },
    {
      "epoch": 7.422171602126044,
      "grad_norm": 31.79573631286621,
      "learning_rate": 1.4844343204252089e-05,
      "loss": 0.4973,
      "step": 39100
    },
    {
      "epoch": 7.431662870159453,
      "grad_norm": 33.198219299316406,
      "learning_rate": 1.4863325740318907e-05,
      "loss": 0.546,
      "step": 39150
    },
    {
      "epoch": 7.4411541381928625,
      "grad_norm": 9.679105758666992,
      "learning_rate": 1.4882308276385726e-05,
      "loss": 0.5782,
      "step": 39200
    },
    {
      "epoch": 7.450645406226272,
      "grad_norm": 12.812886238098145,
      "learning_rate": 1.4901290812452544e-05,
      "loss": 0.6085,
      "step": 39250
    },
    {
      "epoch": 7.460136674259681,
      "grad_norm": 13.369176864624023,
      "learning_rate": 1.4920273348519363e-05,
      "loss": 0.6144,
      "step": 39300
    },
    {
      "epoch": 7.469627942293091,
      "grad_norm": 6.147424697875977,
      "learning_rate": 1.4939255884586181e-05,
      "loss": 0.4839,
      "step": 39350
    },
    {
      "epoch": 7.4791192103265,
      "grad_norm": 9.175309181213379,
      "learning_rate": 1.4958238420653e-05,
      "loss": 0.4382,
      "step": 39400
    },
    {
      "epoch": 7.488610478359909,
      "grad_norm": 7.491729259490967,
      "learning_rate": 1.4977220956719818e-05,
      "loss": 0.5269,
      "step": 39450
    },
    {
      "epoch": 7.498101746393318,
      "grad_norm": 8.839003562927246,
      "learning_rate": 1.4996203492786639e-05,
      "loss": 0.587,
      "step": 39500
    },
    {
      "epoch": 7.507593014426727,
      "grad_norm": 8.807816505432129,
      "learning_rate": 1.5015186028853457e-05,
      "loss": 0.5434,
      "step": 39550
    },
    {
      "epoch": 7.517084282460137,
      "grad_norm": 27.357179641723633,
      "learning_rate": 1.5034168564920276e-05,
      "loss": 0.5571,
      "step": 39600
    },
    {
      "epoch": 7.526575550493546,
      "grad_norm": 9.317084312438965,
      "learning_rate": 1.5053151100987093e-05,
      "loss": 0.5443,
      "step": 39650
    },
    {
      "epoch": 7.5360668185269555,
      "grad_norm": 22.372400283813477,
      "learning_rate": 1.5072133637053911e-05,
      "loss": 0.5767,
      "step": 39700
    },
    {
      "epoch": 7.545558086560364,
      "grad_norm": 25.443571090698242,
      "learning_rate": 1.509111617312073e-05,
      "loss": 0.532,
      "step": 39750
    },
    {
      "epoch": 7.555049354593773,
      "grad_norm": 48.970157623291016,
      "learning_rate": 1.5110098709187548e-05,
      "loss": 0.6107,
      "step": 39800
    },
    {
      "epoch": 7.564540622627183,
      "grad_norm": 16.690975189208984,
      "learning_rate": 1.5129081245254367e-05,
      "loss": 0.6441,
      "step": 39850
    },
    {
      "epoch": 7.574031890660592,
      "grad_norm": 7.1419291496276855,
      "learning_rate": 1.5148063781321185e-05,
      "loss": 0.5946,
      "step": 39900
    },
    {
      "epoch": 7.5835231586940015,
      "grad_norm": 13.152792930603027,
      "learning_rate": 1.5167046317388004e-05,
      "loss": 0.6159,
      "step": 39950
    },
    {
      "epoch": 7.593014426727411,
      "grad_norm": 30.973020553588867,
      "learning_rate": 1.5186028853454824e-05,
      "loss": 0.5444,
      "step": 40000
    },
    {
      "epoch": 7.60250569476082,
      "grad_norm": 6.959517002105713,
      "learning_rate": 1.5205011389521643e-05,
      "loss": 0.4932,
      "step": 40050
    },
    {
      "epoch": 7.61199696279423,
      "grad_norm": 15.629541397094727,
      "learning_rate": 1.5223993925588461e-05,
      "loss": 0.6499,
      "step": 40100
    },
    {
      "epoch": 7.621488230827639,
      "grad_norm": 9.137734413146973,
      "learning_rate": 1.524297646165528e-05,
      "loss": 0.5515,
      "step": 40150
    },
    {
      "epoch": 7.6309794988610475,
      "grad_norm": 14.317780494689941,
      "learning_rate": 1.5261958997722096e-05,
      "loss": 0.5297,
      "step": 40200
    },
    {
      "epoch": 7.640470766894457,
      "grad_norm": 12.816329002380371,
      "learning_rate": 1.5280941533788915e-05,
      "loss": 0.508,
      "step": 40250
    },
    {
      "epoch": 7.649962034927866,
      "grad_norm": 26.346622467041016,
      "learning_rate": 1.5299924069855733e-05,
      "loss": 0.5419,
      "step": 40300
    },
    {
      "epoch": 7.659453302961276,
      "grad_norm": 24.487070083618164,
      "learning_rate": 1.5318906605922552e-05,
      "loss": 0.4558,
      "step": 40350
    },
    {
      "epoch": 7.668944570994685,
      "grad_norm": 19.231412887573242,
      "learning_rate": 1.533788914198937e-05,
      "loss": 0.5998,
      "step": 40400
    },
    {
      "epoch": 7.6784358390280945,
      "grad_norm": 2.098339796066284,
      "learning_rate": 1.535687167805619e-05,
      "loss": 0.5789,
      "step": 40450
    },
    {
      "epoch": 7.687927107061503,
      "grad_norm": 12.110445976257324,
      "learning_rate": 1.5375854214123008e-05,
      "loss": 0.4767,
      "step": 40500
    },
    {
      "epoch": 7.697418375094912,
      "grad_norm": 16.23146629333496,
      "learning_rate": 1.5394836750189826e-05,
      "loss": 0.6075,
      "step": 40550
    },
    {
      "epoch": 7.706909643128322,
      "grad_norm": 6.286144733428955,
      "learning_rate": 1.5413819286256645e-05,
      "loss": 0.4816,
      "step": 40600
    },
    {
      "epoch": 7.716400911161731,
      "grad_norm": 8.386427879333496,
      "learning_rate": 1.5432801822323463e-05,
      "loss": 0.5919,
      "step": 40650
    },
    {
      "epoch": 7.7258921791951405,
      "grad_norm": 21.27353858947754,
      "learning_rate": 1.545178435839028e-05,
      "loss": 0.5457,
      "step": 40700
    },
    {
      "epoch": 7.73538344722855,
      "grad_norm": 22.27974510192871,
      "learning_rate": 1.54707668944571e-05,
      "loss": 0.5803,
      "step": 40750
    },
    {
      "epoch": 7.744874715261959,
      "grad_norm": 15.341715812683105,
      "learning_rate": 1.548974943052392e-05,
      "loss": 0.5491,
      "step": 40800
    },
    {
      "epoch": 7.754365983295369,
      "grad_norm": 27.858402252197266,
      "learning_rate": 1.5508731966590737e-05,
      "loss": 0.4728,
      "step": 40850
    },
    {
      "epoch": 7.763857251328778,
      "grad_norm": 18.61507225036621,
      "learning_rate": 1.5527714502657556e-05,
      "loss": 0.5067,
      "step": 40900
    },
    {
      "epoch": 7.7733485193621865,
      "grad_norm": 19.063861846923828,
      "learning_rate": 1.5546697038724374e-05,
      "loss": 0.5513,
      "step": 40950
    },
    {
      "epoch": 7.782839787395596,
      "grad_norm": 22.48990821838379,
      "learning_rate": 1.5565679574791193e-05,
      "loss": 0.588,
      "step": 41000
    },
    {
      "epoch": 7.792331055429005,
      "grad_norm": 11.17407512664795,
      "learning_rate": 1.558466211085801e-05,
      "loss": 0.5044,
      "step": 41050
    },
    {
      "epoch": 7.801822323462415,
      "grad_norm": 21.077167510986328,
      "learning_rate": 1.560364464692483e-05,
      "loss": 0.556,
      "step": 41100
    },
    {
      "epoch": 7.811313591495824,
      "grad_norm": 24.125106811523438,
      "learning_rate": 1.562262718299165e-05,
      "loss": 0.5548,
      "step": 41150
    },
    {
      "epoch": 7.820804859529233,
      "grad_norm": 30.685630798339844,
      "learning_rate": 1.5641609719058467e-05,
      "loss": 0.4655,
      "step": 41200
    },
    {
      "epoch": 7.830296127562642,
      "grad_norm": 18.51430892944336,
      "learning_rate": 1.5660592255125285e-05,
      "loss": 0.5286,
      "step": 41250
    },
    {
      "epoch": 7.839787395596051,
      "grad_norm": 33.769996643066406,
      "learning_rate": 1.5679574791192104e-05,
      "loss": 0.5783,
      "step": 41300
    },
    {
      "epoch": 7.849278663629461,
      "grad_norm": 17.50501251220703,
      "learning_rate": 1.5698557327258923e-05,
      "loss": 0.5942,
      "step": 41350
    },
    {
      "epoch": 7.85876993166287,
      "grad_norm": 6.640693187713623,
      "learning_rate": 1.571753986332574e-05,
      "loss": 0.6112,
      "step": 41400
    },
    {
      "epoch": 7.8682611996962795,
      "grad_norm": 22.600765228271484,
      "learning_rate": 1.573652239939256e-05,
      "loss": 0.4319,
      "step": 41450
    },
    {
      "epoch": 7.877752467729689,
      "grad_norm": 4.588994026184082,
      "learning_rate": 1.5755504935459378e-05,
      "loss": 0.5195,
      "step": 41500
    },
    {
      "epoch": 7.887243735763098,
      "grad_norm": 18.46226692199707,
      "learning_rate": 1.5774487471526197e-05,
      "loss": 0.5439,
      "step": 41550
    },
    {
      "epoch": 7.896735003796508,
      "grad_norm": 16.3050479888916,
      "learning_rate": 1.5793470007593015e-05,
      "loss": 0.5831,
      "step": 41600
    },
    {
      "epoch": 7.906226271829916,
      "grad_norm": 9.04898452758789,
      "learning_rate": 1.5812452543659834e-05,
      "loss": 0.4519,
      "step": 41650
    },
    {
      "epoch": 7.9157175398633255,
      "grad_norm": 20.203001022338867,
      "learning_rate": 1.5831435079726652e-05,
      "loss": 0.5412,
      "step": 41700
    },
    {
      "epoch": 7.925208807896735,
      "grad_norm": 4.9344916343688965,
      "learning_rate": 1.585041761579347e-05,
      "loss": 0.5105,
      "step": 41750
    },
    {
      "epoch": 7.934700075930144,
      "grad_norm": 22.87862777709961,
      "learning_rate": 1.586940015186029e-05,
      "loss": 0.5473,
      "step": 41800
    },
    {
      "epoch": 7.944191343963554,
      "grad_norm": 16.837984085083008,
      "learning_rate": 1.5888382687927108e-05,
      "loss": 0.5948,
      "step": 41850
    },
    {
      "epoch": 7.953682611996963,
      "grad_norm": 25.01272201538086,
      "learning_rate": 1.5907365223993926e-05,
      "loss": 0.5632,
      "step": 41900
    },
    {
      "epoch": 7.963173880030372,
      "grad_norm": 1.1233195066452026,
      "learning_rate": 1.5926347760060745e-05,
      "loss": 0.5207,
      "step": 41950
    },
    {
      "epoch": 7.972665148063781,
      "grad_norm": 55.873863220214844,
      "learning_rate": 1.5945330296127563e-05,
      "loss": 0.5479,
      "step": 42000
    },
    {
      "epoch": 7.98215641609719,
      "grad_norm": 4.438815593719482,
      "learning_rate": 1.5964312832194382e-05,
      "loss": 0.4956,
      "step": 42050
    },
    {
      "epoch": 7.9916476841306,
      "grad_norm": 18.541034698486328,
      "learning_rate": 1.59832953682612e-05,
      "loss": 0.548,
      "step": 42100
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.7983296953592104,
      "eval_f1": 0.7584251848607737,
      "eval_loss": 0.5423048734664917,
      "eval_precision": 0.7777225404377974,
      "eval_recall": 0.7548274067061504,
      "eval_runtime": 155.9128,
      "eval_samples_per_second": 67.583,
      "eval_steps_per_second": 8.453,
      "step": 42144
    }
  ],
  "logging_steps": 50,
  "max_steps": 263400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1671225932906496.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
