{
  "best_metric": 0.737375795841217,
  "best_model_checkpoint": "./results\\checkpoint-21072",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 21072,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009491268033409264,
      "grad_norm": 6.781533718109131,
      "learning_rate": 1.8982536066818528e-08,
      "loss": 1.9045,
      "step": 50
    },
    {
      "epoch": 0.018982536066818528,
      "grad_norm": 7.266733646392822,
      "learning_rate": 3.7965072133637056e-08,
      "loss": 1.8964,
      "step": 100
    },
    {
      "epoch": 0.02847380410022779,
      "grad_norm": 7.532681941986084,
      "learning_rate": 5.694760820045559e-08,
      "loss": 1.9081,
      "step": 150
    },
    {
      "epoch": 0.037965072133637055,
      "grad_norm": 6.661933422088623,
      "learning_rate": 7.593014426727411e-08,
      "loss": 1.8962,
      "step": 200
    },
    {
      "epoch": 0.04745634016704632,
      "grad_norm": 4.782284259796143,
      "learning_rate": 9.491268033409265e-08,
      "loss": 1.9219,
      "step": 250
    },
    {
      "epoch": 0.05694760820045558,
      "grad_norm": 6.672574043273926,
      "learning_rate": 1.1389521640091118e-07,
      "loss": 1.8861,
      "step": 300
    },
    {
      "epoch": 0.06643887623386484,
      "grad_norm": 6.003439903259277,
      "learning_rate": 1.328777524677297e-07,
      "loss": 1.9094,
      "step": 350
    },
    {
      "epoch": 0.07593014426727411,
      "grad_norm": 6.052431583404541,
      "learning_rate": 1.5186028853454822e-07,
      "loss": 1.93,
      "step": 400
    },
    {
      "epoch": 0.08542141230068337,
      "grad_norm": 8.3948392868042,
      "learning_rate": 1.7084282460136675e-07,
      "loss": 1.9126,
      "step": 450
    },
    {
      "epoch": 0.09491268033409264,
      "grad_norm": 6.39821720123291,
      "learning_rate": 1.898253606681853e-07,
      "loss": 1.8955,
      "step": 500
    },
    {
      "epoch": 0.1044039483675019,
      "grad_norm": 6.71638298034668,
      "learning_rate": 2.0880789673500383e-07,
      "loss": 1.885,
      "step": 550
    },
    {
      "epoch": 0.11389521640091116,
      "grad_norm": 5.5600361824035645,
      "learning_rate": 2.2779043280182236e-07,
      "loss": 1.8805,
      "step": 600
    },
    {
      "epoch": 0.12338648443432043,
      "grad_norm": 8.162986755371094,
      "learning_rate": 2.4677296886864086e-07,
      "loss": 1.8604,
      "step": 650
    },
    {
      "epoch": 0.13287775246772968,
      "grad_norm": 6.437964916229248,
      "learning_rate": 2.657555049354594e-07,
      "loss": 1.88,
      "step": 700
    },
    {
      "epoch": 0.14236902050113895,
      "grad_norm": 6.0843186378479,
      "learning_rate": 2.847380410022779e-07,
      "loss": 1.8939,
      "step": 750
    },
    {
      "epoch": 0.15186028853454822,
      "grad_norm": 6.881246566772461,
      "learning_rate": 3.0372057706909645e-07,
      "loss": 1.8767,
      "step": 800
    },
    {
      "epoch": 0.16135155656795747,
      "grad_norm": 7.419300556182861,
      "learning_rate": 3.22703113135915e-07,
      "loss": 1.8718,
      "step": 850
    },
    {
      "epoch": 0.17084282460136674,
      "grad_norm": 5.736841678619385,
      "learning_rate": 3.416856492027335e-07,
      "loss": 1.8611,
      "step": 900
    },
    {
      "epoch": 0.180334092634776,
      "grad_norm": 6.552237033843994,
      "learning_rate": 3.6066818526955203e-07,
      "loss": 1.8646,
      "step": 950
    },
    {
      "epoch": 0.18982536066818528,
      "grad_norm": 7.06653356552124,
      "learning_rate": 3.796507213363706e-07,
      "loss": 1.8617,
      "step": 1000
    },
    {
      "epoch": 0.19931662870159453,
      "grad_norm": 6.590609073638916,
      "learning_rate": 3.9863325740318914e-07,
      "loss": 1.8511,
      "step": 1050
    },
    {
      "epoch": 0.2088078967350038,
      "grad_norm": 8.35180950164795,
      "learning_rate": 4.1761579347000767e-07,
      "loss": 1.8463,
      "step": 1100
    },
    {
      "epoch": 0.21829916476841307,
      "grad_norm": 7.201739311218262,
      "learning_rate": 4.365983295368262e-07,
      "loss": 1.8574,
      "step": 1150
    },
    {
      "epoch": 0.22779043280182232,
      "grad_norm": 6.361911773681641,
      "learning_rate": 4.555808656036447e-07,
      "loss": 1.8475,
      "step": 1200
    },
    {
      "epoch": 0.2372817008352316,
      "grad_norm": 8.124828338623047,
      "learning_rate": 4.7456340167046325e-07,
      "loss": 1.8672,
      "step": 1250
    },
    {
      "epoch": 0.24677296886864086,
      "grad_norm": 7.5856804847717285,
      "learning_rate": 4.935459377372817e-07,
      "loss": 1.8176,
      "step": 1300
    },
    {
      "epoch": 0.25626423690205014,
      "grad_norm": 5.980137825012207,
      "learning_rate": 5.125284738041003e-07,
      "loss": 1.8091,
      "step": 1350
    },
    {
      "epoch": 0.26575550493545935,
      "grad_norm": 7.428492069244385,
      "learning_rate": 5.315110098709188e-07,
      "loss": 1.8369,
      "step": 1400
    },
    {
      "epoch": 0.2752467729688686,
      "grad_norm": 6.454822063446045,
      "learning_rate": 5.504935459377373e-07,
      "loss": 1.8164,
      "step": 1450
    },
    {
      "epoch": 0.2847380410022779,
      "grad_norm": 6.958491325378418,
      "learning_rate": 5.694760820045558e-07,
      "loss": 1.7924,
      "step": 1500
    },
    {
      "epoch": 0.29422930903568717,
      "grad_norm": 6.073852062225342,
      "learning_rate": 5.884586180713744e-07,
      "loss": 1.7821,
      "step": 1550
    },
    {
      "epoch": 0.30372057706909644,
      "grad_norm": 6.060319423675537,
      "learning_rate": 6.074411541381929e-07,
      "loss": 1.8076,
      "step": 1600
    },
    {
      "epoch": 0.3132118451025057,
      "grad_norm": 6.922138214111328,
      "learning_rate": 6.264236902050115e-07,
      "loss": 1.7945,
      "step": 1650
    },
    {
      "epoch": 0.32270311313591493,
      "grad_norm": 8.807982444763184,
      "learning_rate": 6.4540622627183e-07,
      "loss": 1.7946,
      "step": 1700
    },
    {
      "epoch": 0.3321943811693242,
      "grad_norm": 8.394856452941895,
      "learning_rate": 6.643887623386485e-07,
      "loss": 1.7592,
      "step": 1750
    },
    {
      "epoch": 0.3416856492027335,
      "grad_norm": 7.335812568664551,
      "learning_rate": 6.83371298405467e-07,
      "loss": 1.7673,
      "step": 1800
    },
    {
      "epoch": 0.35117691723614275,
      "grad_norm": 5.55386209487915,
      "learning_rate": 7.023538344722855e-07,
      "loss": 1.7621,
      "step": 1850
    },
    {
      "epoch": 0.360668185269552,
      "grad_norm": 6.270002841949463,
      "learning_rate": 7.213363705391041e-07,
      "loss": 1.7644,
      "step": 1900
    },
    {
      "epoch": 0.3701594533029613,
      "grad_norm": 6.1040472984313965,
      "learning_rate": 7.403189066059226e-07,
      "loss": 1.742,
      "step": 1950
    },
    {
      "epoch": 0.37965072133637057,
      "grad_norm": 7.086050987243652,
      "learning_rate": 7.593014426727412e-07,
      "loss": 1.7636,
      "step": 2000
    },
    {
      "epoch": 0.3891419893697798,
      "grad_norm": 6.752301216125488,
      "learning_rate": 7.782839787395596e-07,
      "loss": 1.7136,
      "step": 2050
    },
    {
      "epoch": 0.39863325740318906,
      "grad_norm": 7.262324333190918,
      "learning_rate": 7.972665148063783e-07,
      "loss": 1.7126,
      "step": 2100
    },
    {
      "epoch": 0.40812452543659833,
      "grad_norm": 6.354671478271484,
      "learning_rate": 8.162490508731967e-07,
      "loss": 1.768,
      "step": 2150
    },
    {
      "epoch": 0.4176157934700076,
      "grad_norm": 6.536283016204834,
      "learning_rate": 8.352315869400153e-07,
      "loss": 1.7133,
      "step": 2200
    },
    {
      "epoch": 0.4271070615034169,
      "grad_norm": 5.915724277496338,
      "learning_rate": 8.542141230068338e-07,
      "loss": 1.6628,
      "step": 2250
    },
    {
      "epoch": 0.43659832953682615,
      "grad_norm": 6.443665981292725,
      "learning_rate": 8.731966590736524e-07,
      "loss": 1.6854,
      "step": 2300
    },
    {
      "epoch": 0.44608959757023536,
      "grad_norm": 6.667612552642822,
      "learning_rate": 8.921791951404708e-07,
      "loss": 1.6875,
      "step": 2350
    },
    {
      "epoch": 0.45558086560364464,
      "grad_norm": 5.933440208435059,
      "learning_rate": 9.111617312072894e-07,
      "loss": 1.6823,
      "step": 2400
    },
    {
      "epoch": 0.4650721336370539,
      "grad_norm": 5.7874932289123535,
      "learning_rate": 9.301442672741079e-07,
      "loss": 1.6848,
      "step": 2450
    },
    {
      "epoch": 0.4745634016704632,
      "grad_norm": 6.891671180725098,
      "learning_rate": 9.491268033409265e-07,
      "loss": 1.665,
      "step": 2500
    },
    {
      "epoch": 0.48405466970387245,
      "grad_norm": 6.06433629989624,
      "learning_rate": 9.68109339407745e-07,
      "loss": 1.6278,
      "step": 2550
    },
    {
      "epoch": 0.4935459377372817,
      "grad_norm": 6.95818567276001,
      "learning_rate": 9.870918754745634e-07,
      "loss": 1.6393,
      "step": 2600
    },
    {
      "epoch": 0.503037205770691,
      "grad_norm": 6.200324535369873,
      "learning_rate": 1.006074411541382e-06,
      "loss": 1.6185,
      "step": 2650
    },
    {
      "epoch": 0.5125284738041003,
      "grad_norm": 6.010601043701172,
      "learning_rate": 1.0250569476082005e-06,
      "loss": 1.5811,
      "step": 2700
    },
    {
      "epoch": 0.5220197418375095,
      "grad_norm": 8.4634428024292,
      "learning_rate": 1.044039483675019e-06,
      "loss": 1.599,
      "step": 2750
    },
    {
      "epoch": 0.5315110098709187,
      "grad_norm": 5.99493408203125,
      "learning_rate": 1.0630220197418376e-06,
      "loss": 1.6122,
      "step": 2800
    },
    {
      "epoch": 0.541002277904328,
      "grad_norm": 5.33767032623291,
      "learning_rate": 1.082004555808656e-06,
      "loss": 1.6236,
      "step": 2850
    },
    {
      "epoch": 0.5504935459377372,
      "grad_norm": 5.12007999420166,
      "learning_rate": 1.1009870918754746e-06,
      "loss": 1.604,
      "step": 2900
    },
    {
      "epoch": 0.5599848139711465,
      "grad_norm": 6.034331321716309,
      "learning_rate": 1.1199696279422931e-06,
      "loss": 1.6182,
      "step": 2950
    },
    {
      "epoch": 0.5694760820045558,
      "grad_norm": 5.838982582092285,
      "learning_rate": 1.1389521640091117e-06,
      "loss": 1.5432,
      "step": 3000
    },
    {
      "epoch": 0.5789673500379651,
      "grad_norm": 5.8897223472595215,
      "learning_rate": 1.1579347000759302e-06,
      "loss": 1.5486,
      "step": 3050
    },
    {
      "epoch": 0.5884586180713743,
      "grad_norm": 5.698884963989258,
      "learning_rate": 1.1769172361427487e-06,
      "loss": 1.5531,
      "step": 3100
    },
    {
      "epoch": 0.5979498861047836,
      "grad_norm": 6.753091335296631,
      "learning_rate": 1.1958997722095673e-06,
      "loss": 1.526,
      "step": 3150
    },
    {
      "epoch": 0.6074411541381929,
      "grad_norm": 5.610910415649414,
      "learning_rate": 1.2148823082763858e-06,
      "loss": 1.497,
      "step": 3200
    },
    {
      "epoch": 0.6169324221716022,
      "grad_norm": 5.57576847076416,
      "learning_rate": 1.2338648443432043e-06,
      "loss": 1.4717,
      "step": 3250
    },
    {
      "epoch": 0.6264236902050114,
      "grad_norm": 5.431122303009033,
      "learning_rate": 1.252847380410023e-06,
      "loss": 1.5459,
      "step": 3300
    },
    {
      "epoch": 0.6359149582384207,
      "grad_norm": 5.800791263580322,
      "learning_rate": 1.2718299164768414e-06,
      "loss": 1.4849,
      "step": 3350
    },
    {
      "epoch": 0.6454062262718299,
      "grad_norm": 6.740391254425049,
      "learning_rate": 1.29081245254366e-06,
      "loss": 1.547,
      "step": 3400
    },
    {
      "epoch": 0.6548974943052391,
      "grad_norm": 6.926745414733887,
      "learning_rate": 1.3097949886104786e-06,
      "loss": 1.6048,
      "step": 3450
    },
    {
      "epoch": 0.6643887623386484,
      "grad_norm": 6.111976146697998,
      "learning_rate": 1.328777524677297e-06,
      "loss": 1.5252,
      "step": 3500
    },
    {
      "epoch": 0.6738800303720577,
      "grad_norm": 7.135305881500244,
      "learning_rate": 1.3477600607441155e-06,
      "loss": 1.5267,
      "step": 3550
    },
    {
      "epoch": 0.683371298405467,
      "grad_norm": 6.208704948425293,
      "learning_rate": 1.366742596810934e-06,
      "loss": 1.4807,
      "step": 3600
    },
    {
      "epoch": 0.6928625664388762,
      "grad_norm": 7.233537673950195,
      "learning_rate": 1.3857251328777527e-06,
      "loss": 1.5276,
      "step": 3650
    },
    {
      "epoch": 0.7023538344722855,
      "grad_norm": 5.419264793395996,
      "learning_rate": 1.404707668944571e-06,
      "loss": 1.4692,
      "step": 3700
    },
    {
      "epoch": 0.7118451025056948,
      "grad_norm": 5.498508453369141,
      "learning_rate": 1.4236902050113896e-06,
      "loss": 1.4666,
      "step": 3750
    },
    {
      "epoch": 0.721336370539104,
      "grad_norm": 5.954653263092041,
      "learning_rate": 1.4426727410782081e-06,
      "loss": 1.4721,
      "step": 3800
    },
    {
      "epoch": 0.7308276385725133,
      "grad_norm": 5.4256205558776855,
      "learning_rate": 1.4616552771450269e-06,
      "loss": 1.4695,
      "step": 3850
    },
    {
      "epoch": 0.7403189066059226,
      "grad_norm": 5.338573932647705,
      "learning_rate": 1.4806378132118452e-06,
      "loss": 1.498,
      "step": 3900
    },
    {
      "epoch": 0.7498101746393319,
      "grad_norm": 5.638768672943115,
      "learning_rate": 1.4996203492786637e-06,
      "loss": 1.4868,
      "step": 3950
    },
    {
      "epoch": 0.7593014426727411,
      "grad_norm": 6.837031364440918,
      "learning_rate": 1.5186028853454824e-06,
      "loss": 1.4734,
      "step": 4000
    },
    {
      "epoch": 0.7687927107061503,
      "grad_norm": 5.757120609283447,
      "learning_rate": 1.537585421412301e-06,
      "loss": 1.4525,
      "step": 4050
    },
    {
      "epoch": 0.7782839787395596,
      "grad_norm": 4.101736068725586,
      "learning_rate": 1.5565679574791193e-06,
      "loss": 1.4537,
      "step": 4100
    },
    {
      "epoch": 0.7877752467729688,
      "grad_norm": 5.155644416809082,
      "learning_rate": 1.5755504935459378e-06,
      "loss": 1.4402,
      "step": 4150
    },
    {
      "epoch": 0.7972665148063781,
      "grad_norm": 5.0357770919799805,
      "learning_rate": 1.5945330296127566e-06,
      "loss": 1.3498,
      "step": 4200
    },
    {
      "epoch": 0.8067577828397874,
      "grad_norm": 5.147688388824463,
      "learning_rate": 1.6135155656795749e-06,
      "loss": 1.4572,
      "step": 4250
    },
    {
      "epoch": 0.8162490508731967,
      "grad_norm": 8.150679588317871,
      "learning_rate": 1.6324981017463934e-06,
      "loss": 1.4161,
      "step": 4300
    },
    {
      "epoch": 0.8257403189066059,
      "grad_norm": 5.87668514251709,
      "learning_rate": 1.651480637813212e-06,
      "loss": 1.45,
      "step": 4350
    },
    {
      "epoch": 0.8352315869400152,
      "grad_norm": 6.621004104614258,
      "learning_rate": 1.6704631738800307e-06,
      "loss": 1.4432,
      "step": 4400
    },
    {
      "epoch": 0.8447228549734245,
      "grad_norm": 5.4572649002075195,
      "learning_rate": 1.689445709946849e-06,
      "loss": 1.4434,
      "step": 4450
    },
    {
      "epoch": 0.8542141230068337,
      "grad_norm": 5.729280948638916,
      "learning_rate": 1.7084282460136675e-06,
      "loss": 1.4217,
      "step": 4500
    },
    {
      "epoch": 0.863705391040243,
      "grad_norm": 6.268518447875977,
      "learning_rate": 1.727410782080486e-06,
      "loss": 1.3319,
      "step": 4550
    },
    {
      "epoch": 0.8731966590736523,
      "grad_norm": 5.37209939956665,
      "learning_rate": 1.7463933181473048e-06,
      "loss": 1.4134,
      "step": 4600
    },
    {
      "epoch": 0.8826879271070615,
      "grad_norm": 5.681509494781494,
      "learning_rate": 1.765375854214123e-06,
      "loss": 1.4411,
      "step": 4650
    },
    {
      "epoch": 0.8921791951404707,
      "grad_norm": 5.10499382019043,
      "learning_rate": 1.7843583902809416e-06,
      "loss": 1.3592,
      "step": 4700
    },
    {
      "epoch": 0.90167046317388,
      "grad_norm": 5.10657262802124,
      "learning_rate": 1.8033409263477604e-06,
      "loss": 1.3716,
      "step": 4750
    },
    {
      "epoch": 0.9111617312072893,
      "grad_norm": 5.1439666748046875,
      "learning_rate": 1.8223234624145789e-06,
      "loss": 1.3919,
      "step": 4800
    },
    {
      "epoch": 0.9206529992406985,
      "grad_norm": 6.941906452178955,
      "learning_rate": 1.8413059984813972e-06,
      "loss": 1.3528,
      "step": 4850
    },
    {
      "epoch": 0.9301442672741078,
      "grad_norm": 4.646480560302734,
      "learning_rate": 1.8602885345482157e-06,
      "loss": 1.3679,
      "step": 4900
    },
    {
      "epoch": 0.9396355353075171,
      "grad_norm": 4.150309085845947,
      "learning_rate": 1.8792710706150345e-06,
      "loss": 1.354,
      "step": 4950
    },
    {
      "epoch": 0.9491268033409264,
      "grad_norm": 6.288318157196045,
      "learning_rate": 1.898253606681853e-06,
      "loss": 1.4323,
      "step": 5000
    },
    {
      "epoch": 0.9586180713743356,
      "grad_norm": 5.229976177215576,
      "learning_rate": 1.9172361427486713e-06,
      "loss": 1.3896,
      "step": 5050
    },
    {
      "epoch": 0.9681093394077449,
      "grad_norm": 5.8981852531433105,
      "learning_rate": 1.93621867881549e-06,
      "loss": 1.3476,
      "step": 5100
    },
    {
      "epoch": 0.9776006074411542,
      "grad_norm": 7.595494747161865,
      "learning_rate": 1.9552012148823084e-06,
      "loss": 1.4024,
      "step": 5150
    },
    {
      "epoch": 0.9870918754745635,
      "grad_norm": 7.758345127105713,
      "learning_rate": 1.974183750949127e-06,
      "loss": 1.3631,
      "step": 5200
    },
    {
      "epoch": 0.9965831435079726,
      "grad_norm": 5.87869930267334,
      "learning_rate": 1.9931662870159454e-06,
      "loss": 1.3904,
      "step": 5250
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5406662237828604,
      "eval_f1": 0.19993373817835974,
      "eval_loss": 1.3035351037979126,
      "eval_precision": 0.3118988727776137,
      "eval_recall": 0.257349917072959,
      "eval_runtime": 190.9898,
      "eval_samples_per_second": 55.17,
      "eval_steps_per_second": 6.901,
      "step": 5268
    },
    {
      "epoch": 1.006074411541382,
      "grad_norm": 5.436108112335205,
      "learning_rate": 2.012148823082764e-06,
      "loss": 1.3635,
      "step": 5300
    },
    {
      "epoch": 1.0155656795747912,
      "grad_norm": 6.2826313972473145,
      "learning_rate": 2.0311313591495825e-06,
      "loss": 1.3878,
      "step": 5350
    },
    {
      "epoch": 1.0250569476082005,
      "grad_norm": 5.39915132522583,
      "learning_rate": 2.050113895216401e-06,
      "loss": 1.3445,
      "step": 5400
    },
    {
      "epoch": 1.0345482156416097,
      "grad_norm": 5.8375630378723145,
      "learning_rate": 2.0690964312832195e-06,
      "loss": 1.3352,
      "step": 5450
    },
    {
      "epoch": 1.044039483675019,
      "grad_norm": 5.015005588531494,
      "learning_rate": 2.088078967350038e-06,
      "loss": 1.3592,
      "step": 5500
    },
    {
      "epoch": 1.0535307517084282,
      "grad_norm": 6.2365288734436035,
      "learning_rate": 2.1070615034168566e-06,
      "loss": 1.312,
      "step": 5550
    },
    {
      "epoch": 1.0630220197418374,
      "grad_norm": 7.182867527008057,
      "learning_rate": 2.126044039483675e-06,
      "loss": 1.3133,
      "step": 5600
    },
    {
      "epoch": 1.0725132877752468,
      "grad_norm": 6.894881248474121,
      "learning_rate": 2.1450265755504936e-06,
      "loss": 1.3833,
      "step": 5650
    },
    {
      "epoch": 1.082004555808656,
      "grad_norm": 4.371942520141602,
      "learning_rate": 2.164009111617312e-06,
      "loss": 1.3228,
      "step": 5700
    },
    {
      "epoch": 1.0914958238420653,
      "grad_norm": 6.1741943359375,
      "learning_rate": 2.1829916476841307e-06,
      "loss": 1.2974,
      "step": 5750
    },
    {
      "epoch": 1.1009870918754745,
      "grad_norm": 6.626845836639404,
      "learning_rate": 2.2019741837509492e-06,
      "loss": 1.23,
      "step": 5800
    },
    {
      "epoch": 1.1104783599088839,
      "grad_norm": 5.182972431182861,
      "learning_rate": 2.2209567198177678e-06,
      "loss": 1.2712,
      "step": 5850
    },
    {
      "epoch": 1.119969627942293,
      "grad_norm": 8.018512725830078,
      "learning_rate": 2.2399392558845863e-06,
      "loss": 1.2832,
      "step": 5900
    },
    {
      "epoch": 1.1294608959757024,
      "grad_norm": 5.832951068878174,
      "learning_rate": 2.258921791951405e-06,
      "loss": 1.3009,
      "step": 5950
    },
    {
      "epoch": 1.1389521640091116,
      "grad_norm": 9.562484741210938,
      "learning_rate": 2.2779043280182233e-06,
      "loss": 1.2876,
      "step": 6000
    },
    {
      "epoch": 1.148443432042521,
      "grad_norm": 4.55044412612915,
      "learning_rate": 2.296886864085042e-06,
      "loss": 1.2829,
      "step": 6050
    },
    {
      "epoch": 1.1579347000759301,
      "grad_norm": 6.148865222930908,
      "learning_rate": 2.3158694001518604e-06,
      "loss": 1.3325,
      "step": 6100
    },
    {
      "epoch": 1.1674259681093395,
      "grad_norm": 5.95477819442749,
      "learning_rate": 2.334851936218679e-06,
      "loss": 1.2746,
      "step": 6150
    },
    {
      "epoch": 1.1769172361427487,
      "grad_norm": 7.444988250732422,
      "learning_rate": 2.3538344722854975e-06,
      "loss": 1.2723,
      "step": 6200
    },
    {
      "epoch": 1.1864085041761578,
      "grad_norm": 4.726620197296143,
      "learning_rate": 2.372817008352316e-06,
      "loss": 1.3168,
      "step": 6250
    },
    {
      "epoch": 1.1958997722095672,
      "grad_norm": 4.022007465362549,
      "learning_rate": 2.3917995444191345e-06,
      "loss": 1.2664,
      "step": 6300
    },
    {
      "epoch": 1.2053910402429764,
      "grad_norm": 7.383367538452148,
      "learning_rate": 2.410782080485953e-06,
      "loss": 1.304,
      "step": 6350
    },
    {
      "epoch": 1.2148823082763858,
      "grad_norm": 7.247731685638428,
      "learning_rate": 2.4297646165527716e-06,
      "loss": 1.2088,
      "step": 6400
    },
    {
      "epoch": 1.224373576309795,
      "grad_norm": 6.626587390899658,
      "learning_rate": 2.44874715261959e-06,
      "loss": 1.2823,
      "step": 6450
    },
    {
      "epoch": 1.2338648443432043,
      "grad_norm": 5.657367706298828,
      "learning_rate": 2.4677296886864086e-06,
      "loss": 1.2416,
      "step": 6500
    },
    {
      "epoch": 1.2433561123766135,
      "grad_norm": 5.688270568847656,
      "learning_rate": 2.486712224753227e-06,
      "loss": 1.2169,
      "step": 6550
    },
    {
      "epoch": 1.2528473804100229,
      "grad_norm": 5.919439792633057,
      "learning_rate": 2.505694760820046e-06,
      "loss": 1.2919,
      "step": 6600
    },
    {
      "epoch": 1.262338648443432,
      "grad_norm": 11.849943161010742,
      "learning_rate": 2.524677296886864e-06,
      "loss": 1.2256,
      "step": 6650
    },
    {
      "epoch": 1.2718299164768414,
      "grad_norm": 7.510192394256592,
      "learning_rate": 2.5436598329536827e-06,
      "loss": 1.2183,
      "step": 6700
    },
    {
      "epoch": 1.2813211845102506,
      "grad_norm": 4.747722625732422,
      "learning_rate": 2.5626423690205017e-06,
      "loss": 1.269,
      "step": 6750
    },
    {
      "epoch": 1.2908124525436597,
      "grad_norm": 5.539303302764893,
      "learning_rate": 2.58162490508732e-06,
      "loss": 1.1963,
      "step": 6800
    },
    {
      "epoch": 1.300303720577069,
      "grad_norm": 8.497318267822266,
      "learning_rate": 2.6006074411541383e-06,
      "loss": 1.2793,
      "step": 6850
    },
    {
      "epoch": 1.3097949886104785,
      "grad_norm": 4.934028625488281,
      "learning_rate": 2.6195899772209573e-06,
      "loss": 1.1998,
      "step": 6900
    },
    {
      "epoch": 1.3192862566438877,
      "grad_norm": 7.271026134490967,
      "learning_rate": 2.6385725132877754e-06,
      "loss": 1.2192,
      "step": 6950
    },
    {
      "epoch": 1.3287775246772968,
      "grad_norm": 4.72705602645874,
      "learning_rate": 2.657555049354594e-06,
      "loss": 1.2457,
      "step": 7000
    },
    {
      "epoch": 1.3382687927107062,
      "grad_norm": 5.830032825469971,
      "learning_rate": 2.6765375854214124e-06,
      "loss": 1.2299,
      "step": 7050
    },
    {
      "epoch": 1.3477600607441154,
      "grad_norm": 8.889971733093262,
      "learning_rate": 2.695520121488231e-06,
      "loss": 1.3395,
      "step": 7100
    },
    {
      "epoch": 1.3572513287775247,
      "grad_norm": 8.790315628051758,
      "learning_rate": 2.71450265755505e-06,
      "loss": 1.2323,
      "step": 7150
    },
    {
      "epoch": 1.366742596810934,
      "grad_norm": 8.498093605041504,
      "learning_rate": 2.733485193621868e-06,
      "loss": 1.2371,
      "step": 7200
    },
    {
      "epoch": 1.3762338648443433,
      "grad_norm": 7.1971917152404785,
      "learning_rate": 2.7524677296886865e-06,
      "loss": 1.2715,
      "step": 7250
    },
    {
      "epoch": 1.3857251328777525,
      "grad_norm": 7.510544776916504,
      "learning_rate": 2.7714502657555055e-06,
      "loss": 1.1283,
      "step": 7300
    },
    {
      "epoch": 1.3952164009111616,
      "grad_norm": 7.349056720733643,
      "learning_rate": 2.7904328018223236e-06,
      "loss": 1.195,
      "step": 7350
    },
    {
      "epoch": 1.404707668944571,
      "grad_norm": 6.695216655731201,
      "learning_rate": 2.809415337889142e-06,
      "loss": 1.199,
      "step": 7400
    },
    {
      "epoch": 1.4141989369779804,
      "grad_norm": 4.827058792114258,
      "learning_rate": 2.828397873955961e-06,
      "loss": 1.1474,
      "step": 7450
    },
    {
      "epoch": 1.4236902050113895,
      "grad_norm": 4.820675373077393,
      "learning_rate": 2.847380410022779e-06,
      "loss": 1.2232,
      "step": 7500
    },
    {
      "epoch": 1.4331814730447987,
      "grad_norm": 4.273978233337402,
      "learning_rate": 2.8663629460895977e-06,
      "loss": 1.2519,
      "step": 7550
    },
    {
      "epoch": 1.442672741078208,
      "grad_norm": 8.836321830749512,
      "learning_rate": 2.8853454821564162e-06,
      "loss": 1.1708,
      "step": 7600
    },
    {
      "epoch": 1.4521640091116172,
      "grad_norm": 7.872626304626465,
      "learning_rate": 2.9043280182232348e-06,
      "loss": 1.1577,
      "step": 7650
    },
    {
      "epoch": 1.4616552771450266,
      "grad_norm": 11.652875900268555,
      "learning_rate": 2.9233105542900537e-06,
      "loss": 1.2186,
      "step": 7700
    },
    {
      "epoch": 1.4711465451784358,
      "grad_norm": 7.26560640335083,
      "learning_rate": 2.942293090356872e-06,
      "loss": 1.1554,
      "step": 7750
    },
    {
      "epoch": 1.4806378132118452,
      "grad_norm": 4.772738933563232,
      "learning_rate": 2.9612756264236903e-06,
      "loss": 1.2332,
      "step": 7800
    },
    {
      "epoch": 1.4901290812452543,
      "grad_norm": 8.238712310791016,
      "learning_rate": 2.9802581624905093e-06,
      "loss": 1.1547,
      "step": 7850
    },
    {
      "epoch": 1.4996203492786635,
      "grad_norm": 7.161807060241699,
      "learning_rate": 2.9992406985573274e-06,
      "loss": 1.1491,
      "step": 7900
    },
    {
      "epoch": 1.5091116173120729,
      "grad_norm": 9.07370662689209,
      "learning_rate": 3.018223234624146e-06,
      "loss": 1.2157,
      "step": 7950
    },
    {
      "epoch": 1.5186028853454823,
      "grad_norm": 10.924820899963379,
      "learning_rate": 3.037205770690965e-06,
      "loss": 1.1609,
      "step": 8000
    },
    {
      "epoch": 1.5280941533788914,
      "grad_norm": 9.03322696685791,
      "learning_rate": 3.056188306757783e-06,
      "loss": 1.1391,
      "step": 8050
    },
    {
      "epoch": 1.5375854214123006,
      "grad_norm": 9.818289756774902,
      "learning_rate": 3.075170842824602e-06,
      "loss": 1.1589,
      "step": 8100
    },
    {
      "epoch": 1.54707668944571,
      "grad_norm": 12.107394218444824,
      "learning_rate": 3.09415337889142e-06,
      "loss": 1.103,
      "step": 8150
    },
    {
      "epoch": 1.5565679574791194,
      "grad_norm": 8.504629135131836,
      "learning_rate": 3.1131359149582386e-06,
      "loss": 1.1712,
      "step": 8200
    },
    {
      "epoch": 1.5660592255125285,
      "grad_norm": 8.453020095825195,
      "learning_rate": 3.1321184510250575e-06,
      "loss": 1.134,
      "step": 8250
    },
    {
      "epoch": 1.5755504935459377,
      "grad_norm": 10.963301658630371,
      "learning_rate": 3.1511009870918756e-06,
      "loss": 1.1265,
      "step": 8300
    },
    {
      "epoch": 1.585041761579347,
      "grad_norm": 11.298511505126953,
      "learning_rate": 3.170083523158694e-06,
      "loss": 1.1353,
      "step": 8350
    },
    {
      "epoch": 1.5945330296127562,
      "grad_norm": 8.937603950500488,
      "learning_rate": 3.189066059225513e-06,
      "loss": 1.1502,
      "step": 8400
    },
    {
      "epoch": 1.6040242976461654,
      "grad_norm": 6.993666172027588,
      "learning_rate": 3.208048595292331e-06,
      "loss": 1.1709,
      "step": 8450
    },
    {
      "epoch": 1.6135155656795748,
      "grad_norm": 8.333600997924805,
      "learning_rate": 3.2270311313591497e-06,
      "loss": 1.1268,
      "step": 8500
    },
    {
      "epoch": 1.6230068337129842,
      "grad_norm": 9.15424919128418,
      "learning_rate": 3.2460136674259683e-06,
      "loss": 1.1556,
      "step": 8550
    },
    {
      "epoch": 1.6324981017463933,
      "grad_norm": 17.349645614624023,
      "learning_rate": 3.264996203492787e-06,
      "loss": 1.1123,
      "step": 8600
    },
    {
      "epoch": 1.6419893697798025,
      "grad_norm": 8.872285842895508,
      "learning_rate": 3.2839787395596057e-06,
      "loss": 1.1356,
      "step": 8650
    },
    {
      "epoch": 1.6514806378132119,
      "grad_norm": 8.16823673248291,
      "learning_rate": 3.302961275626424e-06,
      "loss": 1.0176,
      "step": 8700
    },
    {
      "epoch": 1.6609719058466212,
      "grad_norm": 7.7541117668151855,
      "learning_rate": 3.3219438116932424e-06,
      "loss": 1.1424,
      "step": 8750
    },
    {
      "epoch": 1.6704631738800304,
      "grad_norm": 6.881016254425049,
      "learning_rate": 3.3409263477600613e-06,
      "loss": 1.1307,
      "step": 8800
    },
    {
      "epoch": 1.6799544419134396,
      "grad_norm": 8.083966255187988,
      "learning_rate": 3.3599088838268794e-06,
      "loss": 1.1504,
      "step": 8850
    },
    {
      "epoch": 1.689445709946849,
      "grad_norm": 8.695137023925781,
      "learning_rate": 3.378891419893698e-06,
      "loss": 1.0736,
      "step": 8900
    },
    {
      "epoch": 1.6989369779802581,
      "grad_norm": 8.855749130249023,
      "learning_rate": 3.397873955960517e-06,
      "loss": 1.1542,
      "step": 8950
    },
    {
      "epoch": 1.7084282460136673,
      "grad_norm": 20.272502899169922,
      "learning_rate": 3.416856492027335e-06,
      "loss": 1.096,
      "step": 9000
    },
    {
      "epoch": 1.7179195140470767,
      "grad_norm": 11.040966987609863,
      "learning_rate": 3.435839028094154e-06,
      "loss": 1.1346,
      "step": 9050
    },
    {
      "epoch": 1.727410782080486,
      "grad_norm": 5.113995552062988,
      "learning_rate": 3.454821564160972e-06,
      "loss": 1.1019,
      "step": 9100
    },
    {
      "epoch": 1.7369020501138952,
      "grad_norm": 11.643346786499023,
      "learning_rate": 3.4738041002277906e-06,
      "loss": 1.0748,
      "step": 9150
    },
    {
      "epoch": 1.7463933181473044,
      "grad_norm": 10.62111759185791,
      "learning_rate": 3.4927866362946096e-06,
      "loss": 1.1142,
      "step": 9200
    },
    {
      "epoch": 1.7558845861807137,
      "grad_norm": 8.423551559448242,
      "learning_rate": 3.5117691723614277e-06,
      "loss": 1.0018,
      "step": 9250
    },
    {
      "epoch": 1.7653758542141231,
      "grad_norm": 10.693641662597656,
      "learning_rate": 3.530751708428246e-06,
      "loss": 1.0869,
      "step": 9300
    },
    {
      "epoch": 1.7748671222475323,
      "grad_norm": 10.273809432983398,
      "learning_rate": 3.549734244495065e-06,
      "loss": 1.0726,
      "step": 9350
    },
    {
      "epoch": 1.7843583902809415,
      "grad_norm": 6.998386383056641,
      "learning_rate": 3.5687167805618832e-06,
      "loss": 1.1555,
      "step": 9400
    },
    {
      "epoch": 1.7938496583143508,
      "grad_norm": 6.818355083465576,
      "learning_rate": 3.5876993166287018e-06,
      "loss": 1.077,
      "step": 9450
    },
    {
      "epoch": 1.8033409263477602,
      "grad_norm": 12.327679634094238,
      "learning_rate": 3.6066818526955207e-06,
      "loss": 1.1372,
      "step": 9500
    },
    {
      "epoch": 1.8128321943811692,
      "grad_norm": 12.766919136047363,
      "learning_rate": 3.625664388762339e-06,
      "loss": 1.088,
      "step": 9550
    },
    {
      "epoch": 1.8223234624145785,
      "grad_norm": 7.449977397918701,
      "learning_rate": 3.6446469248291578e-06,
      "loss": 1.0675,
      "step": 9600
    },
    {
      "epoch": 1.831814730447988,
      "grad_norm": 11.831316947937012,
      "learning_rate": 3.663629460895976e-06,
      "loss": 1.1741,
      "step": 9650
    },
    {
      "epoch": 1.841305998481397,
      "grad_norm": 15.062392234802246,
      "learning_rate": 3.6826119969627944e-06,
      "loss": 1.0829,
      "step": 9700
    },
    {
      "epoch": 1.8507972665148062,
      "grad_norm": 6.773420810699463,
      "learning_rate": 3.7015945330296134e-06,
      "loss": 1.0558,
      "step": 9750
    },
    {
      "epoch": 1.8602885345482156,
      "grad_norm": 9.245349884033203,
      "learning_rate": 3.7205770690964315e-06,
      "loss": 1.118,
      "step": 9800
    },
    {
      "epoch": 1.869779802581625,
      "grad_norm": 16.190425872802734,
      "learning_rate": 3.73955960516325e-06,
      "loss": 0.98,
      "step": 9850
    },
    {
      "epoch": 1.8792710706150342,
      "grad_norm": 5.989975929260254,
      "learning_rate": 3.758542141230069e-06,
      "loss": 1.0671,
      "step": 9900
    },
    {
      "epoch": 1.8887623386484433,
      "grad_norm": 10.689846992492676,
      "learning_rate": 3.777524677296887e-06,
      "loss": 1.1324,
      "step": 9950
    },
    {
      "epoch": 1.8982536066818527,
      "grad_norm": 7.839616775512695,
      "learning_rate": 3.796507213363706e-06,
      "loss": 1.0502,
      "step": 10000
    },
    {
      "epoch": 1.907744874715262,
      "grad_norm": 11.228259086608887,
      "learning_rate": 3.815489749430524e-06,
      "loss": 1.0626,
      "step": 10050
    },
    {
      "epoch": 1.9172361427486713,
      "grad_norm": 8.731637001037598,
      "learning_rate": 3.834472285497343e-06,
      "loss": 1.0557,
      "step": 10100
    },
    {
      "epoch": 1.9267274107820804,
      "grad_norm": 7.369215488433838,
      "learning_rate": 3.853454821564161e-06,
      "loss": 1.0551,
      "step": 10150
    },
    {
      "epoch": 1.9362186788154898,
      "grad_norm": 13.776960372924805,
      "learning_rate": 3.87243735763098e-06,
      "loss": 0.9098,
      "step": 10200
    },
    {
      "epoch": 1.945709946848899,
      "grad_norm": 14.922910690307617,
      "learning_rate": 3.891419893697798e-06,
      "loss": 1.0094,
      "step": 10250
    },
    {
      "epoch": 1.9552012148823081,
      "grad_norm": 14.49189281463623,
      "learning_rate": 3.910402429764617e-06,
      "loss": 1.0408,
      "step": 10300
    },
    {
      "epoch": 1.9646924829157175,
      "grad_norm": 8.074590682983398,
      "learning_rate": 3.929384965831435e-06,
      "loss": 1.0013,
      "step": 10350
    },
    {
      "epoch": 1.974183750949127,
      "grad_norm": 13.741362571716309,
      "learning_rate": 3.948367501898254e-06,
      "loss": 1.0558,
      "step": 10400
    },
    {
      "epoch": 1.983675018982536,
      "grad_norm": 8.54836368560791,
      "learning_rate": 3.967350037965072e-06,
      "loss": 1.1325,
      "step": 10450
    },
    {
      "epoch": 1.9931662870159452,
      "grad_norm": 8.981844902038574,
      "learning_rate": 3.986332574031891e-06,
      "loss": 1.025,
      "step": 10500
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6111796526525577,
      "eval_f1": 0.3383157578598897,
      "eval_loss": 0.9857298731803894,
      "eval_precision": 0.32815693863098555,
      "eval_recall": 0.38700316043361305,
      "eval_runtime": 171.5949,
      "eval_samples_per_second": 61.406,
      "eval_steps_per_second": 7.681,
      "step": 10536
    },
    {
      "epoch": 2.0026575550493546,
      "grad_norm": 8.053272247314453,
      "learning_rate": 4.005315110098709e-06,
      "loss": 1.0352,
      "step": 10550
    },
    {
      "epoch": 2.012148823082764,
      "grad_norm": 6.401072025299072,
      "learning_rate": 4.024297646165528e-06,
      "loss": 0.9993,
      "step": 10600
    },
    {
      "epoch": 2.021640091116173,
      "grad_norm": 8.908872604370117,
      "learning_rate": 4.0432801822323464e-06,
      "loss": 0.9309,
      "step": 10650
    },
    {
      "epoch": 2.0311313591495823,
      "grad_norm": 18.409168243408203,
      "learning_rate": 4.062262718299165e-06,
      "loss": 1.0781,
      "step": 10700
    },
    {
      "epoch": 2.0406226271829917,
      "grad_norm": 8.006268501281738,
      "learning_rate": 4.0812452543659835e-06,
      "loss": 0.9994,
      "step": 10750
    },
    {
      "epoch": 2.050113895216401,
      "grad_norm": 12.994401931762695,
      "learning_rate": 4.100227790432802e-06,
      "loss": 0.9454,
      "step": 10800
    },
    {
      "epoch": 2.05960516324981,
      "grad_norm": 13.9449462890625,
      "learning_rate": 4.1192103264996205e-06,
      "loss": 0.9682,
      "step": 10850
    },
    {
      "epoch": 2.0690964312832194,
      "grad_norm": 10.504644393920898,
      "learning_rate": 4.138192862566439e-06,
      "loss": 0.9618,
      "step": 10900
    },
    {
      "epoch": 2.078587699316629,
      "grad_norm": 15.46628475189209,
      "learning_rate": 4.157175398633258e-06,
      "loss": 1.0413,
      "step": 10950
    },
    {
      "epoch": 2.088078967350038,
      "grad_norm": 18.973779678344727,
      "learning_rate": 4.176157934700076e-06,
      "loss": 1.0523,
      "step": 11000
    },
    {
      "epoch": 2.097570235383447,
      "grad_norm": 13.982255935668945,
      "learning_rate": 4.195140470766895e-06,
      "loss": 1.0198,
      "step": 11050
    },
    {
      "epoch": 2.1070615034168565,
      "grad_norm": 5.509376525878906,
      "learning_rate": 4.214123006833713e-06,
      "loss": 0.9973,
      "step": 11100
    },
    {
      "epoch": 2.116552771450266,
      "grad_norm": 6.099702835083008,
      "learning_rate": 4.233105542900532e-06,
      "loss": 0.9418,
      "step": 11150
    },
    {
      "epoch": 2.126044039483675,
      "grad_norm": 22.023571014404297,
      "learning_rate": 4.25208807896735e-06,
      "loss": 1.0543,
      "step": 11200
    },
    {
      "epoch": 2.135535307517084,
      "grad_norm": 13.960505485534668,
      "learning_rate": 4.271070615034169e-06,
      "loss": 1.0449,
      "step": 11250
    },
    {
      "epoch": 2.1450265755504936,
      "grad_norm": 16.882123947143555,
      "learning_rate": 4.290053151100987e-06,
      "loss": 0.9795,
      "step": 11300
    },
    {
      "epoch": 2.154517843583903,
      "grad_norm": 14.241255760192871,
      "learning_rate": 4.309035687167806e-06,
      "loss": 0.975,
      "step": 11350
    },
    {
      "epoch": 2.164009111617312,
      "grad_norm": 17.469114303588867,
      "learning_rate": 4.328018223234624e-06,
      "loss": 0.9592,
      "step": 11400
    },
    {
      "epoch": 2.1735003796507213,
      "grad_norm": 19.448890686035156,
      "learning_rate": 4.347000759301443e-06,
      "loss": 1.0652,
      "step": 11450
    },
    {
      "epoch": 2.1829916476841307,
      "grad_norm": 17.956144332885742,
      "learning_rate": 4.365983295368261e-06,
      "loss": 0.9862,
      "step": 11500
    },
    {
      "epoch": 2.19248291571754,
      "grad_norm": 6.523434638977051,
      "learning_rate": 4.38496583143508e-06,
      "loss": 1.0048,
      "step": 11550
    },
    {
      "epoch": 2.201974183750949,
      "grad_norm": 15.391818046569824,
      "learning_rate": 4.4039483675018985e-06,
      "loss": 1.0413,
      "step": 11600
    },
    {
      "epoch": 2.2114654517843584,
      "grad_norm": 7.022346019744873,
      "learning_rate": 4.422930903568717e-06,
      "loss": 1.0113,
      "step": 11650
    },
    {
      "epoch": 2.2209567198177678,
      "grad_norm": 10.486368179321289,
      "learning_rate": 4.4419134396355355e-06,
      "loss": 0.9765,
      "step": 11700
    },
    {
      "epoch": 2.2304479878511767,
      "grad_norm": 12.600993156433105,
      "learning_rate": 4.460895975702354e-06,
      "loss": 0.9611,
      "step": 11750
    },
    {
      "epoch": 2.239939255884586,
      "grad_norm": 21.893531799316406,
      "learning_rate": 4.4798785117691726e-06,
      "loss": 1.0058,
      "step": 11800
    },
    {
      "epoch": 2.2494305239179955,
      "grad_norm": 10.432188034057617,
      "learning_rate": 4.498861047835991e-06,
      "loss": 0.8984,
      "step": 11850
    },
    {
      "epoch": 2.258921791951405,
      "grad_norm": 12.501123428344727,
      "learning_rate": 4.51784358390281e-06,
      "loss": 0.9922,
      "step": 11900
    },
    {
      "epoch": 2.268413059984814,
      "grad_norm": 11.425581932067871,
      "learning_rate": 4.536826119969628e-06,
      "loss": 1.0186,
      "step": 11950
    },
    {
      "epoch": 2.277904328018223,
      "grad_norm": 4.125765323638916,
      "learning_rate": 4.555808656036447e-06,
      "loss": 1.0221,
      "step": 12000
    },
    {
      "epoch": 2.2873955960516326,
      "grad_norm": 24.960241317749023,
      "learning_rate": 4.574791192103265e-06,
      "loss": 1.0535,
      "step": 12050
    },
    {
      "epoch": 2.296886864085042,
      "grad_norm": 5.319580554962158,
      "learning_rate": 4.593773728170084e-06,
      "loss": 0.9933,
      "step": 12100
    },
    {
      "epoch": 2.306378132118451,
      "grad_norm": 5.7121148109436035,
      "learning_rate": 4.612756264236902e-06,
      "loss": 0.9485,
      "step": 12150
    },
    {
      "epoch": 2.3158694001518603,
      "grad_norm": 10.599714279174805,
      "learning_rate": 4.631738800303721e-06,
      "loss": 1.0054,
      "step": 12200
    },
    {
      "epoch": 2.3253606681852697,
      "grad_norm": 9.165621757507324,
      "learning_rate": 4.650721336370539e-06,
      "loss": 0.9181,
      "step": 12250
    },
    {
      "epoch": 2.334851936218679,
      "grad_norm": 9.547904014587402,
      "learning_rate": 4.669703872437358e-06,
      "loss": 0.9029,
      "step": 12300
    },
    {
      "epoch": 2.344343204252088,
      "grad_norm": 15.514917373657227,
      "learning_rate": 4.688686408504176e-06,
      "loss": 0.9714,
      "step": 12350
    },
    {
      "epoch": 2.3538344722854974,
      "grad_norm": 5.501564025878906,
      "learning_rate": 4.707668944570995e-06,
      "loss": 0.8902,
      "step": 12400
    },
    {
      "epoch": 2.3633257403189067,
      "grad_norm": 24.39622688293457,
      "learning_rate": 4.726651480637814e-06,
      "loss": 0.9044,
      "step": 12450
    },
    {
      "epoch": 2.3728170083523157,
      "grad_norm": 6.758551597595215,
      "learning_rate": 4.745634016704632e-06,
      "loss": 0.9038,
      "step": 12500
    },
    {
      "epoch": 2.382308276385725,
      "grad_norm": 5.8299641609191895,
      "learning_rate": 4.7646165527714505e-06,
      "loss": 0.9166,
      "step": 12550
    },
    {
      "epoch": 2.3917995444191344,
      "grad_norm": 14.342028617858887,
      "learning_rate": 4.783599088838269e-06,
      "loss": 1.0343,
      "step": 12600
    },
    {
      "epoch": 2.401290812452544,
      "grad_norm": 9.4110107421875,
      "learning_rate": 4.8025816249050875e-06,
      "loss": 0.9538,
      "step": 12650
    },
    {
      "epoch": 2.4107820804859528,
      "grad_norm": 24.431760787963867,
      "learning_rate": 4.821564160971906e-06,
      "loss": 0.9065,
      "step": 12700
    },
    {
      "epoch": 2.420273348519362,
      "grad_norm": 17.534738540649414,
      "learning_rate": 4.840546697038725e-06,
      "loss": 0.9507,
      "step": 12750
    },
    {
      "epoch": 2.4297646165527715,
      "grad_norm": 7.148378849029541,
      "learning_rate": 4.859529233105543e-06,
      "loss": 0.8933,
      "step": 12800
    },
    {
      "epoch": 2.4392558845861805,
      "grad_norm": 14.349488258361816,
      "learning_rate": 4.878511769172362e-06,
      "loss": 1.0324,
      "step": 12850
    },
    {
      "epoch": 2.44874715261959,
      "grad_norm": 12.241732597351074,
      "learning_rate": 4.89749430523918e-06,
      "loss": 0.9053,
      "step": 12900
    },
    {
      "epoch": 2.4582384206529992,
      "grad_norm": 12.187847137451172,
      "learning_rate": 4.916476841305999e-06,
      "loss": 0.9049,
      "step": 12950
    },
    {
      "epoch": 2.4677296886864086,
      "grad_norm": 5.260674476623535,
      "learning_rate": 4.935459377372817e-06,
      "loss": 0.9509,
      "step": 13000
    },
    {
      "epoch": 2.477220956719818,
      "grad_norm": 19.768796920776367,
      "learning_rate": 4.954441913439636e-06,
      "loss": 0.9798,
      "step": 13050
    },
    {
      "epoch": 2.486712224753227,
      "grad_norm": 14.84368896484375,
      "learning_rate": 4.973424449506454e-06,
      "loss": 0.9732,
      "step": 13100
    },
    {
      "epoch": 2.4962034927866363,
      "grad_norm": 7.285784721374512,
      "learning_rate": 4.992406985573273e-06,
      "loss": 0.9158,
      "step": 13150
    },
    {
      "epoch": 2.5056947608200457,
      "grad_norm": 10.427937507629395,
      "learning_rate": 5.011389521640092e-06,
      "loss": 1.004,
      "step": 13200
    },
    {
      "epoch": 2.5151860288534547,
      "grad_norm": 25.47098731994629,
      "learning_rate": 5.030372057706911e-06,
      "loss": 0.9438,
      "step": 13250
    },
    {
      "epoch": 2.524677296886864,
      "grad_norm": 20.943105697631836,
      "learning_rate": 5.049354593773728e-06,
      "loss": 1.0211,
      "step": 13300
    },
    {
      "epoch": 2.5341685649202734,
      "grad_norm": 21.593299865722656,
      "learning_rate": 5.068337129840547e-06,
      "loss": 0.9352,
      "step": 13350
    },
    {
      "epoch": 2.543659832953683,
      "grad_norm": 5.397031784057617,
      "learning_rate": 5.0873196659073655e-06,
      "loss": 0.9186,
      "step": 13400
    },
    {
      "epoch": 2.5531511009870917,
      "grad_norm": 9.584195137023926,
      "learning_rate": 5.106302201974184e-06,
      "loss": 0.8648,
      "step": 13450
    },
    {
      "epoch": 2.562642369020501,
      "grad_norm": 11.039591789245605,
      "learning_rate": 5.125284738041003e-06,
      "loss": 0.9105,
      "step": 13500
    },
    {
      "epoch": 2.5721336370539105,
      "grad_norm": 25.08954620361328,
      "learning_rate": 5.144267274107821e-06,
      "loss": 0.9889,
      "step": 13550
    },
    {
      "epoch": 2.5816249050873195,
      "grad_norm": 8.909172058105469,
      "learning_rate": 5.16324981017464e-06,
      "loss": 0.9447,
      "step": 13600
    },
    {
      "epoch": 2.591116173120729,
      "grad_norm": 5.847607135772705,
      "learning_rate": 5.182232346241458e-06,
      "loss": 0.9235,
      "step": 13650
    },
    {
      "epoch": 2.600607441154138,
      "grad_norm": 12.02507209777832,
      "learning_rate": 5.201214882308277e-06,
      "loss": 0.8637,
      "step": 13700
    },
    {
      "epoch": 2.6100987091875476,
      "grad_norm": 17.63705825805664,
      "learning_rate": 5.220197418375096e-06,
      "loss": 0.9516,
      "step": 13750
    },
    {
      "epoch": 2.619589977220957,
      "grad_norm": 6.873347759246826,
      "learning_rate": 5.2391799544419145e-06,
      "loss": 0.9503,
      "step": 13800
    },
    {
      "epoch": 2.629081245254366,
      "grad_norm": 3.940760374069214,
      "learning_rate": 5.258162490508732e-06,
      "loss": 0.88,
      "step": 13850
    },
    {
      "epoch": 2.6385725132877753,
      "grad_norm": 6.009947776794434,
      "learning_rate": 5.277145026575551e-06,
      "loss": 0.9167,
      "step": 13900
    },
    {
      "epoch": 2.6480637813211843,
      "grad_norm": 16.393348693847656,
      "learning_rate": 5.296127562642369e-06,
      "loss": 0.8733,
      "step": 13950
    },
    {
      "epoch": 2.6575550493545936,
      "grad_norm": 11.104180335998535,
      "learning_rate": 5.315110098709188e-06,
      "loss": 0.9286,
      "step": 14000
    },
    {
      "epoch": 2.667046317388003,
      "grad_norm": 21.233993530273438,
      "learning_rate": 5.334092634776007e-06,
      "loss": 0.927,
      "step": 14050
    },
    {
      "epoch": 2.6765375854214124,
      "grad_norm": 11.242561340332031,
      "learning_rate": 5.353075170842825e-06,
      "loss": 0.9426,
      "step": 14100
    },
    {
      "epoch": 2.686028853454822,
      "grad_norm": 5.567835807800293,
      "learning_rate": 5.372057706909643e-06,
      "loss": 0.8982,
      "step": 14150
    },
    {
      "epoch": 2.6955201214882307,
      "grad_norm": 18.347885131835938,
      "learning_rate": 5.391040242976462e-06,
      "loss": 0.9318,
      "step": 14200
    },
    {
      "epoch": 2.70501138952164,
      "grad_norm": 36.40949630737305,
      "learning_rate": 5.4100227790432804e-06,
      "loss": 0.9532,
      "step": 14250
    },
    {
      "epoch": 2.7145026575550495,
      "grad_norm": 29.326385498046875,
      "learning_rate": 5.4290053151101e-06,
      "loss": 0.8995,
      "step": 14300
    },
    {
      "epoch": 2.7239939255884584,
      "grad_norm": 2.1680731773376465,
      "learning_rate": 5.447987851176918e-06,
      "loss": 0.9124,
      "step": 14350
    },
    {
      "epoch": 2.733485193621868,
      "grad_norm": 10.704232215881348,
      "learning_rate": 5.466970387243736e-06,
      "loss": 0.8577,
      "step": 14400
    },
    {
      "epoch": 2.742976461655277,
      "grad_norm": 12.967594146728516,
      "learning_rate": 5.4859529233105546e-06,
      "loss": 0.8849,
      "step": 14450
    },
    {
      "epoch": 2.7524677296886866,
      "grad_norm": 20.072662353515625,
      "learning_rate": 5.504935459377373e-06,
      "loss": 0.9293,
      "step": 14500
    },
    {
      "epoch": 2.7619589977220955,
      "grad_norm": 9.76231575012207,
      "learning_rate": 5.523917995444192e-06,
      "loss": 0.9103,
      "step": 14550
    },
    {
      "epoch": 2.771450265755505,
      "grad_norm": 17.904836654663086,
      "learning_rate": 5.542900531511011e-06,
      "loss": 0.8766,
      "step": 14600
    },
    {
      "epoch": 2.7809415337889143,
      "grad_norm": 12.68100357055664,
      "learning_rate": 5.561883067577829e-06,
      "loss": 0.868,
      "step": 14650
    },
    {
      "epoch": 2.7904328018223232,
      "grad_norm": 8.943753242492676,
      "learning_rate": 5.580865603644647e-06,
      "loss": 0.9286,
      "step": 14700
    },
    {
      "epoch": 2.7999240698557326,
      "grad_norm": 22.137622833251953,
      "learning_rate": 5.599848139711466e-06,
      "loss": 0.8854,
      "step": 14750
    },
    {
      "epoch": 2.809415337889142,
      "grad_norm": 24.715463638305664,
      "learning_rate": 5.618830675778284e-06,
      "loss": 0.8831,
      "step": 14800
    },
    {
      "epoch": 2.8189066059225514,
      "grad_norm": 17.743698120117188,
      "learning_rate": 5.637813211845104e-06,
      "loss": 0.8238,
      "step": 14850
    },
    {
      "epoch": 2.8283978739559608,
      "grad_norm": 8.613927841186523,
      "learning_rate": 5.656795747911922e-06,
      "loss": 0.9054,
      "step": 14900
    },
    {
      "epoch": 2.8378891419893697,
      "grad_norm": 15.881367683410645,
      "learning_rate": 5.67577828397874e-06,
      "loss": 0.8643,
      "step": 14950
    },
    {
      "epoch": 2.847380410022779,
      "grad_norm": 7.400337219238281,
      "learning_rate": 5.694760820045558e-06,
      "loss": 0.9015,
      "step": 15000
    },
    {
      "epoch": 2.8568716780561885,
      "grad_norm": 11.40087604522705,
      "learning_rate": 5.713743356112377e-06,
      "loss": 0.8406,
      "step": 15050
    },
    {
      "epoch": 2.8663629460895974,
      "grad_norm": 4.264243125915527,
      "learning_rate": 5.732725892179195e-06,
      "loss": 0.9179,
      "step": 15100
    },
    {
      "epoch": 2.875854214123007,
      "grad_norm": 11.606842994689941,
      "learning_rate": 5.751708428246015e-06,
      "loss": 0.9134,
      "step": 15150
    },
    {
      "epoch": 2.885345482156416,
      "grad_norm": 10.16572380065918,
      "learning_rate": 5.7706909643128325e-06,
      "loss": 0.8177,
      "step": 15200
    },
    {
      "epoch": 2.8948367501898256,
      "grad_norm": 12.598143577575684,
      "learning_rate": 5.789673500379651e-06,
      "loss": 0.8407,
      "step": 15250
    },
    {
      "epoch": 2.9043280182232345,
      "grad_norm": 6.530345916748047,
      "learning_rate": 5.8086560364464695e-06,
      "loss": 0.8022,
      "step": 15300
    },
    {
      "epoch": 2.913819286256644,
      "grad_norm": 16.85447883605957,
      "learning_rate": 5.827638572513288e-06,
      "loss": 0.8925,
      "step": 15350
    },
    {
      "epoch": 2.9233105542900533,
      "grad_norm": 14.597683906555176,
      "learning_rate": 5.8466211085801074e-06,
      "loss": 0.9195,
      "step": 15400
    },
    {
      "epoch": 2.932801822323462,
      "grad_norm": 11.083624839782715,
      "learning_rate": 5.865603644646926e-06,
      "loss": 0.998,
      "step": 15450
    },
    {
      "epoch": 2.9422930903568716,
      "grad_norm": 5.017083168029785,
      "learning_rate": 5.884586180713744e-06,
      "loss": 0.9216,
      "step": 15500
    },
    {
      "epoch": 2.951784358390281,
      "grad_norm": 13.89981746673584,
      "learning_rate": 5.903568716780562e-06,
      "loss": 0.8791,
      "step": 15550
    },
    {
      "epoch": 2.9612756264236904,
      "grad_norm": 29.379423141479492,
      "learning_rate": 5.922551252847381e-06,
      "loss": 0.8502,
      "step": 15600
    },
    {
      "epoch": 2.9707668944570997,
      "grad_norm": 11.952815055847168,
      "learning_rate": 5.9415337889142e-06,
      "loss": 0.9269,
      "step": 15650
    },
    {
      "epoch": 2.9802581624905087,
      "grad_norm": 12.616369247436523,
      "learning_rate": 5.960516324981019e-06,
      "loss": 0.9211,
      "step": 15700
    },
    {
      "epoch": 2.989749430523918,
      "grad_norm": 4.125850200653076,
      "learning_rate": 5.979498861047836e-06,
      "loss": 0.8543,
      "step": 15750
    },
    {
      "epoch": 2.999240698557327,
      "grad_norm": 8.022698402404785,
      "learning_rate": 5.998481397114655e-06,
      "loss": 0.9157,
      "step": 15800
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.6592009110752586,
      "eval_f1": 0.4567417197688027,
      "eval_loss": 0.8483139276504517,
      "eval_precision": 0.47811200573693696,
      "eval_recall": 0.4885486975096485,
      "eval_runtime": 178.9489,
      "eval_samples_per_second": 58.883,
      "eval_steps_per_second": 7.365,
      "step": 15804
    },
    {
      "epoch": 3.0087319665907364,
      "grad_norm": 5.997342586517334,
      "learning_rate": 6.017463933181473e-06,
      "loss": 0.8226,
      "step": 15850
    },
    {
      "epoch": 3.0182232346241458,
      "grad_norm": 24.48729705810547,
      "learning_rate": 6.036446469248292e-06,
      "loss": 0.8325,
      "step": 15900
    },
    {
      "epoch": 3.027714502657555,
      "grad_norm": 12.83834457397461,
      "learning_rate": 6.055429005315111e-06,
      "loss": 0.8701,
      "step": 15950
    },
    {
      "epoch": 3.0372057706909645,
      "grad_norm": 32.75111389160156,
      "learning_rate": 6.07441154138193e-06,
      "loss": 0.8171,
      "step": 16000
    },
    {
      "epoch": 3.0466970387243735,
      "grad_norm": 11.756657600402832,
      "learning_rate": 6.0933940774487474e-06,
      "loss": 0.861,
      "step": 16050
    },
    {
      "epoch": 3.056188306757783,
      "grad_norm": 10.750813484191895,
      "learning_rate": 6.112376613515566e-06,
      "loss": 0.8855,
      "step": 16100
    },
    {
      "epoch": 3.0656795747911922,
      "grad_norm": 14.193553924560547,
      "learning_rate": 6.1313591495823845e-06,
      "loss": 0.84,
      "step": 16150
    },
    {
      "epoch": 3.075170842824601,
      "grad_norm": 9.536857604980469,
      "learning_rate": 6.150341685649204e-06,
      "loss": 0.8989,
      "step": 16200
    },
    {
      "epoch": 3.0846621108580106,
      "grad_norm": 17.304798126220703,
      "learning_rate": 6.169324221716022e-06,
      "loss": 0.866,
      "step": 16250
    },
    {
      "epoch": 3.09415337889142,
      "grad_norm": 18.733915328979492,
      "learning_rate": 6.18830675778284e-06,
      "loss": 0.875,
      "step": 16300
    },
    {
      "epoch": 3.1036446469248293,
      "grad_norm": 6.891103744506836,
      "learning_rate": 6.207289293849659e-06,
      "loss": 0.8241,
      "step": 16350
    },
    {
      "epoch": 3.1131359149582383,
      "grad_norm": 10.04080581665039,
      "learning_rate": 6.226271829916477e-06,
      "loss": 0.9091,
      "step": 16400
    },
    {
      "epoch": 3.1226271829916477,
      "grad_norm": 14.0333251953125,
      "learning_rate": 6.245254365983296e-06,
      "loss": 0.7884,
      "step": 16450
    },
    {
      "epoch": 3.132118451025057,
      "grad_norm": 16.61358642578125,
      "learning_rate": 6.264236902050115e-06,
      "loss": 0.8353,
      "step": 16500
    },
    {
      "epoch": 3.141609719058466,
      "grad_norm": 4.471676826477051,
      "learning_rate": 6.283219438116933e-06,
      "loss": 0.8904,
      "step": 16550
    },
    {
      "epoch": 3.1511009870918754,
      "grad_norm": 20.4199161529541,
      "learning_rate": 6.302201974183751e-06,
      "loss": 0.8378,
      "step": 16600
    },
    {
      "epoch": 3.1605922551252847,
      "grad_norm": 19.12438201904297,
      "learning_rate": 6.32118451025057e-06,
      "loss": 0.9128,
      "step": 16650
    },
    {
      "epoch": 3.170083523158694,
      "grad_norm": 7.2920918464660645,
      "learning_rate": 6.340167046317388e-06,
      "loss": 0.8988,
      "step": 16700
    },
    {
      "epoch": 3.179574791192103,
      "grad_norm": 14.303485870361328,
      "learning_rate": 6.359149582384208e-06,
      "loss": 0.8257,
      "step": 16750
    },
    {
      "epoch": 3.1890660592255125,
      "grad_norm": 17.748291015625,
      "learning_rate": 6.378132118451026e-06,
      "loss": 0.8944,
      "step": 16800
    },
    {
      "epoch": 3.198557327258922,
      "grad_norm": 7.693470478057861,
      "learning_rate": 6.397114654517844e-06,
      "loss": 0.8192,
      "step": 16850
    },
    {
      "epoch": 3.208048595292331,
      "grad_norm": 16.337238311767578,
      "learning_rate": 6.416097190584662e-06,
      "loss": 0.8521,
      "step": 16900
    },
    {
      "epoch": 3.21753986332574,
      "grad_norm": 18.741249084472656,
      "learning_rate": 6.435079726651481e-06,
      "loss": 0.7989,
      "step": 16950
    },
    {
      "epoch": 3.2270311313591495,
      "grad_norm": 23.731075286865234,
      "learning_rate": 6.4540622627182995e-06,
      "loss": 0.8399,
      "step": 17000
    },
    {
      "epoch": 3.236522399392559,
      "grad_norm": 24.212236404418945,
      "learning_rate": 6.473044798785119e-06,
      "loss": 0.854,
      "step": 17050
    },
    {
      "epoch": 3.2460136674259683,
      "grad_norm": 12.772549629211426,
      "learning_rate": 6.4920273348519365e-06,
      "loss": 0.7862,
      "step": 17100
    },
    {
      "epoch": 3.2555049354593772,
      "grad_norm": 12.787792205810547,
      "learning_rate": 6.511009870918755e-06,
      "loss": 0.7595,
      "step": 17150
    },
    {
      "epoch": 3.2649962034927866,
      "grad_norm": 12.363590240478516,
      "learning_rate": 6.529992406985574e-06,
      "loss": 0.7687,
      "step": 17200
    },
    {
      "epoch": 3.274487471526196,
      "grad_norm": 6.721357345581055,
      "learning_rate": 6.548974943052392e-06,
      "loss": 0.8439,
      "step": 17250
    },
    {
      "epoch": 3.283978739559605,
      "grad_norm": 13.997309684753418,
      "learning_rate": 6.5679574791192115e-06,
      "loss": 0.8081,
      "step": 17300
    },
    {
      "epoch": 3.2934700075930143,
      "grad_norm": 30.56717300415039,
      "learning_rate": 6.58694001518603e-06,
      "loss": 0.862,
      "step": 17350
    },
    {
      "epoch": 3.3029612756264237,
      "grad_norm": 9.836735725402832,
      "learning_rate": 6.605922551252848e-06,
      "loss": 0.8419,
      "step": 17400
    },
    {
      "epoch": 3.312452543659833,
      "grad_norm": 19.659711837768555,
      "learning_rate": 6.624905087319666e-06,
      "loss": 0.775,
      "step": 17450
    },
    {
      "epoch": 3.321943811693242,
      "grad_norm": 10.509561538696289,
      "learning_rate": 6.643887623386485e-06,
      "loss": 0.8521,
      "step": 17500
    },
    {
      "epoch": 3.3314350797266514,
      "grad_norm": 12.767932891845703,
      "learning_rate": 6.662870159453303e-06,
      "loss": 0.855,
      "step": 17550
    },
    {
      "epoch": 3.340926347760061,
      "grad_norm": 17.596858978271484,
      "learning_rate": 6.681852695520123e-06,
      "loss": 0.8339,
      "step": 17600
    },
    {
      "epoch": 3.35041761579347,
      "grad_norm": 23.406169891357422,
      "learning_rate": 6.70083523158694e-06,
      "loss": 0.7395,
      "step": 17650
    },
    {
      "epoch": 3.359908883826879,
      "grad_norm": 4.497544765472412,
      "learning_rate": 6.719817767653759e-06,
      "loss": 0.8464,
      "step": 17700
    },
    {
      "epoch": 3.3694001518602885,
      "grad_norm": 40.90102005004883,
      "learning_rate": 6.738800303720577e-06,
      "loss": 0.8081,
      "step": 17750
    },
    {
      "epoch": 3.378891419893698,
      "grad_norm": 19.229198455810547,
      "learning_rate": 6.757782839787396e-06,
      "loss": 0.8396,
      "step": 17800
    },
    {
      "epoch": 3.3883826879271073,
      "grad_norm": 6.52122688293457,
      "learning_rate": 6.776765375854215e-06,
      "loss": 0.9306,
      "step": 17850
    },
    {
      "epoch": 3.3978739559605162,
      "grad_norm": 9.627395629882812,
      "learning_rate": 6.795747911921034e-06,
      "loss": 0.8613,
      "step": 17900
    },
    {
      "epoch": 3.4073652239939256,
      "grad_norm": 25.42774772644043,
      "learning_rate": 6.8147304479878515e-06,
      "loss": 0.8171,
      "step": 17950
    },
    {
      "epoch": 3.416856492027335,
      "grad_norm": 17.591506958007812,
      "learning_rate": 6.83371298405467e-06,
      "loss": 0.8034,
      "step": 18000
    },
    {
      "epoch": 3.426347760060744,
      "grad_norm": 4.845323085784912,
      "learning_rate": 6.8526955201214886e-06,
      "loss": 0.8759,
      "step": 18050
    },
    {
      "epoch": 3.4358390280941533,
      "grad_norm": 11.12725830078125,
      "learning_rate": 6.871678056188308e-06,
      "loss": 0.8415,
      "step": 18100
    },
    {
      "epoch": 3.4453302961275627,
      "grad_norm": 11.668724060058594,
      "learning_rate": 6.8906605922551265e-06,
      "loss": 0.7611,
      "step": 18150
    },
    {
      "epoch": 3.454821564160972,
      "grad_norm": 12.761807441711426,
      "learning_rate": 6.909643128321944e-06,
      "loss": 0.8,
      "step": 18200
    },
    {
      "epoch": 3.464312832194381,
      "grad_norm": 18.002777099609375,
      "learning_rate": 6.928625664388763e-06,
      "loss": 0.7748,
      "step": 18250
    },
    {
      "epoch": 3.4738041002277904,
      "grad_norm": 11.603888511657715,
      "learning_rate": 6.947608200455581e-06,
      "loss": 0.8725,
      "step": 18300
    },
    {
      "epoch": 3.4832953682612,
      "grad_norm": 15.807866096496582,
      "learning_rate": 6.9665907365224e-06,
      "loss": 0.8671,
      "step": 18350
    },
    {
      "epoch": 3.4927866362946087,
      "grad_norm": 8.409049987792969,
      "learning_rate": 6.985573272589219e-06,
      "loss": 0.8368,
      "step": 18400
    },
    {
      "epoch": 3.502277904328018,
      "grad_norm": 10.034148216247559,
      "learning_rate": 7.004555808656038e-06,
      "loss": 0.8282,
      "step": 18450
    },
    {
      "epoch": 3.5117691723614275,
      "grad_norm": 13.656588554382324,
      "learning_rate": 7.023538344722855e-06,
      "loss": 0.8355,
      "step": 18500
    },
    {
      "epoch": 3.521260440394837,
      "grad_norm": 12.090187072753906,
      "learning_rate": 7.042520880789674e-06,
      "loss": 0.8207,
      "step": 18550
    },
    {
      "epoch": 3.5307517084282463,
      "grad_norm": 13.884026527404785,
      "learning_rate": 7.061503416856492e-06,
      "loss": 0.8674,
      "step": 18600
    },
    {
      "epoch": 3.540242976461655,
      "grad_norm": 18.692401885986328,
      "learning_rate": 7.080485952923312e-06,
      "loss": 0.7906,
      "step": 18650
    },
    {
      "epoch": 3.5497342444950646,
      "grad_norm": 21.117862701416016,
      "learning_rate": 7.09946848899013e-06,
      "loss": 0.9248,
      "step": 18700
    },
    {
      "epoch": 3.559225512528474,
      "grad_norm": 8.100530624389648,
      "learning_rate": 7.118451025056948e-06,
      "loss": 0.8401,
      "step": 18750
    },
    {
      "epoch": 3.568716780561883,
      "grad_norm": 12.483731269836426,
      "learning_rate": 7.1374335611237665e-06,
      "loss": 0.8803,
      "step": 18800
    },
    {
      "epoch": 3.5782080485952923,
      "grad_norm": 11.097543716430664,
      "learning_rate": 7.156416097190585e-06,
      "loss": 0.7843,
      "step": 18850
    },
    {
      "epoch": 3.5876993166287017,
      "grad_norm": 3.8094444274902344,
      "learning_rate": 7.1753986332574035e-06,
      "loss": 0.7935,
      "step": 18900
    },
    {
      "epoch": 3.597190584662111,
      "grad_norm": 7.1298508644104,
      "learning_rate": 7.194381169324223e-06,
      "loss": 0.8501,
      "step": 18950
    },
    {
      "epoch": 3.60668185269552,
      "grad_norm": 25.148744583129883,
      "learning_rate": 7.2133637053910414e-06,
      "loss": 0.7719,
      "step": 19000
    },
    {
      "epoch": 3.6161731207289294,
      "grad_norm": 13.403111457824707,
      "learning_rate": 7.232346241457859e-06,
      "loss": 0.8431,
      "step": 19050
    },
    {
      "epoch": 3.6256643887623388,
      "grad_norm": 31.748830795288086,
      "learning_rate": 7.251328777524678e-06,
      "loss": 0.831,
      "step": 19100
    },
    {
      "epoch": 3.6351556567957477,
      "grad_norm": 13.751605033874512,
      "learning_rate": 7.270311313591496e-06,
      "loss": 0.832,
      "step": 19150
    },
    {
      "epoch": 3.644646924829157,
      "grad_norm": 10.035350799560547,
      "learning_rate": 7.2892938496583155e-06,
      "loss": 0.9519,
      "step": 19200
    },
    {
      "epoch": 3.6541381928625665,
      "grad_norm": 24.165945053100586,
      "learning_rate": 7.308276385725134e-06,
      "loss": 0.7673,
      "step": 19250
    },
    {
      "epoch": 3.663629460895976,
      "grad_norm": 30.963836669921875,
      "learning_rate": 7.327258921791952e-06,
      "loss": 0.8229,
      "step": 19300
    },
    {
      "epoch": 3.6731207289293852,
      "grad_norm": 8.261945724487305,
      "learning_rate": 7.34624145785877e-06,
      "loss": 0.8127,
      "step": 19350
    },
    {
      "epoch": 3.682611996962794,
      "grad_norm": 13.631850242614746,
      "learning_rate": 7.365223993925589e-06,
      "loss": 0.6971,
      "step": 19400
    },
    {
      "epoch": 3.6921032649962036,
      "grad_norm": 23.3999080657959,
      "learning_rate": 7.384206529992407e-06,
      "loss": 0.908,
      "step": 19450
    },
    {
      "epoch": 3.7015945330296125,
      "grad_norm": 9.245640754699707,
      "learning_rate": 7.403189066059227e-06,
      "loss": 0.755,
      "step": 19500
    },
    {
      "epoch": 3.711085801063022,
      "grad_norm": 6.858055591583252,
      "learning_rate": 7.422171602126044e-06,
      "loss": 0.8711,
      "step": 19550
    },
    {
      "epoch": 3.7205770690964313,
      "grad_norm": 20.75617027282715,
      "learning_rate": 7.441154138192863e-06,
      "loss": 0.7696,
      "step": 19600
    },
    {
      "epoch": 3.7300683371298406,
      "grad_norm": 26.74806022644043,
      "learning_rate": 7.4601366742596815e-06,
      "loss": 0.8172,
      "step": 19650
    },
    {
      "epoch": 3.73955960516325,
      "grad_norm": 11.003649711608887,
      "learning_rate": 7.4791192103265e-06,
      "loss": 0.8036,
      "step": 19700
    },
    {
      "epoch": 3.749050873196659,
      "grad_norm": 15.964193344116211,
      "learning_rate": 7.498101746393319e-06,
      "loss": 0.7941,
      "step": 19750
    },
    {
      "epoch": 3.7585421412300684,
      "grad_norm": 7.206452369689941,
      "learning_rate": 7.517084282460138e-06,
      "loss": 0.7575,
      "step": 19800
    },
    {
      "epoch": 3.7680334092634777,
      "grad_norm": 10.4799222946167,
      "learning_rate": 7.5360668185269556e-06,
      "loss": 0.8496,
      "step": 19850
    },
    {
      "epoch": 3.7775246772968867,
      "grad_norm": 12.289904594421387,
      "learning_rate": 7.555049354593774e-06,
      "loss": 0.7831,
      "step": 19900
    },
    {
      "epoch": 3.787015945330296,
      "grad_norm": 13.299138069152832,
      "learning_rate": 7.574031890660593e-06,
      "loss": 0.7025,
      "step": 19950
    },
    {
      "epoch": 3.7965072133637054,
      "grad_norm": 21.67481803894043,
      "learning_rate": 7.593014426727412e-06,
      "loss": 0.9252,
      "step": 20000
    },
    {
      "epoch": 3.805998481397115,
      "grad_norm": 11.432907104492188,
      "learning_rate": 7.6119969627942305e-06,
      "loss": 0.7447,
      "step": 20050
    },
    {
      "epoch": 3.8154897494305238,
      "grad_norm": 24.214984893798828,
      "learning_rate": 7.630979498861048e-06,
      "loss": 0.8385,
      "step": 20100
    },
    {
      "epoch": 3.824981017463933,
      "grad_norm": 15.736380577087402,
      "learning_rate": 7.649962034927867e-06,
      "loss": 0.8498,
      "step": 20150
    },
    {
      "epoch": 3.8344722854973425,
      "grad_norm": 13.740530967712402,
      "learning_rate": 7.668944570994685e-06,
      "loss": 0.8154,
      "step": 20200
    },
    {
      "epoch": 3.8439635535307515,
      "grad_norm": 8.861577987670898,
      "learning_rate": 7.687927107061504e-06,
      "loss": 0.7466,
      "step": 20250
    },
    {
      "epoch": 3.853454821564161,
      "grad_norm": 14.518856048583984,
      "learning_rate": 7.706909643128322e-06,
      "loss": 0.7409,
      "step": 20300
    },
    {
      "epoch": 3.8629460895975702,
      "grad_norm": 14.758543968200684,
      "learning_rate": 7.72589217919514e-06,
      "loss": 0.6812,
      "step": 20350
    },
    {
      "epoch": 3.8724373576309796,
      "grad_norm": 11.504440307617188,
      "learning_rate": 7.74487471526196e-06,
      "loss": 0.7466,
      "step": 20400
    },
    {
      "epoch": 3.881928625664389,
      "grad_norm": 16.662830352783203,
      "learning_rate": 7.763857251328778e-06,
      "loss": 0.791,
      "step": 20450
    },
    {
      "epoch": 3.891419893697798,
      "grad_norm": 24.831533432006836,
      "learning_rate": 7.782839787395596e-06,
      "loss": 0.8391,
      "step": 20500
    },
    {
      "epoch": 3.9009111617312073,
      "grad_norm": 16.89289093017578,
      "learning_rate": 7.801822323462415e-06,
      "loss": 0.8154,
      "step": 20550
    },
    {
      "epoch": 3.9104024297646167,
      "grad_norm": 4.467473983764648,
      "learning_rate": 7.820804859529233e-06,
      "loss": 0.7032,
      "step": 20600
    },
    {
      "epoch": 3.9198936977980257,
      "grad_norm": 10.108407974243164,
      "learning_rate": 7.839787395596052e-06,
      "loss": 0.8041,
      "step": 20650
    },
    {
      "epoch": 3.929384965831435,
      "grad_norm": 5.1586713790893555,
      "learning_rate": 7.85876993166287e-06,
      "loss": 0.7562,
      "step": 20700
    },
    {
      "epoch": 3.9388762338648444,
      "grad_norm": 14.441492080688477,
      "learning_rate": 7.877752467729689e-06,
      "loss": 0.8079,
      "step": 20750
    },
    {
      "epoch": 3.948367501898254,
      "grad_norm": 17.93697738647461,
      "learning_rate": 7.896735003796508e-06,
      "loss": 0.778,
      "step": 20800
    },
    {
      "epoch": 3.9578587699316627,
      "grad_norm": 4.091307640075684,
      "learning_rate": 7.915717539863326e-06,
      "loss": 0.7562,
      "step": 20850
    },
    {
      "epoch": 3.967350037965072,
      "grad_norm": 19.032649993896484,
      "learning_rate": 7.934700075930145e-06,
      "loss": 0.766,
      "step": 20900
    },
    {
      "epoch": 3.9768413059984815,
      "grad_norm": 20.59929656982422,
      "learning_rate": 7.953682611996963e-06,
      "loss": 0.7645,
      "step": 20950
    },
    {
      "epoch": 3.9863325740318905,
      "grad_norm": 11.491216659545898,
      "learning_rate": 7.972665148063782e-06,
      "loss": 0.7518,
      "step": 21000
    },
    {
      "epoch": 3.9958238420653,
      "grad_norm": 18.77153205871582,
      "learning_rate": 7.9916476841306e-06,
      "loss": 0.7588,
      "step": 21050
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.7058935180791497,
      "eval_f1": 0.5245115568310004,
      "eval_loss": 0.737375795841217,
      "eval_precision": 0.5338324176391311,
      "eval_recall": 0.5416911104105614,
      "eval_runtime": 144.881,
      "eval_samples_per_second": 72.729,
      "eval_steps_per_second": 9.097,
      "step": 21072
    }
  ],
  "logging_steps": 50,
  "max_steps": 263400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 835612966453248.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
